I1209 11:52:12.925356 140177781462848 configurable.py:222] loading configurations from wmt14_en_de/training_args.yml
I1209 11:52:12.933125 140177781462848 configurable.py:222] loading configurations from wmt14_en_de/translation_bpe.yml
I1209 11:52:12.942057 140177781462848 configurable.py:222] loading configurations from wmt14_en_de/validation_args.yml
I1209 11:52:12.953521 140177781462848 hparams_sets.py:50] matched the pre-defined hyper-parameters set: dynamic_conv_toy
I1209 11:52:18.618932 140177781462848 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I1209 11:52:18.621831 140177781462848 training_utils.py:132] Using distribution strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f7d56c311d0> with num_replicas_in_sync=1
I1209 11:52:18.621941 140177781462848 flags_core.py:514] ==========================================================================
I1209 11:52:18.621990 140177781462848 flags_core.py:515] Parsed all matched flags: 
I1209 11:52:18.622365 140177781462848 flags_core.py:520]  distribution_strategy: mirrored     # (default: mirrored) The distribution strategy.
I1209 11:52:18.622482 140177781462848 flags_core.py:520]  dtype: float16     # (default: float16) The computation type of the whole model.
I1209 11:52:18.622552 140177781462848 flags_core.py:520]  enable_check_numerics: None     # (default: None) Whether to open the tf.debugging.enable_check_numerics. Note that this may lower down the training speed.
I1209 11:52:18.622622 140177781462848 flags_core.py:520]  enable_xla: True     # (default: None) Whether to enable XLA for training.
I1209 11:52:18.622689 140177781462848 flags_core.py:520]  hparams_set: dynamic_conv_toy     # (default: None) A string indicating a set of pre-defined hyper-parameters, e.g. transformer_base, transformer_big or transformer_768_16e_3d.
I1209 11:52:18.622760 140177781462848 flags_core.py:520]  model_dir: ./wmt14_en_de/benchmark_dy-1209     # (default: None) The path to the checkpoint for saving and loading.
I1209 11:52:18.622823 140177781462848 flags_core.py:520]  enable_quant: False     # (default: False) Whether to enable quantization for finetuning.
I1209 11:52:18.622884 140177781462848 flags_core.py:520]  quant_params: None     # (default: None) A dict of parameters for quantization.
I1209 11:52:18.622948 140177781462848 flags_core.py:523]  entry.class: trainer
I1209 11:52:18.623017 140177781462848 flags_core.py:527]  entry.params:
I1209 11:52:18.623156 140177781462848 flags_core.py:535]    criterion.class: label_smoothed_cross_entropy
I1209 11:52:18.623286 140177781462848 flags_core.py:510]    criterion.params: {"label_smoothing": 0.1}     # The criterion for training or evaluation.
I1209 11:52:18.623378 140177781462848 flags_core.py:535]    optimizer.class: Adam
I1209 11:52:18.623470 140177781462848 flags_core.py:510]    optimizer.params: {"epsilon": 1e-09, "beta_1": 0.9, "beta_2": 0.98}     # The optimizer for training.
I1209 11:52:18.623536 140177781462848 flags_core.py:535]    lr_schedule.class: noam
I1209 11:52:18.623612 140177781462848 flags_core.py:510]    lr_schedule.params: {"dmodel": 256, "warmup_steps": 3000, "initial_factor": 1.0}     # The learning schedule for training.
I1209 11:52:18.623678 140177781462848 flags_core.py:535]    validator.class: SeqGenerationValidator
I1209 11:52:18.623786 140177781462848 flags_core.py:510]    validator.params: {"eval_dataset.params": {"src_file": "/root/neurst/wmt14_en_de/newstest2013.en.txt", "trg_file": "/root/neurst/wmt14_en_de/newstest2013.de.txt"}, "eval_search_method.params": {"beam_size": 4, "length_penalty": 0.6, "maximum_decode_length": 160, "extra_decode_length": 50}, "eval_steps": 10000, "eval_start_at": 10000, "eval_criterion.class": "label_smoothed_cross_entropy", "eval_criterion.params": {}, "eval_dataset.class": "ParallelTextDataset", "eval_batch_size": 64, "eval_metric.class": "bleu", "eval_metric.params": {}, "eval_search_method.class": "beam_search", "eval_auto_average_checkpoints": true, "eval_top_checkpoints_to_keep": 10}     # The validation process while training.
I1209 11:52:18.623865 140177781462848 flags_core.py:535]    pruning_schedule.class: PolynomialDecay
I1209 11:52:18.623937 140177781462848 flags_core.py:510]    pruning_schedule.params: {}     # The schedule for weight weight_pruning.
I1209 11:52:18.624008 140177781462848 flags_core.py:510]    train_steps: 100000     # (default: 10000000) The maximum steps for training loop.
I1209 11:52:18.624079 140177781462848 flags_core.py:510]    summary_steps: 1000     # (default: 200) Doing summary(logging & tensorboard) this every steps.
I1209 11:52:18.624144 140177781462848 flags_core.py:510]    save_checkpoint_steps: 2000     # (default: 1000) Saving checkpoints this every steps.
I1209 11:52:18.624232 140177781462848 flags_core.py:523]  task.class: translation
I1209 11:52:18.624311 140177781462848 flags_core.py:527]  task.params:
I1209 11:52:18.624431 140177781462848 flags_core.py:510]    batch_size: 2048     # The number of samples per update.
I1209 11:52:18.624499 140177781462848 flags_core.py:535]    src_data_pipeline.class: TextDataPipeline
I1209 11:52:18.624582 140177781462848 flags_core.py:510]    src_data_pipeline.params: {"language": "en", "tokenizer": "moses", "subtokenizer": "bpe", "subtokenizer_codes": "/root/neurst/wmt14_en_de/codes.bpe", "vocab_path": "/root/neurst/wmt14_en_de/vocab.en"}     # The source side data pipeline.
I1209 11:52:18.624647 140177781462848 flags_core.py:535]    trg_data_pipeline.class: TextDataPipeline
I1209 11:52:18.624729 140177781462848 flags_core.py:510]    trg_data_pipeline.params: {"language": "de", "tokenizer": "moses", "subtokenizer": "bpe", "subtokenizer_codes": "/root/neurst/wmt14_en_de/codes.bpe", "vocab_path": "/root/neurst/wmt14_en_de/vocab.de"}     # The target side data pipeline.
I1209 11:52:18.624798 140177781462848 flags_core.py:510]    max_src_len: 128     # The maximum source length of training data.
I1209 11:52:18.624861 140177781462848 flags_core.py:510]    max_trg_len: 128     # The maximum target length of training data.
I1209 11:52:18.624927 140177781462848 flags_core.py:510]    batch_by_tokens: True     # Whether to batch the data by word tokens.
I1209 11:52:18.624996 140177781462848 flags_core.py:523]  model.class: LightConvolutionModel
I1209 11:52:18.625064 140177781462848 flags_core.py:527]  model.params:
I1209 11:52:18.625237 140177781462848 flags_core.py:510]    encoder.params: {}     # The encoder.
I1209 11:52:18.625321 140177781462848 flags_core.py:510]    decoder.params: {}     # The decoder.
I1209 11:52:18.625393 140177781462848 flags_core.py:510]    modality.share_source_target_embedding: False     # (default: False) Whether to share source and target embedding table.
I1209 11:52:18.625469 140177781462848 flags_core.py:510]    modality.share_embedding_and_softmax_weights: True     # (default: False) Whether to share the target embedding table and softmax weights.
I1209 11:52:18.625536 140177781462848 flags_core.py:510]    modality.dim: 256     # The default embedding dimension for both source and target side.
I1209 11:52:18.625601 140177781462848 flags_core.py:510]    modality.timing: sinusoids     # The arbitrary parameters for positional encoding of both source and target side.
I1209 11:52:18.625667 140177781462848 flags_core.py:510]    encoder.num_layers: 5     # The number of stacking layers of the encoder.
I1209 11:52:18.625739 140177781462848 flags_core.py:510]    encoder.conv_kernel_size_list: [3, 7, 15, 15, 15]     # A list of kernel sizes for each encoder layers. The length of the list must be equal to `encoder.num_layers`.
I1209 11:52:18.625806 140177781462848 flags_core.py:510]    encoder.num_conv_heads: 4     # The number of heads of encoder convolution shared weights.
I1209 11:52:18.625868 140177781462848 flags_core.py:510]    encoder.conv_hidden_size: 256     # The number of hidden units of the encoder convolution layer.
I1209 11:52:18.625931 140177781462848 flags_core.py:510]    encoder.conv_type: dynamic     # (default: lightweight) The type of encoder conv layer, one of lightweight or dynamic.
I1209 11:52:18.626001 140177781462848 flags_core.py:510]    encoder.filter_size: 1024     # The number of the filter size of encoder ffn.
I1209 11:52:18.626068 140177781462848 flags_core.py:510]    encoder.ffn_activation: relu     # (default: relu) The activation function of encoder ffn layer.
I1209 11:52:18.626144 140177781462848 flags_core.py:510]    encoder.conv_weight_dropout_rate: 0.1     # (default: 0.0) The dropout rate of encoder convolution weights.
I1209 11:52:18.626209 140177781462848 flags_core.py:510]    encoder.glu_after_proj: True     # (default: False) Whether to apply glu activation after input projection in encoder convolution layer.
I1209 11:52:18.626272 140177781462848 flags_core.py:510]    encoder.ffn_dropout_rate: 0.1     # (default: 0.0) The dropout rate of encoder ffn layer.
I1209 11:52:18.626345 140177781462848 flags_core.py:510]    encoder.layer_postprocess_dropout_rate: 0.1     # (default: 0.0) The dropout rate for each layer's post process in encoder.
I1209 11:52:18.626414 140177781462848 flags_core.py:510]    decoder.num_layers: 5     # The number of stacking layers of the decoder.
I1209 11:52:18.626484 140177781462848 flags_core.py:510]    decoder.conv_kernel_size_list: [3, 7, 15, 15, 15]     # A list of kernel sizes for each decoder layers. The length of the list must be equal to `decoder.num_layers`.
I1209 11:52:18.626547 140177781462848 flags_core.py:510]    decoder.num_conv_heads: 4     # The number of heads of decoder convolution shared weights.
I1209 11:52:18.626609 140177781462848 flags_core.py:510]    decoder.conv_hidden_size: 256     # The number of hidden units of the decoder convolution layer.
I1209 11:52:18.626671 140177781462848 flags_core.py:510]    decoder.num_attention_heads: 4     # The number of heads of decoder's encoder-decoder attention.
I1209 11:52:18.626735 140177781462848 flags_core.py:510]    decoder.conv_type: dynamic     # (default: lightweight) The type of decoder conv layer, one of lightweight or dynamic.
I1209 11:52:18.626803 140177781462848 flags_core.py:510]    decoder.filter_size: 1024     # The number of the filter size of decoder ffn.
I1209 11:52:18.626865 140177781462848 flags_core.py:510]    decoder.ffn_activation: relu     # (default: relu) The activation function of decoder ffn layer.
I1209 11:52:18.626929 140177781462848 flags_core.py:510]    decoder.attention_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder's encoder-decoder attention.
I1209 11:52:18.626992 140177781462848 flags_core.py:510]    decoder.conv_weight_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder convolution weights.
I1209 11:52:18.627072 140177781462848 flags_core.py:510]    decoder.glu_after_proj: True     # (default: False) Whether to apply glu activation after input projection in decoder convolution layer.
I1209 11:52:18.627141 140177781462848 flags_core.py:510]    decoder.attention_type: dot_product     # (default: dot_product) The type of the attention function of decoder's encoder-decoder attention.
I1209 11:52:18.627206 140177781462848 flags_core.py:510]    decoder.ffn_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder ffn layer.
I1209 11:52:18.627270 140177781462848 flags_core.py:510]    decoder.layer_postprocess_dropout_rate: 0.1     # (default: 0.0) The dropout rate for each layer's post process in decoder.
I1209 11:52:18.627356 140177781462848 flags_core.py:523]  dataset.class: ParallelTextDataset
I1209 11:52:18.627424 140177781462848 flags_core.py:527]  dataset.params:
I1209 11:52:18.627513 140177781462848 flags_core.py:510]    src_file: /root/neurst/wmt14_en_de/train.en.tok.bpe.txt     # The source text file
I1209 11:52:18.627577 140177781462848 flags_core.py:510]    trg_file: /root/neurst/wmt14_en_de/train.de.tok.bpe.txt     # The target text file
I1209 11:52:18.627641 140177781462848 flags_core.py:510]    data_is_processed: True     # Whether the text data is already processed.
I1209 11:52:18.627705 140177781462848 flags_core.py:545] 
I1209 11:52:18.627767 140177781462848 flags_core.py:546] Other flags:
I1209 11:52:18.627832 140177781462848 flags_core.py:548]  config_paths: ['wmt14_en_de/training_args.yml,wmt14_en_de/translation_bpe.yml,wmt14_en_de/validation_args.yml']
I1209 11:52:18.627897 140177781462848 flags_core.py:563] ==========================================================================
I1209 11:52:18.627961 140177781462848 training_utils.py:74] Using float16 as computation dtype.
I1209 11:52:18.717604 140177781462848 device_compatibility_check.py:124] Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100S-PCIE-32GB, compute capability 7.0
I1209 11:52:18.718354 140177781462848 registry.py:39] Creating task: <class 'neurst.tasks.translation.Translation'>
I1209 11:52:18.718456 140177781462848 registry.py:41]   (task) arguments: 
I1209 11:52:18.718507 140177781462848 registry.py:51]     batch_size: 2048
I1209 11:52:18.718551 140177781462848 registry.py:51]     src_data_pipeline.class: TextDataPipeline
I1209 11:52:18.718595 140177781462848 registry.py:44]     src_data_pipeline.params:
I1209 11:52:18.718637 140177781462848 registry.py:49]       language: en
I1209 11:52:18.718679 140177781462848 registry.py:49]       tokenizer: moses
I1209 11:52:18.718719 140177781462848 registry.py:49]       subtokenizer: bpe
I1209 11:52:18.718761 140177781462848 registry.py:49]       subtokenizer_codes: /root/neurst/wmt14_en_de/codes.bpe
I1209 11:52:18.718802 140177781462848 registry.py:49]       vocab_path: /root/neurst/wmt14_en_de/vocab.en
I1209 11:52:18.718843 140177781462848 registry.py:51]     trg_data_pipeline.class: TextDataPipeline
I1209 11:52:18.718884 140177781462848 registry.py:44]     trg_data_pipeline.params:
I1209 11:52:18.718926 140177781462848 registry.py:49]       language: de
I1209 11:52:18.718966 140177781462848 registry.py:49]       tokenizer: moses
I1209 11:52:18.719007 140177781462848 registry.py:49]       subtokenizer: bpe
I1209 11:52:18.719047 140177781462848 registry.py:49]       subtokenizer_codes: /root/neurst/wmt14_en_de/codes.bpe
I1209 11:52:18.719088 140177781462848 registry.py:49]       vocab_path: /root/neurst/wmt14_en_de/vocab.de
I1209 11:52:18.719128 140177781462848 registry.py:51]     max_src_len: 128
I1209 11:52:18.719169 140177781462848 registry.py:51]     max_trg_len: 128
I1209 11:52:18.719212 140177781462848 registry.py:51]     batch_by_tokens: True
I1209 11:52:18.719253 140177781462848 registry.py:51]     shuffle_buffer: 0
I1209 11:52:18.719301 140177781462848 registry.py:51]     batch_size_per_gpu: None
I1209 11:52:18.719344 140177781462848 registry.py:51]     cache_dataset: None
I1209 11:52:18.719386 140177781462848 registry.py:51]     truncate_src: None
I1209 11:52:18.719427 140177781462848 registry.py:51]     truncate_trg: None
I1209 11:52:18.719468 140177781462848 registry.py:51]     target_begin_of_sentence: bos
I1209 11:52:18.719508 140177781462848 registry.py:51]     gpu_efficient_level: 0
I1209 11:52:18.719549 140177781462848 registry.py:51]     auto_scaling_batch_size: None
I1209 11:52:20.001629 140177781462848 registry.py:39] Creating dataset: <class 'neurst.data.datasets.parallel_text_dataset.ParallelTextDataset'>
I1209 11:52:20.001920 140177781462848 registry.py:41]   (dataset) arguments: 
I1209 11:52:20.001977 140177781462848 registry.py:51]     src_file: /root/neurst/wmt14_en_de/train.en.tok.bpe.txt
I1209 11:52:20.002023 140177781462848 registry.py:51]     trg_file: /root/neurst/wmt14_en_de/train.de.tok.bpe.txt
I1209 11:52:20.002068 140177781462848 registry.py:51]     data_is_processed: True
I1209 11:52:20.002113 140177781462848 registry.py:51]     raw_trg_file: None
I1209 11:52:20.002155 140177781462848 registry.py:51]     src_lang: None
I1209 11:52:20.002197 140177781462848 registry.py:51]     trg_lang: None
I1209 11:52:20.002472 140177781462848 registry.py:39] Creating model: <class 'neurst.models.light_convolution_model.LightConvolutionModel'>
I1209 11:52:20.002523 140177781462848 registry.py:41]   (model) arguments: 
I1209 11:52:20.002569 140177781462848 registry.py:44]     encoder.params:
I1209 11:52:20.002620 140177781462848 registry.py:44]     decoder.params:
I1209 11:52:20.002666 140177781462848 registry.py:51]     modality.share_source_target_embedding: False
I1209 11:52:20.002709 140177781462848 registry.py:51]     modality.share_embedding_and_softmax_weights: True
I1209 11:52:20.002751 140177781462848 registry.py:51]     modality.dim: 256
I1209 11:52:20.002792 140177781462848 registry.py:51]     modality.timing: sinusoids
I1209 11:52:20.002833 140177781462848 registry.py:51]     encoder.num_layers: 5
I1209 11:52:20.002885 140177781462848 registry.py:51]     encoder.conv_kernel_size_list: [3, 7, 15, 15, 15]
I1209 11:52:20.002927 140177781462848 registry.py:51]     encoder.num_conv_heads: 4
I1209 11:52:20.002969 140177781462848 registry.py:51]     encoder.conv_hidden_size: 256
I1209 11:52:20.003011 140177781462848 registry.py:51]     encoder.conv_type: dynamic
I1209 11:52:20.003053 140177781462848 registry.py:51]     encoder.filter_size: 1024
I1209 11:52:20.003094 140177781462848 registry.py:51]     encoder.ffn_activation: relu
I1209 11:52:20.003141 140177781462848 registry.py:51]     encoder.conv_weight_dropout_rate: 0.1
I1209 11:52:20.003186 140177781462848 registry.py:51]     encoder.glu_after_proj: True
I1209 11:52:20.003229 140177781462848 registry.py:51]     encoder.ffn_dropout_rate: 0.1
I1209 11:52:20.003270 140177781462848 registry.py:51]     encoder.layer_postprocess_dropout_rate: 0.1
I1209 11:52:20.003319 140177781462848 registry.py:51]     decoder.num_layers: 5
I1209 11:52:20.003363 140177781462848 registry.py:51]     decoder.conv_kernel_size_list: [3, 7, 15, 15, 15]
I1209 11:52:20.003405 140177781462848 registry.py:51]     decoder.num_conv_heads: 4
I1209 11:52:20.003445 140177781462848 registry.py:51]     decoder.conv_hidden_size: 256
I1209 11:52:20.003486 140177781462848 registry.py:51]     decoder.num_attention_heads: 4
I1209 11:52:20.003527 140177781462848 registry.py:51]     decoder.conv_type: dynamic
I1209 11:52:20.003568 140177781462848 registry.py:51]     decoder.filter_size: 1024
I1209 11:52:20.003609 140177781462848 registry.py:51]     decoder.ffn_activation: relu
I1209 11:52:20.003650 140177781462848 registry.py:51]     decoder.attention_dropout_rate: 0.1
I1209 11:52:20.003692 140177781462848 registry.py:51]     decoder.conv_weight_dropout_rate: 0.1
I1209 11:52:20.003734 140177781462848 registry.py:51]     decoder.glu_after_proj: True
I1209 11:52:20.003774 140177781462848 registry.py:51]     decoder.attention_type: dot_product
I1209 11:52:20.003816 140177781462848 registry.py:51]     decoder.ffn_dropout_rate: 0.1
I1209 11:52:20.003858 140177781462848 registry.py:51]     decoder.layer_postprocess_dropout_rate: 0.1
I1209 11:52:20.003899 140177781462848 registry.py:51]     encoder.class: None
I1209 11:52:20.003940 140177781462848 registry.py:51]     decoder.class: None
I1209 11:52:20.003981 140177781462848 registry.py:51]     modality.source.dim: None
I1209 11:52:20.004022 140177781462848 registry.py:51]     modality.target.dim: None
I1209 11:52:20.004063 140177781462848 registry.py:51]     modality.source.timing: None
I1209 11:52:20.004103 140177781462848 registry.py:51]     modality.target.timing: None
I1209 11:52:20.004150 140177781462848 registry.py:51]     encoder.layer_postprocess_epsilon: 1e-06
I1209 11:52:20.004193 140177781462848 registry.py:51]     decoder.layer_postprocess_epsilon: 1e-06
I1209 11:52:20.004235 140177781462848 registry.py:53]   (model) extra args: 
I1209 11:52:20.004282 140177781462848 registry.py:55]     - {'language': 'en', 'vocab_size': 42293, 'eos_id': 42292, 'bos_id': 42291, 'unk_id': 42290, 'pad_id': 42292, 'padding_mode': 2}
I1209 11:52:20.004336 140177781462848 registry.py:55]     - {'language': 'de', 'vocab_size': 43629, 'eos_id': 43628, 'bos_id': 43627, 'unk_id': 43626, 'pad_id': 43628, 'padding_mode': 2}
I1209 11:52:20.004378 140177781462848 registry.py:57]   (model) extra k-v args: 
I1209 11:52:20.004420 140177781462848 registry.py:59]     name: None
I1209 11:52:23.246015 140177781462848 registry.py:39] Creating entry: <class 'neurst.exps.trainer.Trainer'>
I1209 11:52:23.246306 140177781462848 registry.py:41]   (entry) arguments: 
I1209 11:52:23.246372 140177781462848 registry.py:51]     criterion.class: label_smoothed_cross_entropy
I1209 11:52:23.246428 140177781462848 registry.py:44]     criterion.params:
I1209 11:52:23.246488 140177781462848 registry.py:49]       label_smoothing: 0.1
I1209 11:52:23.246539 140177781462848 registry.py:51]     optimizer.class: Adam
I1209 11:52:23.246584 140177781462848 registry.py:44]     optimizer.params:
I1209 11:52:23.246632 140177781462848 registry.py:49]       epsilon: 1e-09
I1209 11:52:23.246677 140177781462848 registry.py:49]       beta_1: 0.9
I1209 11:52:23.246720 140177781462848 registry.py:49]       beta_2: 0.98
I1209 11:52:23.246762 140177781462848 registry.py:51]     lr_schedule.class: noam
I1209 11:52:23.246803 140177781462848 registry.py:44]     lr_schedule.params:
I1209 11:52:23.246846 140177781462848 registry.py:49]       dmodel: 256
I1209 11:52:23.246887 140177781462848 registry.py:49]       warmup_steps: 3000
I1209 11:52:23.246929 140177781462848 registry.py:49]       initial_factor: 1.0
I1209 11:52:23.246971 140177781462848 registry.py:51]     validator.class: SeqGenerationValidator
I1209 11:52:23.247012 140177781462848 registry.py:44]     validator.params:
I1209 11:52:23.247058 140177781462848 registry.py:49]       eval_dataset.params: {'src_file': '/root/neurst/wmt14_en_de/newstest2013.en.txt', 'trg_file': '/root/neurst/wmt14_en_de/newstest2013.de.txt'}
I1209 11:52:23.247106 140177781462848 registry.py:49]       eval_search_method.params: {'beam_size': 4, 'length_penalty': 0.6, 'maximum_decode_length': 160, 'extra_decode_length': 50}
I1209 11:52:23.247152 140177781462848 registry.py:49]       eval_steps: 10000
I1209 11:52:23.247193 140177781462848 registry.py:49]       eval_start_at: 10000
I1209 11:52:23.247235 140177781462848 registry.py:49]       eval_criterion.class: label_smoothed_cross_entropy
I1209 11:52:23.247277 140177781462848 registry.py:49]       eval_criterion.params: {}
I1209 11:52:23.247322 140177781462848 registry.py:49]       eval_dataset.class: ParallelTextDataset
I1209 11:52:23.247364 140177781462848 registry.py:49]       eval_batch_size: 64
I1209 11:52:23.247405 140177781462848 registry.py:49]       eval_metric.class: bleu
I1209 11:52:23.247446 140177781462848 registry.py:49]       eval_metric.params: {}
I1209 11:52:23.247488 140177781462848 registry.py:49]       eval_search_method.class: beam_search
I1209 11:52:23.247532 140177781462848 registry.py:49]       eval_auto_average_checkpoints: True
I1209 11:52:23.247574 140177781462848 registry.py:49]       eval_top_checkpoints_to_keep: 10
I1209 11:52:23.247616 140177781462848 registry.py:51]     pruning_schedule.class: PolynomialDecay
I1209 11:52:23.247657 140177781462848 registry.py:44]     pruning_schedule.params:
I1209 11:52:23.247700 140177781462848 registry.py:51]     train_steps: 100000
I1209 11:52:23.247741 140177781462848 registry.py:51]     summary_steps: 1000
I1209 11:52:23.247782 140177781462848 registry.py:51]     save_checkpoint_steps: 2000
I1209 11:52:23.247824 140177781462848 registry.py:51]     tb_log_dir: None
I1209 11:52:23.247865 140177781462848 registry.py:51]     train_epochs: None
I1209 11:52:23.247906 140177781462848 registry.py:51]     checkpoints_max_to_keep: 8
I1209 11:52:23.247947 140177781462848 registry.py:51]     initial_global_step: None
I1209 11:52:23.247989 140177781462848 registry.py:51]     pretrain_model: None
I1209 11:52:23.248030 140177781462848 registry.py:51]     pretrain_variable_pattern: None
I1209 11:52:23.248071 140177781462848 registry.py:51]     update_cycle: 1
I1209 11:52:23.248112 140177781462848 registry.py:51]     clip_value: None
I1209 11:52:23.248154 140177781462848 registry.py:51]     clip_norm: None
I1209 11:52:23.248195 140177781462848 registry.py:51]     experimental_count_batch_num: None
I1209 11:52:23.248236 140177781462848 registry.py:51]     freeze_variables: None
I1209 11:52:23.248277 140177781462848 registry.py:51]     pruning_variable_pattern: None
I1209 11:52:23.248329 140177781462848 registry.py:51]     nopruning_variable_pattern: None
I1209 11:52:23.248370 140177781462848 registry.py:51]     optimizer_controller: None
I1209 11:52:23.248412 140177781462848 registry.py:51]     optimizer_controller_args: None
I1209 11:52:23.248453 140177781462848 registry.py:57]   (entry) extra k-v args: 
I1209 11:52:23.248499 140177781462848 registry.py:59]     strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f7d56c311d0>
I1209 11:52:23.248544 140177781462848 registry.py:59]     model: <neurst.models.light_convolution_model.LightConvolutionModel object at 0x7f7bfc7ecd68>
I1209 11:52:23.248593 140177781462848 registry.py:59]     task: <neurst.tasks.translation.Translation object at 0x7f7d59321ac8>
I1209 11:52:23.248634 140177781462848 registry.py:59]     model_dir: ./wmt14_en_de/benchmark_dy-1209
I1209 11:52:23.248682 140177781462848 registry.py:59]     custom_dataset: <neurst.data.datasets.parallel_text_dataset.ParallelTextDataset object at 0x7f7d593214e0>
I1209 11:52:23.248883 140177781462848 registry.py:39] Creating criterion: <class 'neurst.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropy'>
I1209 11:52:23.248935 140177781462848 registry.py:41]   (criterion) arguments: 
I1209 11:52:23.248980 140177781462848 registry.py:51]     label_smoothing: 0.1
I1209 11:52:23.249035 140177781462848 label_smoothed_cross_entropy.py:38] Using LabelSmoothedCrossEntropy with label_smoothing=0.1
I1209 11:52:23.249104 140177781462848 registry.py:39] Creating optimizer: <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>
I1209 11:52:23.249148 140177781462848 registry.py:57]   (optimizer) extra k-v args: 
I1209 11:52:23.249192 140177781462848 registry.py:59]     epsilon: 1e-09
I1209 11:52:23.249236 140177781462848 registry.py:59]     beta_1: 0.9
I1209 11:52:23.249279 140177781462848 registry.py:59]     beta_2: 0.98
I1209 11:52:23.249527 140177781462848 registry.py:39] Creating validator: <class 'neurst.training.seq_generation_validator.SeqGenerationValidator'>
I1209 11:52:23.249577 140177781462848 registry.py:41]   (validator) arguments: 
I1209 11:52:23.249622 140177781462848 registry.py:44]     eval_dataset.params:
I1209 11:52:23.249665 140177781462848 registry.py:49]       src_file: /root/neurst/wmt14_en_de/newstest2013.en.txt
I1209 11:52:23.249706 140177781462848 registry.py:49]       trg_file: /root/neurst/wmt14_en_de/newstest2013.de.txt
I1209 11:52:23.249748 140177781462848 registry.py:44]     eval_search_method.params:
I1209 11:52:23.249790 140177781462848 registry.py:49]       beam_size: 4
I1209 11:52:23.249832 140177781462848 registry.py:49]       length_penalty: 0.6
I1209 11:52:23.249874 140177781462848 registry.py:49]       maximum_decode_length: 160
I1209 11:52:23.249915 140177781462848 registry.py:49]       extra_decode_length: 50
I1209 11:52:23.249956 140177781462848 registry.py:51]     eval_steps: 10000
I1209 11:52:23.249997 140177781462848 registry.py:51]     eval_start_at: 10000
I1209 11:52:23.250038 140177781462848 registry.py:51]     eval_criterion.class: label_smoothed_cross_entropy
I1209 11:52:23.250079 140177781462848 registry.py:44]     eval_criterion.params:
I1209 11:52:23.250120 140177781462848 registry.py:51]     eval_dataset.class: ParallelTextDataset
I1209 11:52:23.250161 140177781462848 registry.py:51]     eval_batch_size: 64
I1209 11:52:23.250202 140177781462848 registry.py:51]     eval_metric.class: bleu
I1209 11:52:23.250242 140177781462848 registry.py:44]     eval_metric.params:
I1209 11:52:23.250283 140177781462848 registry.py:51]     eval_search_method.class: beam_search
I1209 11:52:23.250330 140177781462848 registry.py:51]     eval_auto_average_checkpoints: True
I1209 11:52:23.250372 140177781462848 registry.py:51]     eval_top_checkpoints_to_keep: 10
I1209 11:52:23.250413 140177781462848 registry.py:51]     eval_on_begin: False
I1209 11:52:23.250455 140177781462848 registry.py:51]     eval_task_args: None
I1209 11:52:23.250497 140177781462848 registry.py:51]     eval_estop_patience: 0
I1209 11:52:23.250541 140177781462848 registry.py:51]     eval_best_checkpoint_path: None
I1209 11:52:23.250583 140177781462848 registry.py:51]     eval_best_avg_checkpoint_path: None
I1209 11:52:23.250703 140177781462848 registry.py:39] Creating pruning_schedule: <class 'neurst.sparsity.pruning_schedule.PolynomialDecay'>
I1209 11:52:23.250749 140177781462848 registry.py:41]   (pruning_schedule) arguments: 
I1209 11:52:23.250792 140177781462848 registry.py:51]     target_sparsity: 0.0
I1209 11:52:23.250835 140177781462848 registry.py:51]     begin_pruning_step: 0
I1209 11:52:23.250876 140177781462848 registry.py:51]     end_pruning_step: -1
I1209 11:52:23.250917 140177781462848 registry.py:51]     pruning_frequency: 100
I1209 11:52:23.250959 140177781462848 registry.py:51]     initial_sparsity: 0.0
I1209 11:52:23.251000 140177781462848 registry.py:51]     polynomial_power: 3
I1209 11:52:23.251140 140177781462848 translation.py:70] Creating training dataset with GPU efficient level=0.
I1209 11:52:24.679034 140177781462848 dataset_utils.py:330] Filtering empty data and datas exceeded max length={'feature': 128, 'label': 128}
I1209 11:52:24.903462 140177781462848 dataset_utils.py:452] The global batch size is 2048 tokens.
I1209 11:52:24.903656 140177781462848 dataset_utils.py:497] The details of batching logic:
I1209 11:52:24.903718 140177781462848 dataset_utils.py:503]    - batch=256, bucket boundary={'feature': 8, 'label': 8}
I1209 11:52:24.903769 140177781462848 dataset_utils.py:503]    - batch=128, bucket boundary={'feature': 16, 'label': 16}
I1209 11:52:24.903816 140177781462848 dataset_utils.py:503]    - batch=85, bucket boundary={'feature': 24, 'label': 24}
I1209 11:52:24.903861 140177781462848 dataset_utils.py:503]    - batch=64, bucket boundary={'feature': 32, 'label': 32}
I1209 11:52:24.903906 140177781462848 dataset_utils.py:503]    - batch=51, bucket boundary={'feature': 40, 'label': 40}
I1209 11:52:24.903951 140177781462848 dataset_utils.py:503]    - batch=42, bucket boundary={'feature': 48, 'label': 48}
I1209 11:52:24.903995 140177781462848 dataset_utils.py:503]    - batch=36, bucket boundary={'feature': 56, 'label': 56}
I1209 11:52:24.904039 140177781462848 dataset_utils.py:503]    - batch=32, bucket boundary={'feature': 64, 'label': 64}
I1209 11:52:24.904083 140177781462848 dataset_utils.py:503]    - batch=28, bucket boundary={'feature': 72, 'label': 72}
I1209 11:52:24.904127 140177781462848 dataset_utils.py:503]    - batch=25, bucket boundary={'feature': 80, 'label': 80}
I1209 11:52:24.904171 140177781462848 dataset_utils.py:503]    - batch=23, bucket boundary={'feature': 88, 'label': 88}
I1209 11:52:24.904215 140177781462848 dataset_utils.py:503]    - batch=21, bucket boundary={'feature': 96, 'label': 96}
I1209 11:52:24.904258 140177781462848 dataset_utils.py:503]    - batch=19, bucket boundary={'feature': 104, 'label': 104}
I1209 11:52:24.904314 140177781462848 dataset_utils.py:503]    - batch=18, bucket boundary={'feature': 112, 'label': 112}
I1209 11:52:24.904361 140177781462848 dataset_utils.py:503]    - batch=17, bucket boundary={'feature': 120, 'label': 120}
I1209 11:52:24.904405 140177781462848 dataset_utils.py:503]    - batch=16, bucket boundary={'feature': 128, 'label': 128}
I1209 11:52:24.904448 140177781462848 dataset_utils.py:506]   Total 16 input shapes are compiled.
I1209 11:52:31.139852 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:31.142668 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:32.151761 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:32.154224 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:33.185138 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:33.187668 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:34.195693 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:34.198075 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:35.264854 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:35.267205 140177781462848 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 11:52:36.452156 140177781462848 trainer.py:155] No checkpoint restored from model_dir=./wmt14_en_de/benchmark_dy-1209
I1209 11:52:36.452417 140177781462848 registry.py:39] Creating lr_schedule: <class 'neurst.optimizers.schedules.noam_schedule.NoamSchedule'>
I1209 11:52:36.452477 140177781462848 registry.py:41]   (lr_schedule) arguments: 
I1209 11:52:36.452527 140177781462848 registry.py:51]     dmodel: 256
I1209 11:52:36.452571 140177781462848 registry.py:51]     warmup_steps: 3000
I1209 11:52:36.452619 140177781462848 registry.py:51]     initial_factor: 1.0
I1209 11:52:36.452664 140177781462848 registry.py:51]     end_factor: None
I1209 11:52:36.452706 140177781462848 registry.py:51]     start_decay_at: 0
I1209 11:52:36.452748 140177781462848 registry.py:51]     decay_steps: None
I1209 11:52:36.453279 140177781462848 noam_schedule.py:39] Initialize NoamSchedule from global step=0. The result learning rate will be scaled by 1.0
I1209 11:52:36.453535 140177781462848 revised_dynamic_loss_scale.py:50] Using RevisedDynamaicLossScale under FP16 to ensure tf.reduce_all behaviour, especially under XLA
I1209 11:52:36.556055 140177781462848 model_utils.py:80] variable name  | # parameters
I1209 11:52:36.557664 140177781462848 model_utils.py:132]  SequenceToSequence (--/30.71m params)
I1209 11:52:36.557757 140177781462848 model_utils.py:132]    SequenceToSequence/input_symbol_modality_posenc_wrapper (--/10.83m params)
I1209 11:52:36.557811 140177781462848 model_utils.py:132]      SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality (--/10.83m params)
I1209 11:52:36.557861 140177781462848 model_utils.py:132]        SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality/emb (--/10.83m params)
I1209 11:52:36.557915 140177781462848 model_utils.py:137]          SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality/emb/weights (42293x256, 10.83m params)
I1209 11:52:36.557965 140177781462848 model_utils.py:132]    SequenceToSequence/target_symbol_modality_posenc_wrapper (--/11.21m params)
I1209 11:52:36.558011 140177781462848 model_utils.py:132]      SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality (--/11.21m params)
I1209 11:52:36.558057 140177781462848 model_utils.py:132]        SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared (--/11.21m params)
I1209 11:52:36.558104 140177781462848 model_utils.py:137]          SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared/weights (43629x256, 11.17m params)
I1209 11:52:36.558152 140177781462848 model_utils.py:137]          SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared/bias (43629, 43.63k params)
I1209 11:52:36.558198 140177781462848 model_utils.py:132]    SequenceToSequence/LightConvolutionEncoder (--/3.68m params)
I1209 11:52:36.558244 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_0 (--/727.04k params)
I1209 11:52:36.558301 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper (--/200.96k params)
I1209 11:52:36.558348 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv (--/200.45k params)
I1209 11:52:36.558394 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.558443 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.558489 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.558535 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.558583 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.558629 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.558675 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/3.07k params)
I1209 11:52:36.558722 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x12, 3.07k params)
I1209 11:52:36.558771 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.558818 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.558864 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.558910 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.558956 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.559000 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.559047 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.559094 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.559139 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.559186 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.559232 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.559277 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.559329 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.559381 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.559428 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_1 (--/731.14k params)
I1209 11:52:36.559473 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper (--/205.06k params)
I1209 11:52:36.559518 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv (--/204.54k params)
I1209 11:52:36.559563 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.559610 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.559656 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.559702 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.559749 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.559795 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.559841 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/7.17k params)
I1209 11:52:36.559888 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x28, 7.17k params)
I1209 11:52:36.559934 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.559980 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.560026 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.560072 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.560117 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.560163 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.560209 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.560256 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.560307 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.560356 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.560402 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.560451 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.560498 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.560544 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.560591 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_2 (--/739.33k params)
I1209 11:52:36.560636 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper (--/213.25k params)
I1209 11:52:36.560682 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv (--/212.74k params)
I1209 11:52:36.560727 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.560774 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.560820 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.560866 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.560913 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.560958 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.561004 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/15.36k params)
I1209 11:52:36.561052 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x60, 15.36k params)
I1209 11:52:36.561097 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.561143 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.561188 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.561234 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.561280 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.561330 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.561377 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.561424 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.561470 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.561520 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.561567 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.561612 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.561657 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.561703 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.561749 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_3 (--/739.33k params)
I1209 11:52:36.561794 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper (--/213.25k params)
I1209 11:52:36.561839 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv (--/212.74k params)
I1209 11:52:36.561884 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.561931 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.561977 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.562022 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.562069 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.562115 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.562160 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/15.36k params)
I1209 11:52:36.562207 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x60, 15.36k params)
I1209 11:52:36.562252 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.562303 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.562349 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.562395 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.562440 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.562486 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.562533 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.562582 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.562628 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.562675 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.562721 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.562767 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.562812 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.562857 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.562904 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_4 (--/739.33k params)
I1209 11:52:36.562949 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper (--/213.25k params)
I1209 11:52:36.562994 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv (--/212.74k params)
I1209 11:52:36.563040 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.563086 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.563133 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.563179 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.563226 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.563271 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.563321 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/15.36k params)
I1209 11:52:36.563369 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x60, 15.36k params)
I1209 11:52:36.563415 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.563461 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.563506 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.563552 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.563597 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.563642 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.563692 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.563739 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.563784 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.563831 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.563876 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.563921 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.563967 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.564013 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.564058 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/output_ln (--/512 params)
I1209 11:52:36.564104 140177781462848 model_utils.py:137]        SequenceToSequence/LightConvolutionEncoder/output_ln/gamma (256, 256 params)
I1209 11:52:36.564149 140177781462848 model_utils.py:137]        SequenceToSequence/LightConvolutionEncoder/output_ln/beta (256, 256 params)
I1209 11:52:36.564195 140177781462848 model_utils.py:132]    SequenceToSequence/LightConvolutionDecoder (--/5.00m params)
I1209 11:52:36.564240 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_0 (--/990.72k params)
I1209 11:52:36.564285 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper (--/200.96k params)
I1209 11:52:36.564335 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv (--/200.45k params)
I1209 11:52:36.564380 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.564427 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.564473 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.564518 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.564565 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.564611 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.564656 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/3.07k params)
I1209 11:52:36.564703 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x12, 3.07k params)
I1209 11:52:36.564752 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.564798 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.564844 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.564889 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 11:52:36.564935 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 11:52:36.564980 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 11:52:36.565027 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.565073 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 11:52:36.565118 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 11:52:36.565165 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.565210 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 11:52:36.565255 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 11:52:36.565307 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 11:52:36.565354 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 11:52:36.565398 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.565443 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.565489 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.565535 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.565580 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.565626 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.565672 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.565719 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.565766 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.565813 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.565859 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.565904 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.565949 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.565994 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.566040 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_1 (--/994.82k params)
I1209 11:52:36.566086 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper (--/205.06k params)
I1209 11:52:36.566131 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv (--/204.54k params)
I1209 11:52:36.566176 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.566224 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.566269 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.566319 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.566367 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.566412 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.566458 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/7.17k params)
I1209 11:52:36.566505 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x28, 7.17k params)
I1209 11:52:36.566550 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.566596 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.566642 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.566688 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 11:52:36.566733 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 11:52:36.566778 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 11:52:36.566828 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.566874 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 11:52:36.566920 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 11:52:36.566967 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.567013 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 11:52:36.567058 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 11:52:36.567105 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 11:52:36.567151 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 11:52:36.567196 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.567242 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.567288 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.567339 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.567384 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.567430 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.567476 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.567524 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.567568 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.567616 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.567661 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.567706 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.567752 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.567797 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.567846 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_2 (--/1.00m params)
I1209 11:52:36.567892 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper (--/213.25k params)
I1209 11:52:36.567938 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv (--/212.74k params)
I1209 11:52:36.567983 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.568030 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.568076 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.568122 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.568169 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.568214 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.568259 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/15.36k params)
I1209 11:52:36.568311 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x60, 15.36k params)
I1209 11:52:36.568357 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.568403 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.568449 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.568495 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 11:52:36.568540 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 11:52:36.568585 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 11:52:36.568632 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.568678 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 11:52:36.568723 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 11:52:36.568771 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.568817 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 11:52:36.568865 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 11:52:36.568912 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 11:52:36.568957 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 11:52:36.569003 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.569048 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.569094 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.569139 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.569185 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.569229 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.569276 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.569328 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.569373 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.569421 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.569467 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.569512 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.569558 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.569604 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.569650 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_3 (--/1.00m params)
I1209 11:52:36.569695 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper (--/213.25k params)
I1209 11:52:36.569740 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv (--/212.74k params)
I1209 11:52:36.569785 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.569832 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.569878 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.569925 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.569972 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.570018 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.570064 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/15.36k params)
I1209 11:52:36.570111 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x60, 15.36k params)
I1209 11:52:36.570156 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.570202 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.570247 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.570297 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 11:52:36.570343 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 11:52:36.570389 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 11:52:36.570436 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.570482 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 11:52:36.570528 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 11:52:36.570575 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.570621 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 11:52:36.570666 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 11:52:36.570713 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 11:52:36.570759 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 11:52:36.570804 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.570850 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.570899 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.570945 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.570990 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.571035 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.571083 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.571130 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.571176 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.571223 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.571269 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.571318 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.571364 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.571409 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.571455 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_4 (--/1.00m params)
I1209 11:52:36.571501 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper (--/213.25k params)
I1209 11:52:36.571546 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv (--/212.74k params)
I1209 11:52:36.571591 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 11:52:36.571638 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 11:52:36.571683 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 11:52:36.571728 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 11:52:36.571776 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 11:52:36.571821 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 11:52:36.571867 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/conv_weight_linear (--/15.36k params)
I1209 11:52:36.571913 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/conv_weight_linear/kernel (256x60, 15.36k params)
I1209 11:52:36.571961 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.572007 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.572052 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.572098 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 11:52:36.572143 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 11:52:36.572188 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 11:52:36.572235 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.572280 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 11:52:36.572330 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 11:52:36.572378 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 11:52:36.572424 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 11:52:36.572469 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 11:52:36.572516 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 11:52:36.572562 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 11:52:36.572608 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.572654 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.572699 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.572744 140177781462848 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper (--/526.08k params)
I1209 11:52:36.572790 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 11:52:36.572835 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 11:52:36.572883 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 11:52:36.572930 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 11:52:36.572978 140177781462848 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 11:52:36.573025 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 11:52:36.573071 140177781462848 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 11:52:36.573117 140177781462848 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ln (--/512 params)
I1209 11:52:36.573163 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 11:52:36.573209 140177781462848 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 11:52:36.573254 140177781462848 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/output_ln (--/512 params)
I1209 11:52:36.573305 140177781462848 model_utils.py:137]        SequenceToSequence/LightConvolutionDecoder/output_ln/gamma (256, 256 params)
I1209 11:52:36.573352 140177781462848 model_utils.py:137]        SequenceToSequence/LightConvolutionDecoder/output_ln/beta (256, 256 params)
I1209 11:52:36.591849 140177781462848 checkpoints.py:166] Creates checkpoint manager for directory: ./wmt14_en_de/benchmark_dy-1209
I1209 11:52:36.598577 140177781462848 registry.py:39] Creating criterion: <class 'neurst.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropy'>
I1209 11:52:36.598665 140177781462848 registry.py:41]   (criterion) arguments: 
I1209 11:52:36.598718 140177781462848 registry.py:51]     label_smoothing: 0.0
I1209 11:52:36.598817 140177781462848 registry.py:39] Creating dataset: <class 'neurst.data.datasets.parallel_text_dataset.ParallelTextDataset'>
I1209 11:52:36.598864 140177781462848 registry.py:41]   (dataset) arguments: 
I1209 11:52:36.598908 140177781462848 registry.py:51]     src_file: /root/neurst/wmt14_en_de/newstest2013.en.txt
I1209 11:52:36.598950 140177781462848 registry.py:51]     trg_file: /root/neurst/wmt14_en_de/newstest2013.de.txt
I1209 11:52:36.598992 140177781462848 registry.py:51]     raw_trg_file: None
I1209 11:52:36.599034 140177781462848 registry.py:51]     data_is_processed: None
I1209 11:52:36.599076 140177781462848 registry.py:51]     src_lang: None
I1209 11:52:36.599117 140177781462848 registry.py:51]     trg_lang: None
I1209 11:52:38.431036 140177781462848 seq2seq.py:231] Creating evaluation dataset.
I1209 11:52:38.451660 140177781462848 dataset_utils.py:450] The global batch size is 64 samples.
I1209 11:52:38.454468 140177781462848 registry.py:39] Creating search_method: <class 'neurst.layers.search.beam_search.BeamSearch'>
I1209 11:52:38.454582 140177781462848 registry.py:41]   (search_method) arguments: 
I1209 11:52:38.454634 140177781462848 registry.py:51]     beam_size: 4
I1209 11:52:38.454685 140177781462848 registry.py:51]     length_penalty: 0.6
I1209 11:52:38.454730 140177781462848 registry.py:51]     top_k: 1
I1209 11:52:38.454772 140177781462848 registry.py:51]     maximum_decode_length: 160
I1209 11:52:38.454814 140177781462848 registry.py:51]     minimum_decode_length: 0
I1209 11:52:38.454855 140177781462848 registry.py:51]     extra_decode_length: 50
I1209 11:52:38.454898 140177781462848 registry.py:51]     padded_decode: None
I1209 11:52:38.454939 140177781462848 registry.py:51]     ensemble_weights: average
I1209 11:52:38.454981 140177781462848 registry.py:51]     enable_unk: None
I1209 11:52:42.768721 140177781462848 seq2seq.py:223] Creating test dataset.
I1209 11:52:42.768924 140177781462848 dataset_utils.py:450] The global batch size is 64 samples.
I1209 11:52:42.831766 140177781462848 checkpoints.py:209] Creates custom keep-best checkpoint manager for directory: ./wmt14_en_de/benchmark_dy-1209/best
I1209 11:52:42.832657 140177781462848 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_dy-1209/best
I1209 11:52:44.056097 140177781462848 checkpoints.py:271] Create checkpoint manager for averaged checkpoint of the latest 10 checkpoints to dir: ./wmt14_en_de/benchmark_dy-1209/best_avg
I1209 11:52:44.057412 140177781462848 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_dy-1209/best_avg
I1209 11:52:44.071084 140177781462848 trainer.py:306] Training for 100000 steps.
I1209 11:52:44.636514 140177781462848 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_dy-1209
W1209 11:52:44.982367 140177781462848 deprecation.py:323] From /root/.local/conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
I1209 12:04:21.149616 140177781462848 callbacks.py:220] Update 1000	TrainingLoss=6.78	Speed 0.696 secs/step 1.4 steps/sec
I1209 12:04:21.153671 140177781462848 callbacks.py:238] {'step': 1000, 'lr': 0.0003803629, 'loss': 6.7782135009765625, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 2919.4646563191973, 'src_real_tokens_per_step': 1652.174, 'src_real_tokens_per_sec': 2372.6775274430443, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 2919.4646563191973, 'trg_real_tokens_per_step': 1689.269, 'trg_real_tokens_per_sec': 2425.949442435351, 'samples_per_step': 55.584, 'samples_per_sec': 79.82386097674589, 'this_step_loss': 192353.375}
I1209 12:06:42.623111 140177781462848 callbacks.py:220] Update 2000	TrainingLoss=6.09	Speed 0.141 secs/step 7.1 steps/sec
I1209 12:06:42.626508 140177781462848 callbacks.py:238] {'step': 2000, 'lr': 0.0007607258, 'loss': 6.085464954376221, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 14386.039555345864, 'src_real_tokens_per_step': 1653.658, 'src_real_tokens_per_sec': 11701.163840278143, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 14386.039555345864, 'trg_real_tokens_per_step': 1688.407, 'trg_real_tokens_per_sec': 11947.045239144067, 'samples_per_step': 55.958, 'samples_per_sec': 395.9547416541294, 'this_step_loss': 165958.96875}
I1209 12:06:43.285969 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-2000	Elapsed 0.65s
I1209 12:09:16.985883 140177781462848 callbacks.py:220] Update 3000	TrainingLoss=5.63	Speed 0.154 secs/step 6.5 steps/sec
I1209 12:09:16.989781 140177781462848 callbacks.py:238] {'step': 3000, 'lr': 0.0011410887, 'loss': 5.632680416107178, 'src_tokens_per_step': 2033.16, 'src_tokens_per_sec': 13234.32111441502, 'src_real_tokens_per_step': 1651.425, 'src_real_tokens_per_sec': 10749.517375107136, 'trg_tokens_per_step': 2033.16, 'trg_tokens_per_sec': 13234.32111441502, 'trg_real_tokens_per_step': 1689.757, 'trg_real_tokens_per_sec': 10999.029463165998, 'samples_per_step': 55.79, 'samples_per_sec': 363.15035460721924, 'this_step_loss': 304709.28125}
I1209 12:12:07.742333 140177781462848 callbacks.py:220] Update 4000	TrainingLoss=5.29	Speed 0.171 secs/step 5.9 steps/sec
I1209 12:12:07.745100 140177781462848 callbacks.py:238] {'step': 4000, 'lr': 0.0009882118, 'loss': 5.285006046295166, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 11911.334811153918, 'src_real_tokens_per_step': 1650.945, 'src_real_tokens_per_sec': 9673.181374032683, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 11911.334811153918, 'trg_real_tokens_per_step': 1688.576, 'trg_real_tokens_per_sec': 9893.668118464644, 'samples_per_step': 56.014, 'samples_per_sec': 328.19602196624766, 'this_step_loss': 268638.3125}
I1209 12:12:08.406923 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-4000	Elapsed 0.66s
I1209 12:15:01.543349 140177781462848 callbacks.py:220] Update 5000	TrainingLoss=5.01	Speed 0.173 secs/step 5.8 steps/sec
I1209 12:15:01.546072 140177781462848 callbacks.py:238] {'step': 5000, 'lr': 0.0008838835, 'loss': 5.009427547454834, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 11747.081933271102, 'src_real_tokens_per_step': 1650.244, 'src_real_tokens_per_sec': 9535.440963054125, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 11747.081933271102, 'trg_real_tokens_per_step': 1689.368, 'trg_real_tokens_per_sec': 9761.507285512216, 'samples_per_step': 55.759, 'samples_per_sec': 322.1866903675668, 'this_step_loss': 527422.125}
I1209 12:17:54.853042 140177781462848 callbacks.py:220] Update 6000	TrainingLoss=4.79	Speed 0.173 secs/step 5.8 steps/sec
I1209 12:17:54.854565 140177781462848 callbacks.py:238] {'step': 6000, 'lr': 0.00080687157, 'loss': 4.7905073165893555, 'src_tokens_per_step': 2032.928, 'src_tokens_per_sec': 11735.40043153349, 'src_real_tokens_per_step': 1651.805, 'src_real_tokens_per_sec': 9535.307256237887, 'trg_tokens_per_step': 2032.928, 'trg_tokens_per_sec': 11735.40043153349, 'trg_real_tokens_per_step': 1688.948, 'trg_real_tokens_per_sec': 9749.721135248088, 'samples_per_step': 55.921, 'samples_per_sec': 322.81287263089706, 'this_step_loss': 537061.75}
I1209 12:17:55.472426 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-6000	Elapsed 0.61s
I1209 12:20:50.761801 140177781462848 callbacks.py:220] Update 7000	TrainingLoss=4.62	Speed 0.175 secs/step 5.7 steps/sec
I1209 12:20:50.764559 140177781462848 callbacks.py:238] {'step': 7000, 'lr': 0.0007470179, 'loss': 4.61689567565918, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 11603.223005430626, 'src_real_tokens_per_step': 1648.645, 'src_real_tokens_per_sec': 9409.170122074967, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 11603.223005430626, 'trg_real_tokens_per_step': 1690.892, 'trg_real_tokens_per_sec': 9650.28279954483, 'samples_per_step': 55.928, 'samples_per_sec': 319.1930746688394, 'this_step_loss': 960327.75}
I1209 12:23:50.641369 140177781462848 callbacks.py:220] Update 8000	TrainingLoss=4.47	Speed 0.180 secs/step 5.6 steps/sec
I1209 12:23:50.644638 140177781462848 callbacks.py:238] {'step': 8000, 'lr': 0.00069877127, 'loss': 4.473597049713135, 'src_tokens_per_step': 2032.984, 'src_tokens_per_sec': 11306.937198364867, 'src_real_tokens_per_step': 1651.918, 'src_real_tokens_per_sec': 9187.545540372425, 'trg_tokens_per_step': 2032.984, 'trg_tokens_per_sec': 11306.937198364867, 'trg_real_tokens_per_step': 1689.622, 'trg_real_tokens_per_sec': 9397.245547911663, 'samples_per_step': 55.829, 'samples_per_sec': 310.50662319403995, 'this_step_loss': 933296.8125}
I1209 12:23:53.379140 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-8000	Elapsed 2.73s
I1209 12:26:45.239185 140177781462848 callbacks.py:220] Update 9000	TrainingLoss=4.35	Speed 0.172 secs/step 5.8 steps/sec
I1209 12:26:45.597196 140177781462848 callbacks.py:238] {'step': 9000, 'lr': 0.0006588078, 'loss': 4.35371208190918, 'src_tokens_per_step': 2032.896, 'src_tokens_per_sec': 11833.875424077152, 'src_real_tokens_per_step': 1650.892, 'src_real_tokens_per_sec': 9610.157266582048, 'trg_tokens_per_step': 2032.896, 'trg_tokens_per_sec': 11833.875424077152, 'trg_real_tokens_per_step': 1686.96, 'trg_real_tokens_per_sec': 9820.11597514147, 'samples_per_step': 55.944, 'samples_per_sec': 325.6606962306838, 'this_step_loss': 1014397.4375}
I1209 12:29:47.695599 140177781462848 callbacks.py:220] Update 10000	TrainingLoss=4.25	Speed 0.182 secs/step 5.5 steps/sec
I1209 12:29:47.698689 140177781462848 callbacks.py:238] {'step': 10000, 'lr': 0.000625, 'loss': 4.253418445587158, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 11169.325572564763, 'src_real_tokens_per_step': 1650.865, 'src_real_tokens_per_sec': 9069.800071102307, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 11169.325572564763, 'trg_real_tokens_per_step': 1687.753, 'trg_real_tokens_per_sec': 9272.46157584244, 'samples_per_step': 55.714, 'samples_per_sec': 306.09095302244214, 'this_step_loss': 963272.25}
I1209 12:29:49.209658 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-10000	Elapsed 1.51s
I1209 12:30:22.945558 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=95.56 (Best 95.56)  step=10000	Elapsed 33.74s  FromSTART 2264.49s
I1209 12:30:22.947067 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=11.50 (Best 11.50)  step=10000	Elapsed 33.74s  FromSTART 2264.49s
I1209 12:35:09.341755 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 12:35:09.344122 140177781462848 seq_generation_validator.py:179] Sample 1954
I1209 12:35:09.344241 140177781462848 seq_generation_validator.py:181]   Data: This is at least suggested by one newly added article: in all issues affecting Sharia law, the Al Ashar University must be consulted, the country's most important Islamic institution, which has great influence throughout the whole of Sunni Islam.
I1209 12:35:09.344307 140177781462848 seq_generation_validator.py:182]   Reference: Das legt zumindest ein neu hinzugeführter Artikel nahe: In allen die Scharia betreffenden Angelegenheiten müsse die Al-Ashar-Universität zurate gezogen werden, die wichtigste islamische Institution des Landes mit großer Strahlkraft in den gesamten sunnitischen Islam.
I1209 12:35:09.344362 140177781462848 seq_generation_validator.py:183]   Hypothesis: Das ist zumindest von einem neuen Artikel vorgeschlagen: In allen Themen, die Sharia Gesetz, die Al Ashar Universität konsultiert werden müssen, muss die wichtigste islamische Institution, die sich in der ganzen ganzen Sunni-Islam befinden.
I1209 12:35:09.344411 140177781462848 seq_generation_validator.py:179] Sample 564
I1209 12:35:09.344460 140177781462848 seq_generation_validator.py:181]   Data: For the trip, it is better to take dollars, not euros, because they are easily exchanged for shekels (currently 1 dollar = 3.8 shekels).
I1209 12:35:09.344507 140177781462848 seq_generation_validator.py:182]   Reference: Das Geld in der Reise nimmt man lieber in keinen Euros, sondern in Dollars mit, die einfach in Schekel gewechselt werden können (aktueller Wechselkurs: 1 Dollar zu 3,8 Schekel).
I1209 12:35:09.344554 140177781462848 seq_generation_validator.py:183]   Hypothesis: Für die Reise ist es besser, Dollars zu nehmen, nicht Euro zu nehmen, weil sie sich einfach für die shekels (derzeit 1 Dollar = 3.8 shekels).
I1209 12:35:09.344599 140177781462848 seq_generation_validator.py:179] Sample 741
I1209 12:35:09.344644 140177781462848 seq_generation_validator.py:181]   Data: If Walmart fully shifts the cost of increasing wages to the shoulders of consumers, each visit to the store will cost only 46 cents more.
I1209 12:35:09.344690 140177781462848 seq_generation_validator.py:182]   Reference: Würde Walmart die Lohnerhöhungen vollständig auf die Kunden umlegen, kostete jeder Einkauf sie künftig nur 46 Cent mehr.
I1209 12:35:09.344735 140177781462848 seq_generation_validator.py:183]   Hypothesis: Wenn Walmart die Kosten der Erhöhung der Löhne auf die Schultern der Verbraucher, so wird jeder Besuch auf den Shop nur 46 Klagen kosten.
I1209 12:35:09.344780 140177781462848 seq_generation_validator.py:179] Sample 383
I1209 12:35:09.344825 140177781462848 seq_generation_validator.py:181]   Data: The ECB also highlights the possibilities of money laundering using this anonymous service.
I1209 12:35:09.344869 140177781462848 seq_generation_validator.py:182]   Reference: Die EZB berichtet auch über die Möglichkeiten der Geldwäsche mithilfe dieses anonymen Dienstes.
I1209 12:35:09.344914 140177781462848 seq_generation_validator.py:183]   Hypothesis: Die EZB zeigt auch die Möglichkeiten der Gelder, die mit diesem anonymen Dienst verwendet werden.
I1209 12:35:09.344958 140177781462848 seq_generation_validator.py:179] Sample 1974
I1209 12:35:09.345004 140177781462848 seq_generation_validator.py:181]   Data: Via SMS service Twitter, he accused Mursi's followers of wanting to lead "a coup against democracy."
I1209 12:35:09.345054 140177781462848 seq_generation_validator.py:182]   Reference: Via Kurznachrichtendienst Twitter bezichtigte er die Anhänger Mursis, einen "Putsch gegen die Demokratie" führen zu wollen.
I1209 12:35:09.345099 140177781462848 seq_generation_validator.py:183]   Hypothesis: Die Via BMW-Dienst Twitter, er sagte Mursi 's, "ein Staatsstreich gegen Demokratie" zu führen ".
I1209 12:35:10.458892 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.11s
I1209 12:35:11.402762 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 0.94s
I1209 12:35:11.402951 140177781462848 training_utils.py:359] Evaluating bleu at step=10000 with bad count=0 (early_stop_patience=0).
I1209 12:35:11.403024 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=13.38 (Best 13.38)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:35:11.404129 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=13.47 (Best 13.47)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:35:11.404741 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=13.38 (Best 13.38)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:35:11.405240 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=40.95 (Best 40.95)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:35:11.405746 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=13.78 (Best 13.78)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:35:11.406225 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=13.88 (Best 13.88)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:35:11.406727 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=13.78 (Best 13.78)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:35:11.407441 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=42.31 (Best 42.31)  step=10000	Elapsed 274.87s  FromSTART 2533.75s
I1209 12:38:05.603599 140177781462848 callbacks.py:220] Update 11000	TrainingLoss=4.17	Speed 0.174 secs/step 5.7 steps/sec
I1209 12:38:05.606249 140177781462848 callbacks.py:238] {'step': 11000, 'lr': 0.00059591414, 'loss': 4.1658124923706055, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 11675.811504682646, 'src_real_tokens_per_step': 1650.973, 'src_real_tokens_per_sec': 9481.812789878377, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 11675.811504682646, 'trg_real_tokens_per_step': 1690.334, 'trg_real_tokens_per_sec': 9707.86956562359, 'samples_per_step': 55.928, 'samples_per_sec': 321.20381478819934, 'this_step_loss': 817322.375}
I1209 12:41:04.767615 140177781462848 callbacks.py:220] Update 12000	TrainingLoss=4.09	Speed 0.179 secs/step 5.6 steps/sec
I1209 12:41:04.770723 140177781462848 callbacks.py:238] {'step': 12000, 'lr': 0.00057054433, 'loss': 4.0903401374816895, 'src_tokens_per_step': 2033.024, 'src_tokens_per_sec': 11352.426734447288, 'src_real_tokens_per_step': 1647.188, 'src_real_tokens_per_sec': 9197.914578411646, 'trg_tokens_per_step': 2033.024, 'trg_tokens_per_sec': 11352.426734447288, 'trg_real_tokens_per_step': 1688.254, 'trg_real_tokens_per_sec': 9427.227540913227, 'samples_per_step': 55.985, 'samples_per_sec': 312.62081054037304, 'this_step_loss': 390197.03125}
I1209 12:41:05.570313 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-12000	Elapsed 0.79s
I1209 12:44:07.592179 140177781462848 callbacks.py:220] Update 13000	TrainingLoss=4.02	Speed 0.182 secs/step 5.5 steps/sec
I1209 12:44:07.597030 140177781462848 callbacks.py:238] {'step': 13000, 'lr': 0.0005481613, 'loss': 4.023011207580566, 'src_tokens_per_step': 2033.032, 'src_tokens_per_sec': 11173.71401507303, 'src_real_tokens_per_step': 1652.124, 'src_real_tokens_per_sec': 9080.211769140138, 'trg_tokens_per_step': 2033.032, 'trg_tokens_per_sec': 11173.71401507303, 'trg_real_tokens_per_step': 1689.804, 'trg_real_tokens_per_sec': 9287.304202553853, 'samples_per_step': 55.956, 'samples_per_sec': 307.5388589197939, 'this_step_loss': 496457.65625}
I1209 12:47:02.756660 140177781462848 callbacks.py:220] Update 14000	TrainingLoss=3.96	Speed 0.175 secs/step 5.7 steps/sec
I1209 12:47:02.760060 140177781462848 callbacks.py:238] {'step': 14000, 'lr': 0.0005282214, 'loss': 3.9632835388183594, 'src_tokens_per_step': 2033.152, 'src_tokens_per_sec': 11612.633473752976, 'src_real_tokens_per_step': 1654.684, 'src_real_tokens_per_sec': 9450.960285745223, 'trg_tokens_per_step': 2033.152, 'trg_tokens_per_sec': 11612.633473752976, 'trg_real_tokens_per_step': 1685.324, 'trg_real_tokens_per_sec': 9625.964953195464, 'samples_per_step': 55.951, 'samples_per_sec': 319.5720022359139, 'this_step_loss': 715905.625}
I1209 12:47:03.432984 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-14000	Elapsed 0.67s
I1209 12:50:02.725570 140177781462848 callbacks.py:220] Update 15000	TrainingLoss=3.91	Speed 0.179 secs/step 5.6 steps/sec
I1209 12:50:02.728528 140177781462848 callbacks.py:238] {'step': 15000, 'lr': 0.00051031035, 'loss': 3.9094977378845215, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 11343.299663629905, 'src_real_tokens_per_step': 1651.502, 'src_real_tokens_per_sec': 9214.988608143156, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 11343.299663629905, 'trg_real_tokens_per_step': 1689.692, 'trg_real_tokens_per_sec': 9428.079730615298, 'samples_per_step': 55.966, 'samples_per_sec': 312.2769772263914, 'this_step_loss': 402174.875}
I1209 12:53:03.900496 140177781462848 callbacks.py:220] Update 16000	TrainingLoss=3.86	Speed 0.181 secs/step 5.5 steps/sec
I1209 12:53:03.904153 140177781462848 callbacks.py:238] {'step': 16000, 'lr': 0.0004941059, 'loss': 3.8613202571868896, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 11226.726104069952, 'src_real_tokens_per_step': 1651.122, 'src_real_tokens_per_sec': 9117.543066875965, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 11226.726104069952, 'trg_real_tokens_per_step': 1689.034, 'trg_real_tokens_per_sec': 9326.894218851048, 'samples_per_step': 55.867, 'samples_per_sec': 308.49917723654556, 'this_step_loss': 212178.25}
I1209 12:53:04.542279 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-16000	Elapsed 0.62s
I1209 12:56:06.509950 140177781462848 callbacks.py:220] Update 17000	TrainingLoss=3.82	Speed 0.182 secs/step 5.5 steps/sec
I1209 12:56:06.649932 140177781462848 callbacks.py:238] {'step': 17000, 'lr': 0.00047935313, 'loss': 3.817462682723999, 'src_tokens_per_step': 2033.04, 'src_tokens_per_sec': 11177.060466931072, 'src_real_tokens_per_step': 1653.008, 'src_real_tokens_per_sec': 9087.755463896823, 'trg_tokens_per_step': 2033.04, 'trg_tokens_per_sec': 11177.060466931072, 'trg_real_tokens_per_step': 1687.93, 'trg_real_tokens_per_sec': 9279.746426015703, 'samples_per_step': 55.956, 'samples_per_sec': 307.6297542043418, 'this_step_loss': 187783.671875}
I1209 12:59:07.052647 140177781462848 callbacks.py:220] Update 18000	TrainingLoss=3.78	Speed 0.180 secs/step 5.5 steps/sec
I1209 12:59:07.055791 140177781462848 callbacks.py:238] {'step': 18000, 'lr': 0.0004658475, 'loss': 3.776766061782837, 'src_tokens_per_step': 2032.856, 'src_tokens_per_sec': 11273.527467627442, 'src_real_tokens_per_step': 1651.538, 'src_real_tokens_per_sec': 9158.867626054423, 'trg_tokens_per_step': 2032.856, 'trg_tokens_per_sec': 11273.527467627442, 'trg_real_tokens_per_step': 1689.164, 'trg_real_tokens_per_sec': 9367.528615567184, 'samples_per_step': 55.868, 'samples_per_sec': 309.8249126162453, 'this_step_loss': 427651.40625}
I1209 12:59:07.767037 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-18000	Elapsed 0.71s
I1209 13:02:06.643557 140177781462848 callbacks.py:220] Update 19000	TrainingLoss=3.74	Speed 0.179 secs/step 5.6 steps/sec
I1209 13:02:06.651549 140177781462848 callbacks.py:238] {'step': 19000, 'lr': 0.00045342266, 'loss': 3.7400896549224854, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 11370.204575981752, 'src_real_tokens_per_step': 1651.486, 'src_real_tokens_per_sec': 9236.465162011707, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 11370.204575981752, 'trg_real_tokens_per_step': 1688.274, 'trg_real_tokens_per_sec': 9442.213851604041, 'samples_per_step': 55.975, 'samples_per_sec': 313.0581412398321, 'this_step_loss': 407571.34375}
I1209 13:05:08.533867 140177781462848 callbacks.py:220] Update 20000	TrainingLoss=3.71	Speed 0.182 secs/step 5.5 steps/sec
I1209 13:05:08.537328 140177781462848 callbacks.py:238] {'step': 20000, 'lr': 0.00044194175, 'loss': 3.706540107727051, 'src_tokens_per_step': 2033.048, 'src_tokens_per_sec': 11182.738511423573, 'src_real_tokens_per_step': 1652.616, 'src_real_tokens_per_sec': 9090.180156983397, 'trg_tokens_per_step': 2033.048, 'trg_tokens_per_sec': 11182.738511423573, 'trg_real_tokens_per_step': 1688.95, 'trg_real_tokens_per_sec': 9290.03457314773, 'samples_per_step': 55.698, 'samples_per_sec': 306.3656980107062, 'this_step_loss': 202035.8125}
I1209 13:05:09.372461 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-20000	Elapsed 0.83s
I1209 13:07:30.555107 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=85.51 (Best 85.51)  step=20000	Elapsed 141.18s  FromSTART 4492.10s
I1209 13:07:30.557675 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=8.89 (Best 8.89)  step=20000	Elapsed 141.18s  FromSTART 4492.10s
I1209 13:09:17.031774 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 13:09:17.033990 140177781462848 seq_generation_validator.py:179] Sample 1811
I1209 13:09:17.034154 140177781462848 seq_generation_validator.py:181]   Data: Its similarity with the continent resulted in this Nebula acquiring the title 'North America'.
I1209 13:09:17.034211 140177781462848 seq_generation_validator.py:182]   Reference: Seine Ähnlichkeit mit einem irdischen Kontinent brachte diesem Nebel den Titel Nordamerika ein.
I1209 13:09:17.034263 140177781462848 seq_generation_validator.py:183]   Hypothesis: Seine Ähnlichkeit mit dem Kontinent führte zu diesem Weißen den Titel "Nordamerika".
I1209 13:09:17.034322 140177781462848 seq_generation_validator.py:179] Sample 1701
I1209 13:09:17.034368 140177781462848 seq_generation_validator.py:181]   Data: World AIDS day: Stomp, sing, help
I1209 13:09:17.034415 140177781462848 seq_generation_validator.py:182]   Reference: Weltaidstag: Stampfen, singen, helfen
I1209 13:09:17.034461 140177781462848 seq_generation_validator.py:183]   Hypothesis: World Aids-Tag: Stomp, sing, Hilfe
I1209 13:09:17.034506 140177781462848 seq_generation_validator.py:179] Sample 2125
I1209 13:09:17.034553 140177781462848 seq_generation_validator.py:181]   Data: Manning said he always slept with light from outside his cell in his eyes.
I1209 13:09:17.034598 140177781462848 seq_generation_validator.py:182]   Reference: Stets habe er mit Licht schlafen müssen, das von außen in seine Zelle eindrang.
I1209 13:09:17.034643 140177781462848 seq_generation_validator.py:183]   Hypothesis: Man sagte, dass er immer mit Licht aus außerhalb seiner Zelle in seinen Augen.
I1209 13:09:17.034688 140177781462848 seq_generation_validator.py:179] Sample 1537
I1209 13:09:17.034734 140177781462848 seq_generation_validator.py:181]   Data: The thin textile lasts for two seasons and is then disposed of as bulk waste.
I1209 13:09:17.034778 140177781462848 seq_generation_validator.py:182]   Reference: Das dünne Textil hält ein, zwei Saisonen, danach ist es reif für den Sperrmüll.
I1209 13:09:17.034823 140177781462848 seq_generation_validator.py:183]   Hypothesis: Die dünne Textilzeit beträgt zwei Jahreszeiten und ist dann als Massenverschwendung.
I1209 13:09:17.034867 140177781462848 seq_generation_validator.py:179] Sample 435
I1209 13:09:17.034917 140177781462848 seq_generation_validator.py:181]   Data: The Priority is left to private initiative.
I1209 13:09:17.034962 140177781462848 seq_generation_validator.py:182]   Reference: Der privaten Initiative wird der Vorrang gelassen.
I1209 13:09:17.035009 140177781462848 seq_generation_validator.py:183]   Hypothesis: Die Priperheit bleibt auf private Initiative.
I1209 13:09:17.846706 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.81s
I1209 13:09:18.730468 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 0.88s
I1209 13:09:18.730657 140177781462848 training_utils.py:359] Evaluating bleu at step=20000 with bad count=0 (early_stop_patience=0).
I1209 13:09:18.730727 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=15.74 (Best 15.74)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:09:18.731844 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=15.85 (Best 15.85)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:09:18.732428 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=15.74 (Best 15.74)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:09:18.732903 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=44.34 (Best 44.34)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:09:18.733388 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=16.22 (Best 16.22)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:09:18.733861 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=16.34 (Best 16.34)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:09:18.734340 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=16.22 (Best 16.22)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:09:18.734805 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=45.69 (Best 45.69)  step=20000	Elapsed 95.70s  FromSTART 4582.19s
I1209 13:12:16.373077 140177781462848 callbacks.py:220] Update 21000	TrainingLoss=3.67	Speed 0.178 secs/step 5.6 steps/sec
I1209 13:12:16.400109 140177781462848 callbacks.py:238] {'step': 21000, 'lr': 0.00043129097, 'loss': 3.6743998527526855, 'src_tokens_per_step': 2032.84, 'src_tokens_per_sec': 11448.548311192055, 'src_real_tokens_per_step': 1652.48, 'src_real_tokens_per_sec': 9306.436863343228, 'trg_tokens_per_step': 2032.84, 'trg_tokens_per_sec': 11448.548311192055, 'trg_real_tokens_per_step': 1688.708, 'trg_real_tokens_per_sec': 9510.465713728829, 'samples_per_step': 55.936, 'samples_per_sec': 315.0203647777684, 'this_step_loss': 222335.109375}
I1209 13:15:07.353027 140177781462848 callbacks.py:220] Update 22000	TrainingLoss=3.65	Speed 0.171 secs/step 5.9 steps/sec
I1209 13:15:07.358854 140177781462848 callbacks.py:238] {'step': 22000, 'lr': 0.0004213749, 'loss': 3.6454617977142334, 'src_tokens_per_step': 2033.152, 'src_tokens_per_sec': 11899.622002298323, 'src_real_tokens_per_step': 1651.616, 'src_real_tokens_per_sec': 9666.569982444966, 'trg_tokens_per_step': 2033.152, 'trg_tokens_per_sec': 11899.622002298323, 'trg_real_tokens_per_step': 1690.204, 'trg_real_tokens_per_sec': 9892.41763860874, 'samples_per_step': 56.121, 'samples_per_sec': 328.4647121272705, 'this_step_loss': 412638.125}
I1209 13:15:08.089772 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-22000	Elapsed 0.73s
I1209 13:18:06.320650 140177781462848 callbacks.py:220] Update 23000	TrainingLoss=3.62	Speed 0.178 secs/step 5.6 steps/sec
I1209 13:18:06.324236 140177781462848 callbacks.py:238] {'step': 23000, 'lr': 0.00041211277, 'loss': 3.6186273097991943, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 11410.941003629381, 'src_real_tokens_per_step': 1650.164, 'src_real_tokens_per_sec': 9262.428355006294, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 11410.941003629381, 'trg_real_tokens_per_step': 1689.18, 'trg_real_tokens_per_sec': 9481.426530156717, 'samples_per_step': 55.675, 'samples_per_sec': 312.50572589450223, 'this_step_loss': 369058.84375}
I1209 13:21:08.118947 140177781462848 callbacks.py:220] Update 24000	TrainingLoss=3.59	Speed 0.182 secs/step 5.5 steps/sec
I1209 13:21:08.124522 140177781462848 callbacks.py:238] {'step': 24000, 'lr': 0.00040343578, 'loss': 3.593132734298706, 'src_tokens_per_step': 2033.032, 'src_tokens_per_sec': 11188.088154404706, 'src_real_tokens_per_step': 1652.608, 'src_real_tokens_per_sec': 9094.556302446028, 'trg_tokens_per_step': 2033.032, 'trg_tokens_per_sec': 11188.088154404706, 'trg_real_tokens_per_step': 1686.52, 'trg_real_tokens_per_sec': 9281.179260418245, 'samples_per_step': 55.998, 'samples_per_sec': 308.16561690635206, 'this_step_loss': 324930.53125}
I1209 13:21:09.046537 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-24000	Elapsed 0.92s
I1209 13:24:06.353994 140177781462848 callbacks.py:220] Update 25000	TrainingLoss=3.57	Speed 0.177 secs/step 5.6 steps/sec
I1209 13:24:06.357101 140177781462848 callbacks.py:238] {'step': 25000, 'lr': 0.00039528473, 'loss': 3.569502115249634, 'src_tokens_per_step': 2032.864, 'src_tokens_per_sec': 11470.193363966202, 'src_real_tokens_per_step': 1654.028, 'src_real_tokens_per_sec': 9332.656286605641, 'trg_tokens_per_step': 2032.864, 'trg_tokens_per_sec': 11470.193363966202, 'trg_real_tokens_per_step': 1686.928, 'trg_real_tokens_per_sec': 9518.290624010646, 'samples_per_step': 55.844, 'samples_per_sec': 315.0931288159604, 'this_step_loss': 347424.78125}
I1209 13:27:07.574201 140177781462848 callbacks.py:220] Update 26000	TrainingLoss=3.55	Speed 0.181 secs/step 5.5 steps/sec
I1209 13:27:07.578392 140177781462848 callbacks.py:238] {'step': 26000, 'lr': 0.00038760857, 'loss': 3.5473833084106445, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 11223.412556362693, 'src_real_tokens_per_step': 1652.84, 'src_real_tokens_per_sec': 9124.731041567558, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 11223.412556362693, 'trg_real_tokens_per_step': 1688.652, 'trg_real_tokens_per_sec': 9322.436123765845, 'samples_per_step': 55.713, 'samples_per_sec': 307.57129578111216, 'this_step_loss': 766300.8125}
I1209 13:27:08.437753 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-26000	Elapsed 0.85s
I1209 13:30:07.221544 140177781462848 callbacks.py:220] Update 27000	TrainingLoss=3.53	Speed 0.179 secs/step 5.6 steps/sec
I1209 13:30:07.224400 140177781462848 callbacks.py:238] {'step': 27000, 'lr': 0.00038036288, 'loss': 3.5261919498443604, 'src_tokens_per_step': 2033.024, 'src_tokens_per_sec': 11376.036527993623, 'src_real_tokens_per_step': 1653.868, 'src_real_tokens_per_sec': 9254.422368097847, 'trg_tokens_per_step': 2033.024, 'trg_tokens_per_sec': 11376.036527993623, 'trg_real_tokens_per_step': 1690.04, 'trg_real_tokens_per_sec': 9456.82725524654, 'samples_per_step': 55.955, 'samples_per_sec': 313.10310351667425, 'this_step_loss': 377366.21875}
I1209 13:33:05.233102 140177781462848 callbacks.py:220] Update 28000	TrainingLoss=3.51	Speed 0.178 secs/step 5.6 steps/sec
I1209 13:33:05.236672 140177781462848 callbacks.py:238] {'step': 28000, 'lr': 0.00037350896, 'loss': 3.506227731704712, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 11425.993763057677, 'src_real_tokens_per_step': 1651.12, 'src_real_tokens_per_sec': 9279.654868461337, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 11425.993763057677, 'trg_real_tokens_per_step': 1688.58, 'trg_real_tokens_per_sec': 9490.188246636491, 'samples_per_step': 55.723, 'samples_per_sec': 313.17542530843974, 'this_step_loss': 364219.25}
I1209 13:33:07.192856 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-28000	Elapsed 1.95s
I1209 13:36:04.579096 140177781462848 callbacks.py:220] Update 29000	TrainingLoss=3.49	Speed 0.177 secs/step 5.6 steps/sec
I1209 13:36:04.611116 140177781462848 callbacks.py:238] {'step': 29000, 'lr': 0.00036701263, 'loss': 3.4877676963806152, 'src_tokens_per_step': 2032.888, 'src_tokens_per_sec': 11465.019217663112, 'src_real_tokens_per_step': 1653.516, 'src_real_tokens_per_sec': 9325.448680258547, 'trg_tokens_per_step': 2032.888, 'trg_tokens_per_sec': 11465.019217663112, 'trg_real_tokens_per_step': 1685.924, 'trg_real_tokens_per_sec': 9508.222321898435, 'samples_per_step': 55.619, 'samples_per_sec': 313.6783255482863, 'this_step_loss': 693057.0}
I1209 13:39:07.709053 140177781462848 callbacks.py:220] Update 30000	TrainingLoss=3.47	Speed 0.183 secs/step 5.5 steps/sec
I1209 13:39:07.712580 140177781462848 callbacks.py:238] {'step': 30000, 'lr': 0.00036084393, 'loss': 3.4697227478027344, 'src_tokens_per_step': 2032.96, 'src_tokens_per_sec': 11108.094699186333, 'src_real_tokens_per_step': 1651.564, 'src_real_tokens_per_sec': 9024.146718955108, 'trg_tokens_per_step': 2032.96, 'trg_tokens_per_sec': 11108.094699186333, 'trg_real_tokens_per_step': 1686.668, 'trg_real_tokens_per_sec': 9215.954996698023, 'samples_per_step': 55.93, 'samples_per_sec': 305.60155464224164, 'this_step_loss': 404214.375}
I1209 13:39:08.364387 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-30000	Elapsed 0.65s
I1209 13:39:16.416287 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=81.62 (Best 81.62)  step=30000	Elapsed 8.05s  FromSTART 6397.96s
I1209 13:39:16.417714 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=8.05 (Best 8.05)  step=30000	Elapsed 8.05s  FromSTART 6397.96s
I1209 13:40:54.292609 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 13:40:54.294714 140177781462848 seq_generation_validator.py:179] Sample 313
I1209 13:40:54.294823 140177781462848 seq_generation_validator.py:181]   Data: Based on the amounts invested, you should be 15 points ahead at the winter break!
I1209 13:40:54.294878 140177781462848 seq_generation_validator.py:182]   Reference: Im Hinblick auf die investierten Summen glaubte man euch zur Spielpause mit 15 Punkten Vorsprung!
I1209 13:40:54.294926 140177781462848 seq_generation_validator.py:183]   Hypothesis: Auf der Grundlage der investiert, sollten Sie 15 Punkte vor dem Winterbruch sein!
I1209 13:40:54.294971 140177781462848 seq_generation_validator.py:179] Sample 814
I1209 13:40:54.295017 140177781462848 seq_generation_validator.py:181]   Data: Speed, the number of images per second, is also very important.
I1209 13:40:54.295062 140177781462848 seq_generation_validator.py:182]   Reference: Auch die Geschwindigkeit und die Anzahl der Bilder pro Sekunde sind wichtig.
I1209 13:40:54.295106 140177781462848 seq_generation_validator.py:183]   Hypothesis: Die Anzahl der Bilder pro Sekunde ist ebenfalls sehr wichtig.
I1209 13:40:54.295150 140177781462848 seq_generation_validator.py:179] Sample 775
I1209 13:40:54.295195 140177781462848 seq_generation_validator.py:181]   Data: However, according to the conversation with the leader of the group of interactive 3D technologies in the Cambridge laboratory of Microsoft, Shahram Izadi, glasses are a thing of the past for scientists in this company.
I1209 13:40:54.295240 140177781462848 seq_generation_validator.py:182]   Reference: Im Gespräch mit dem Leiter der Gruppe für interaktive 3D-Technologien im Microsoft-Labor in Cambridge Shahram Izadi, wird allerdings deutlich, dass die Brille für die Wissenschaftler des Unternehmens nur eine Durchgangsstation war.
I1209 13:40:54.295288 140177781462848 seq_generation_validator.py:183]   Hypothesis: Doch nach dem Gespräch mit dem Vorsitzenden der Gruppe von interaktiver 3D-Technologien im Labor von Microsoft, Shahdale Izadi, Gläser ist eine Sache der Vergangenheit für Wissenschaftler in diesem Unternehmen.
I1209 13:40:54.295341 140177781462848 seq_generation_validator.py:179] Sample 1239
I1209 13:40:54.295387 140177781462848 seq_generation_validator.py:181]   Data: Poland's admission to COSPAR (Committee for Space Research) in 1960 should be mentioned, as well as the appointment of a national COSPAR board two years later.
I1209 13:40:54.295438 140177781462848 seq_generation_validator.py:182]   Reference: Erwähnenswert ist auch der Beitritt Polens zum COSPAR (Committee for Space Research) im Jahr 1960, auf den zwei Jahre später die Gründung eines nationalen COSPAR-Ausschusses folgte.
I1209 13:40:54.295484 140177781462848 seq_generation_validator.py:183]   Hypothesis: Die Aufnahme von Polen in COSPAR (Ausschuss für Raumforschung) sollte im Jahr 1950 genannt werden, sowie die Ernennung eines nationalen COSPAR-Board zwei Jahre später.
I1209 13:40:54.295529 140177781462848 seq_generation_validator.py:179] Sample 1911
I1209 13:40:54.295573 140177781462848 seq_generation_validator.py:181]   Data: In contrast, Museum Director Fatulayeva believes that it is due to diet.
I1209 13:40:54.295618 140177781462848 seq_generation_validator.py:182]   Reference: Museumsdirektorin Fatulayeva dagegen glaubt, dass es an der Ernährung liege.
I1209 13:40:54.295662 140177781462848 seq_generation_validator.py:183]   Hypothesis: Im Gegensatz dazu ist Museum Direktor Fatulayeva der Ansicht, dass es aufgrund der Diät ist.
I1209 13:40:57.680943 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 3.38s
I1209 13:40:59.921732 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 2.24s
I1209 13:40:59.922003 140177781462848 training_utils.py:359] Evaluating bleu at step=30000 with bad count=0 (early_stop_patience=0).
I1209 13:40:59.922079 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=16.45 (Best 16.45)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:40:59.924253 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=16.55 (Best 16.55)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:40:59.925077 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=16.45 (Best 16.45)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:40:59.925768 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=45.14 (Best 45.14)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:40:59.926432 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=16.92 (Best 16.92)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:40:59.927183 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=17.03 (Best 17.03)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:40:59.927856 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=16.92 (Best 16.92)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:40:59.928487 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=46.48 (Best 46.48)  step=30000	Elapsed 87.94s  FromSTART 6480.29s
I1209 13:43:56.385669 140177781462848 callbacks.py:220] Update 31000	TrainingLoss=3.45	Speed 0.176 secs/step 5.7 steps/sec
I1209 13:43:56.398392 140177781462848 callbacks.py:238] {'step': 31000, 'lr': 0.00035497616, 'loss': 3.452622890472412, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 11526.091712185284, 'src_real_tokens_per_step': 1651.36, 'src_real_tokens_per_sec': 9362.420909592507, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 11526.091712185284, 'trg_real_tokens_per_step': 1689.58, 'trg_real_tokens_per_sec': 9579.11001866904, 'samples_per_step': 56.063, 'samples_per_sec': 317.85037996226424, 'this_step_loss': 330739.84375}
I1209 13:46:57.275014 140177781462848 callbacks.py:220] Update 32000	TrainingLoss=3.44	Speed 0.181 secs/step 5.5 steps/sec
I1209 13:46:57.277869 140177781462848 callbacks.py:238] {'step': 32000, 'lr': 0.00034938563, 'loss': 3.436649799346924, 'src_tokens_per_step': 2032.968, 'src_tokens_per_sec': 11244.453781240198, 'src_real_tokens_per_step': 1650.292, 'src_real_tokens_per_sec': 9127.852538530095, 'trg_tokens_per_step': 2032.968, 'trg_tokens_per_sec': 11244.453781240198, 'trg_real_tokens_per_step': 1689.66, 'trg_real_tokens_per_sec': 9345.599033536344, 'samples_per_step': 55.93, 'samples_per_sec': 309.3517950035437, 'this_step_loss': 415732.1875}
I1209 13:46:58.111927 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-32000	Elapsed 0.83s
I1209 13:49:58.540040 140177781462848 callbacks.py:220] Update 33000	TrainingLoss=3.42	Speed 0.180 secs/step 5.5 steps/sec
I1209 13:49:58.545315 140177781462848 callbacks.py:238] {'step': 33000, 'lr': 0.00034405116, 'loss': 3.421680212020874, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 11272.340455658441, 'src_real_tokens_per_step': 1652.38, 'src_real_tokens_per_sec': 9161.95928076495, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 11272.340455658441, 'trg_real_tokens_per_step': 1686.908, 'trg_real_tokens_per_sec': 9353.406847333325, 'samples_per_step': 55.821, 'samples_per_sec': 309.51096540237734, 'this_step_loss': 367752.28125}
I1209 13:53:00.553429 140177781462848 callbacks.py:220] Update 34000	TrainingLoss=3.41	Speed 0.182 secs/step 5.5 steps/sec
I1209 13:53:00.556842 140177781462848 callbacks.py:238] {'step': 34000, 'lr': 0.00033895386, 'loss': 3.407047986984253, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 11175.203355140591, 'src_real_tokens_per_step': 1650.52, 'src_real_tokens_per_sec': 9072.319576511216, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 11175.203355140591, 'trg_real_tokens_per_step': 1688.272, 'trg_real_tokens_per_sec': 9279.828851559354, 'samples_per_step': 55.93, 'samples_per_sec': 307.4272556008242, 'this_step_loss': 398286.9375}
I1209 13:53:01.475710 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-34000	Elapsed 0.91s
I1209 13:56:01.733632 140177781462848 callbacks.py:220] Update 35000	TrainingLoss=3.39	Speed 0.180 secs/step 5.5 steps/sec
I1209 13:56:01.741356 140177781462848 callbacks.py:238] {'step': 35000, 'lr': 0.00033407655, 'loss': 3.3929390907287598, 'src_tokens_per_step': 2032.976, 'src_tokens_per_sec': 11282.841531850961, 'src_real_tokens_per_step': 1652.0, 'src_real_tokens_per_sec': 9168.457576782897, 'trg_tokens_per_step': 2032.976, 'trg_tokens_per_sec': 11282.841531850961, 'trg_real_tokens_per_step': 1689.452, 'trg_real_tokens_per_sec': 9376.31294794856, 'samples_per_step': 56.197, 'samples_per_sec': 311.8885051104531, 'this_step_loss': 467254.0625}
I1209 13:59:05.429661 140177781462848 callbacks.py:220] Update 36000	TrainingLoss=3.38	Speed 0.184 secs/step 5.4 steps/sec
I1209 13:59:05.432423 140177781462848 callbacks.py:238] {'step': 36000, 'lr': 0.0003294039, 'loss': 3.379890203475952, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 11072.451496327327, 'src_real_tokens_per_step': 1650.252, 'src_real_tokens_per_sec': 8987.903162785275, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 11072.451496327327, 'trg_real_tokens_per_step': 1685.516, 'trg_real_tokens_per_sec': 9179.964385636367, 'samples_per_step': 55.694, 'samples_per_sec': 303.330811747638, 'this_step_loss': 173392.328125}
I1209 13:59:06.098480 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-36000	Elapsed 0.66s
I1209 14:02:05.962423 140177781462848 callbacks.py:220] Update 37000	TrainingLoss=3.37	Speed 0.180 secs/step 5.6 steps/sec
I1209 14:02:05.965248 140177781462848 callbacks.py:238] {'step': 37000, 'lr': 0.00032492203, 'loss': 3.367147445678711, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 11308.148557296112, 'src_real_tokens_per_step': 1652.804, 'src_real_tokens_per_sec': 9192.95162062846, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 11308.148557296112, 'trg_real_tokens_per_step': 1685.672, 'trg_real_tokens_per_sec': 9375.764545734412, 'samples_per_step': 56.036, 'samples_per_sec': 311.67412289269413, 'this_step_loss': 149434.625}
I1209 14:05:10.336134 140177781462848 callbacks.py:220] Update 38000	TrainingLoss=3.36	Speed 0.184 secs/step 5.4 steps/sec
I1209 14:05:10.339517 140177781462848 callbacks.py:238] {'step': 38000, 'lr': 0.00032061824, 'loss': 3.3551089763641357, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 11031.441199851399, 'src_real_tokens_per_step': 1650.216, 'src_real_tokens_per_sec': 8954.383064955227, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 11031.441199851399, 'trg_real_tokens_per_step': 1688.944, 'trg_real_tokens_per_sec': 9164.528492789877, 'samples_per_step': 56.013, 'samples_per_sec': 303.9370958815919, 'this_step_loss': 342446.0625}
I1209 14:05:11.136748 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-38000	Elapsed 0.79s
I1209 14:08:08.120133 140177781462848 callbacks.py:220] Update 39000	TrainingLoss=3.34	Speed 0.177 secs/step 5.7 steps/sec
I1209 14:08:08.122565 140177781462848 callbacks.py:238] {'step': 39000, 'lr': 0.00031648105, 'loss': 3.343533754348755, 'src_tokens_per_step': 2033.04, 'src_tokens_per_sec': 11491.859204674876, 'src_real_tokens_per_step': 1651.396, 'src_real_tokens_per_sec': 9334.597609079641, 'trg_tokens_per_step': 2033.04, 'trg_tokens_per_sec': 11491.859204674876, 'trg_real_tokens_per_step': 1687.732, 'trg_real_tokens_per_sec': 9539.988647161068, 'samples_per_step': 56.055, 'samples_per_sec': 316.8536613731408, 'this_step_loss': 419966.90625}
I1209 14:11:07.504096 140177781462848 callbacks.py:220] Update 40000	TrainingLoss=3.33	Speed 0.179 secs/step 5.6 steps/sec
I1209 14:11:07.507074 140177781462848 callbacks.py:238] {'step': 40000, 'lr': 0.0003125, 'loss': 3.3324055671691895, 'src_tokens_per_step': 2033.128, 'src_tokens_per_sec': 11339.150529715303, 'src_real_tokens_per_step': 1651.392, 'src_real_tokens_per_sec': 9210.134566819017, 'trg_tokens_per_step': 2033.128, 'trg_tokens_per_sec': 11339.150529715303, 'trg_real_tokens_per_step': 1688.524, 'trg_real_tokens_per_sec': 9417.22695719945, 'samples_per_step': 55.937, 'samples_per_sec': 311.97153508322396, 'this_step_loss': 373076.1875}
I1209 14:11:08.176969 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-40000	Elapsed 0.66s
I1209 14:11:16.247797 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=79.11 (Best 79.11)  step=40000	Elapsed 8.07s  FromSTART 8317.79s
I1209 14:11:16.249415 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.55 (Best 7.55)  step=40000	Elapsed 8.07s  FromSTART 8317.79s
I1209 14:12:57.088132 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 14:12:57.090176 140177781462848 seq_generation_validator.py:179] Sample 1092
I1209 14:12:57.090276 140177781462848 seq_generation_validator.py:181]   Data: "In the Sumperk region, traces of snow have remained at the highest altitudes.
I1209 14:12:57.090342 140177781462848 seq_generation_validator.py:182]   Reference: "Im Kreis Šumperk verbleiben in den Höhenlagen noch Schneereste.
I1209 14:12:57.090393 140177781462848 seq_generation_validator.py:183]   Hypothesis: "In der Sumperk-Region sind Spuren von Schnee auf dem höchsten Algen geblieben.
I1209 14:12:57.090440 140177781462848 seq_generation_validator.py:179] Sample 1395
I1209 14:12:57.090487 140177781462848 seq_generation_validator.py:181]   Data: These ideas did not come from Brussels, however, but from Prague.
I1209 14:12:57.090533 140177781462848 seq_generation_validator.py:182]   Reference: Diese Ideen stammten freilich nicht aus Brüssel, sondern aus Prag.
I1209 14:12:57.090578 140177781462848 seq_generation_validator.py:183]   Hypothesis: Diese Ideen stammen jedoch nicht aus Brüssel, sondern aus Prag.
I1209 14:12:57.090622 140177781462848 seq_generation_validator.py:179] Sample 548
I1209 14:12:57.090668 140177781462848 seq_generation_validator.py:181]   Data: Its azure water is saline and easily keeps you afloat, even if you don't know how to swim.
I1209 14:12:57.090712 140177781462848 seq_generation_validator.py:182]   Reference: Sein türkisblaues Wasser hat einen hohen Salzgehalt und trägt dadurch sogar Nichtschwimmer problemlos auf der Oberfläche.
I1209 14:12:57.090761 140177781462848 seq_generation_validator.py:183]   Hypothesis: Sein Areal-Wasser ist saline und immer immer halten Sie afloat, selbst wenn Sie nicht wissen, wie Sie es nicht wissen können.
I1209 14:12:57.090806 140177781462848 seq_generation_validator.py:179] Sample 1599
I1209 14:12:57.090851 140177781462848 seq_generation_validator.py:181]   Data: After all, it signals that the country must be taken seriously as an international player and that its interests must be considered.
I1209 14:12:57.090895 140177781462848 seq_generation_validator.py:182]   Reference: Denn es signalisiert doch, dass es als internationaler Spieler ernst genommen werden und seine Interessen bedacht werden müssen.
I1209 14:12:57.090939 140177781462848 seq_generation_validator.py:183]   Hypothesis: Schließlich signalisiert es, dass das Land als internationaler Spieler ernst genommen werden muss und dass seine Interessen berücksichtigt werden müssen.
I1209 14:12:57.090984 140177781462848 seq_generation_validator.py:179] Sample 1156
I1209 14:12:57.091029 140177781462848 seq_generation_validator.py:181]   Data: There are a lot of things, which we still lag behind on...
I1209 14:12:57.091073 140177781462848 seq_generation_validator.py:182]   Reference: Es gibt noch viele Dinge, in denen wir hinterher hängen...
I1209 14:12:57.091116 140177781462848 seq_generation_validator.py:183]   Hypothesis: Es gibt viele Dinge, die wir immer noch hinter uns liegen...
I1209 14:12:57.879968 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.79s
I1209 14:12:59.902690 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 2.02s
I1209 14:12:59.902895 140177781462848 training_utils.py:359] Evaluating bleu at step=40000 with bad count=0 (early_stop_patience=0).
I1209 14:12:59.902969 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.12 (Best 17.12)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:12:59.903981 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.32 (Best 17.32)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:12:59.904557 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.12 (Best 17.12)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:12:59.905028 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.08 (Best 46.08)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:12:59.905496 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=17.60 (Best 17.60)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:12:59.905952 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=17.83 (Best 17.83)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:12:59.906423 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=17.60 (Best 17.60)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:12:59.907142 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=47.38 (Best 47.38)  step=40000	Elapsed 90.41s  FromSTART 8402.59s
I1209 14:15:57.356300 140177781462848 callbacks.py:220] Update 41000	TrainingLoss=3.32	Speed 0.177 secs/step 5.6 steps/sec
I1209 14:15:57.444232 140177781462848 callbacks.py:238] {'step': 41000, 'lr': 0.00030866548, 'loss': 3.3214430809020996, 'src_tokens_per_step': 2032.984, 'src_tokens_per_sec': 11461.52066101319, 'src_real_tokens_per_step': 1652.716, 'src_real_tokens_per_sec': 9317.65256430305, 'trg_tokens_per_step': 2032.984, 'trg_tokens_per_sec': 11461.52066101319, 'trg_real_tokens_per_step': 1688.304, 'trg_real_tokens_per_sec': 9518.289951161058, 'samples_per_step': 55.985, 'samples_per_sec': 315.6312269092248, 'this_step_loss': 402471.9375}
I1209 14:18:58.168110 140177781462848 callbacks.py:220] Update 42000	TrainingLoss=3.31	Speed 0.181 secs/step 5.5 steps/sec
I1209 14:18:58.171186 140177781462848 callbacks.py:238] {'step': 42000, 'lr': 0.00030496877, 'loss': 3.311203718185425, 'src_tokens_per_step': 2033.072, 'src_tokens_per_sec': 11254.537138974547, 'src_real_tokens_per_step': 1650.856, 'src_real_tokens_per_sec': 9138.692659728216, 'trg_tokens_per_step': 2033.072, 'trg_tokens_per_sec': 11254.537138974547, 'trg_real_tokens_per_step': 1687.32, 'trg_real_tokens_per_sec': 9340.547509057491, 'samples_per_step': 55.921, 'samples_per_sec': 309.56354292843326, 'this_step_loss': 179072.4375}
I1209 14:18:59.115442 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-42000	Elapsed 0.94s
I1209 14:22:01.653776 140177781462848 callbacks.py:220] Update 43000	TrainingLoss=3.30	Speed 0.182 secs/step 5.5 steps/sec
I1209 14:22:01.656182 140177781462848 callbacks.py:238] {'step': 43000, 'lr': 0.00030140177, 'loss': 3.3015806674957275, 'src_tokens_per_step': 2033.144, 'src_tokens_per_sec': 11142.592515977212, 'src_real_tokens_per_step': 1650.016, 'src_real_tokens_per_sec': 9042.86953252827, 'trg_tokens_per_step': 2033.144, 'trg_tokens_per_sec': 11142.592515977212, 'trg_real_tokens_per_step': 1690.32, 'trg_real_tokens_per_sec': 9263.75455039417, 'samples_per_step': 56.01, 'samples_per_sec': 306.9613400821013, 'this_step_loss': 191626.078125}
I1209 14:25:06.044668 140177781462848 callbacks.py:220] Update 44000	TrainingLoss=3.29	Speed 0.184 secs/step 5.4 steps/sec
I1209 14:25:06.049950 140177781462848 callbacks.py:238] {'step': 44000, 'lr': 0.00029795707, 'loss': 3.291935682296753, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 11030.391979082067, 'src_real_tokens_per_step': 1652.96, 'src_real_tokens_per_sec': 8968.34885989264, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 11030.391979082067, 'trg_real_tokens_per_step': 1687.536, 'trg_real_tokens_per_sec': 9155.945432211176, 'samples_per_step': 56.15, 'samples_per_sec': 304.6491073486181, 'this_step_loss': 354025.25}
I1209 14:25:06.741375 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-44000	Elapsed 0.69s
I1209 14:28:06.323599 140177781462848 callbacks.py:220] Update 45000	TrainingLoss=3.28	Speed 0.180 secs/step 5.6 steps/sec
I1209 14:28:06.328312 140177781462848 callbacks.py:238] {'step': 45000, 'lr': 0.00029462783, 'loss': 3.2829999923706055, 'src_tokens_per_step': 2032.88, 'src_tokens_per_sec': 11324.5449868039, 'src_real_tokens_per_step': 1653.104, 'src_real_tokens_per_sec': 9208.930490666186, 'trg_tokens_per_step': 2032.88, 'trg_tokens_per_sec': 11324.5449868039, 'trg_real_tokens_per_step': 1691.744, 'trg_real_tokens_per_sec': 9424.181965563917, 'samples_per_step': 55.661, 'samples_per_sec': 310.07019524541136, 'this_step_loss': 308256.90625}
I1209 14:31:02.063172 140177781462848 callbacks.py:220] Update 46000	TrainingLoss=3.27	Speed 0.176 secs/step 5.7 steps/sec
I1209 14:31:02.065335 140177781462848 callbacks.py:238] {'step': 46000, 'lr': 0.00029140775, 'loss': 3.2741057872772217, 'src_tokens_per_step': 2033.112, 'src_tokens_per_sec': 11574.170861881195, 'src_real_tokens_per_step': 1649.384, 'src_real_tokens_per_sec': 9389.67072785614, 'trg_tokens_per_step': 2033.112, 'trg_tokens_per_sec': 11574.170861881195, 'trg_real_tokens_per_step': 1688.368, 'trg_real_tokens_per_sec': 9611.600201923275, 'samples_per_step': 56.166, 'samples_per_sec': 319.7437625809199, 'this_step_loss': 148201.59375}
I1209 14:31:02.796601 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-46000	Elapsed 0.73s
I1209 14:34:01.244124 140177781462848 callbacks.py:220] Update 47000	TrainingLoss=3.27	Speed 0.178 secs/step 5.6 steps/sec
I1209 14:34:01.246966 140177781462848 callbacks.py:238] {'step': 47000, 'lr': 0.00028829102, 'loss': 3.2655112743377686, 'src_tokens_per_step': 2033.088, 'src_tokens_per_sec': 11397.743607915923, 'src_real_tokens_per_step': 1653.256, 'src_real_tokens_per_sec': 9268.358283679136, 'trg_tokens_per_step': 2033.088, 'trg_tokens_per_sec': 11397.743607915923, 'trg_real_tokens_per_step': 1687.68, 'trg_real_tokens_per_sec': 9461.343499252145, 'samples_per_step': 56.059, 'samples_per_sec': 314.273710196587, 'this_step_loss': 178537.40625}
I1209 14:36:50.855733 140177781462848 callbacks.py:220] Update 48000	TrainingLoss=3.26	Speed 0.170 secs/step 5.9 steps/sec
I1209 14:36:50.859319 140177781462848 callbacks.py:238] {'step': 48000, 'lr': 0.00028527217, 'loss': 3.2573306560516357, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 11991.095906608287, 'src_real_tokens_per_step': 1652.28, 'src_real_tokens_per_sec': 9745.90635370341, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 11991.095906608287, 'trg_real_tokens_per_step': 1691.552, 'trg_real_tokens_per_sec': 9977.550647844017, 'samples_per_step': 55.69, 'samples_per_sec': 328.4851991416364, 'this_step_loss': 150244.609375}
I1209 14:36:51.606612 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-48000	Elapsed 0.74s
I1209 14:39:38.640842 140177781462848 callbacks.py:220] Update 49000	TrainingLoss=3.25	Speed 0.167 secs/step 6.0 steps/sec
I1209 14:39:38.644480 140177781462848 callbacks.py:238] {'step': 49000, 'lr': 0.0002823462, 'loss': 3.249342679977417, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 12176.178158797069, 'src_real_tokens_per_step': 1648.272, 'src_real_tokens_per_sec': 9871.978604026363, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 12176.178158797069, 'trg_real_tokens_per_step': 1690.448, 'trg_real_tokens_per_sec': 10124.58288875814, 'samples_per_step': 55.951, 'samples_per_sec': 335.10675111503383, 'this_step_loss': 168594.34375}
I1209 14:42:28.861588 140177781462848 callbacks.py:220] Update 50000	TrainingLoss=3.24	Speed 0.170 secs/step 5.9 steps/sec
I1209 14:42:28.864045 140177781462848 callbacks.py:238] {'step': 50000, 'lr': 0.0002795085, 'loss': 3.2413699626922607, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 11949.545431200753, 'src_real_tokens_per_step': 1650.904, 'src_real_tokens_per_sec': 9703.283860227364, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 11949.545431200753, 'trg_real_tokens_per_step': 1688.304, 'trg_real_tokens_per_sec': 9923.10452597928, 'samples_per_step': 56.031, 'samples_per_sec': 329.3254471322375, 'this_step_loss': 204366.3125}
I1209 14:42:29.649429 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-50000	Elapsed 0.78s
I1209 14:42:37.620097 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=77.62 (Best 77.62)  step=50000	Elapsed 7.97s  FromSTART 10199.17s
I1209 14:42:37.622413 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.27 (Best 7.27)  step=50000	Elapsed 7.97s  FromSTART 10199.17s
I1209 14:44:16.586073 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 14:44:16.588444 140177781462848 seq_generation_validator.py:179] Sample 2785
I1209 14:44:16.588534 140177781462848 seq_generation_validator.py:181]   Data: He is undoubtedly one of the oldest patients in the HIV Unit of the Guadalajara Civil Hospital (CHG), where he arrived in 1994 after several battles with his health.
I1209 14:44:16.588586 140177781462848 seq_generation_validator.py:182]   Reference: Er ist zweifelsohne einer der ältesten Patienten der HIV-Einheit des Hospital Civil de Guadalajara (HCG), in das er nach mehreren Kämpfen 1994 kam.
I1209 14:44:16.588634 140177781462848 seq_generation_validator.py:183]   Hypothesis: Er ist zweifellos eines der ältesten Patienten in der HIV-Einheit des Guwichtigen ajara Zivilhauses (CHG), wo er 1994 nach mehreren Schlachten mit seiner Gesundheit kam.
I1209 14:44:16.588684 140177781462848 seq_generation_validator.py:179] Sample 85
I1209 14:44:16.588729 140177781462848 seq_generation_validator.py:181]   Data: Weight
I1209 14:44:16.588774 140177781462848 seq_generation_validator.py:182]   Reference: Das Gewicht
I1209 14:44:16.588819 140177781462848 seq_generation_validator.py:183]   Hypothesis: Gewichten
I1209 14:44:16.588863 140177781462848 seq_generation_validator.py:179] Sample 558
I1209 14:44:16.588916 140177781462848 seq_generation_validator.py:181]   Data: In Jaffa, you can meet newlyweds who come from all over Israel and even from other countries for photo sessions.
I1209 14:44:16.588961 140177781462848 seq_generation_validator.py:182]   Reference: In Jaffa können Sie neuvermählte Paare aus ganz Israel und sogar anderen Ländern bei Fototerminen antreffen.
I1209 14:44:16.589004 140177781462848 seq_generation_validator.py:183]   Hypothesis: In Jaffa können Sie neue lyweds, die aus allen Ländern und sogar aus anderen Ländern für FotoSitzungen kommen.
I1209 14:44:16.589048 140177781462848 seq_generation_validator.py:179] Sample 1866
I1209 14:44:16.589091 140177781462848 seq_generation_validator.py:181]   Data: The image documents the final launch preparations on the Kepler space telescope.
I1209 14:44:16.589134 140177781462848 seq_generation_validator.py:182]   Reference: Im Bild sind die letzten Startvorbereitungen am Weltraumteleskop Kepler dokumentiert.
I1209 14:44:16.589177 140177781462848 seq_generation_validator.py:183]   Hypothesis: Das Bild enthält die endgültige Start-Vorbereitung auf den Keplplatz-RaumfahrtTeleskop.
I1209 14:44:16.589221 140177781462848 seq_generation_validator.py:179] Sample 2746
I1209 14:44:16.589266 140177781462848 seq_generation_validator.py:181]   Data: "It is incomprehensible that the Inter-American Commission on Human Rights is in Washington under US funding" he said referring to Ecuador giving political asylum to WikiLeaks founder Julian Assange.
I1209 14:44:16.589315 140177781462848 seq_generation_validator.py:182]   Reference: "Es ist unverständlich, dass die interamerikanische Kommission für Menschenrechte ihren Sitz in Washington hat und von den Vereinigten Staaten finanziert wird", erklärte er als Anspielung auf das politische Asyl, das Ecuador dem Gründer von WikiLeaks, Julian Assange, gewährt hat.
I1209 14:44:16.589362 140177781462848 seq_generation_validator.py:183]   Hypothesis: "Es ist unverständlich, dass die Interamerikanische Menschenrechtskommission in Washington unter US-Finanzierung" unter den US-Finanzmitteln "ist, sagte er, dass die von der amerikanischen Finanzierung vertretene politische Asyl für die Demokratie und die Unterstützung der Menschen in den USA.
I1209 14:44:17.350455 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.76s
I1209 14:44:18.646305 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.30s
I1209 14:44:18.646493 140177781462848 training_utils.py:359] Evaluating bleu at step=50000 with bad count=0 (early_stop_patience=0).
I1209 14:44:18.646556 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.58 (Best 17.58)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:44:18.648193 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.69 (Best 17.69)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:44:18.648829 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.58 (Best 17.58)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:44:18.649343 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.49 (Best 46.49)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:44:18.649822 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.07 (Best 18.07)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:44:18.650283 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.19 (Best 18.19)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:44:18.650751 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.07 (Best 18.07)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:44:18.651192 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=47.83 (Best 47.83)  step=50000	Elapsed 88.27s  FromSTART 10281.82s
I1209 14:47:02.904340 140177781462848 callbacks.py:220] Update 51000	TrainingLoss=3.23	Speed 0.164 secs/step 6.1 steps/sec
I1209 14:47:02.907118 140177781462848 callbacks.py:238] {'step': 51000, 'lr': 0.00027675464, 'loss': 3.2339928150177, 'src_tokens_per_step': 2032.952, 'src_tokens_per_sec': 12382.3718358392, 'src_real_tokens_per_step': 1652.832, 'src_real_tokens_per_sec': 10067.124263717873, 'trg_tokens_per_step': 2032.952, 'trg_tokens_per_sec': 12382.3718358392, 'trg_real_tokens_per_step': 1687.072, 'trg_real_tokens_per_sec': 10275.674397542543, 'samples_per_step': 55.892, 'samples_per_sec': 340.42885746870786, 'this_step_loss': 189851.078125}
I1209 14:49:51.453155 140177781462848 callbacks.py:220] Update 52000	TrainingLoss=3.23	Speed 0.168 secs/step 5.9 steps/sec
I1209 14:49:51.456753 140177781462848 callbacks.py:238] {'step': 52000, 'lr': 0.00027408064, 'loss': 3.2266597747802734, 'src_tokens_per_step': 2032.928, 'src_tokens_per_sec': 12067.047187024864, 'src_real_tokens_per_step': 1652.712, 'src_real_tokens_per_sec': 9810.16233263659, 'trg_tokens_per_step': 2032.928, 'trg_tokens_per_sec': 12067.047187024864, 'trg_real_tokens_per_step': 1687.536, 'trg_real_tokens_per_sec': 10016.870514746803, 'samples_per_step': 55.916, 'samples_per_sec': 331.90600479194654, 'this_step_loss': 102935.1796875}
I1209 14:49:52.095674 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-52000	Elapsed 0.63s
I1209 14:52:41.878690 140177781462848 callbacks.py:220] Update 53000	TrainingLoss=3.22	Speed 0.170 secs/step 5.9 steps/sec
I1209 14:52:41.881176 140177781462848 callbacks.py:238] {'step': 53000, 'lr': 0.00027148266, 'loss': 3.2196004390716553, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 11979.335104925007, 'src_real_tokens_per_step': 1652.744, 'src_real_tokens_per_sec': 9738.6219383685, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 11979.335104925007, 'trg_real_tokens_per_step': 1691.024, 'trg_real_tokens_per_sec': 9964.182852702932, 'samples_per_step': 55.86, 'samples_per_sec': 329.1492339268903, 'this_step_loss': 95036.984375}
I1209 14:55:29.729744 140177781462848 callbacks.py:220] Update 54000	TrainingLoss=3.21	Speed 0.168 secs/step 6.0 steps/sec
I1209 14:55:29.733351 140177781462848 callbacks.py:238] {'step': 54000, 'lr': 0.00026895717, 'loss': 3.2124929428100586, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 12117.936677943377, 'src_real_tokens_per_step': 1654.312, 'src_real_tokens_per_sec': 9860.644511190203, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 12117.936677943377, 'trg_real_tokens_per_step': 1689.336, 'trg_real_tokens_per_sec': 10069.40755791895, 'samples_per_step': 55.766, 'samples_per_sec': 332.39721516318133, 'this_step_loss': 192714.15625}
I1209 14:55:30.428697 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-54000	Elapsed 0.69s
I1209 14:58:19.001162 140177781462848 callbacks.py:220] Update 55000	TrainingLoss=3.21	Speed 0.169 secs/step 5.9 steps/sec
I1209 14:58:19.015624 140177781462848 callbacks.py:238] {'step': 55000, 'lr': 0.0002665009, 'loss': 3.2057366371154785, 'src_tokens_per_step': 2033.072, 'src_tokens_per_sec': 12065.544896852953, 'src_real_tokens_per_step': 1649.472, 'src_real_tokens_per_sec': 9789.018033843284, 'trg_tokens_per_step': 2033.072, 'trg_tokens_per_sec': 12065.544896852953, 'trg_real_tokens_per_step': 1689.808, 'trg_real_tokens_per_sec': 10028.397563421902, 'samples_per_step': 56.06, 'samples_per_sec': 332.69576626778417, 'this_step_loss': 196824.6875}
I1209 15:01:08.460247 140177781462848 callbacks.py:220] Update 56000	TrainingLoss=3.20	Speed 0.169 secs/step 5.9 steps/sec
I1209 15:01:08.466321 140177781462848 callbacks.py:238] {'step': 56000, 'lr': 0.0002641107, 'loss': 3.1993072032928467, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 12003.871596667774, 'src_real_tokens_per_step': 1651.824, 'src_real_tokens_per_sec': 9753.13681559523, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 12003.871596667774, 'trg_real_tokens_per_step': 1687.696, 'trg_real_tokens_per_sec': 9964.941780197409, 'samples_per_step': 55.994, 'samples_per_sec': 330.6146071569606, 'this_step_loss': 186668.265625}
I1209 15:01:09.175816 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-56000	Elapsed 0.70s
I1209 15:03:59.425458 140177781462848 callbacks.py:220] Update 57000	TrainingLoss=3.19	Speed 0.170 secs/step 5.9 steps/sec
I1209 15:03:59.628034 140177781462848 callbacks.py:238] {'step': 57000, 'lr': 0.0002617837, 'loss': 3.1930575370788574, 'src_tokens_per_step': 2032.944, 'src_tokens_per_sec': 11946.128262973045, 'src_real_tokens_per_step': 1651.6, 'src_real_tokens_per_sec': 9705.247876540761, 'trg_tokens_per_step': 2032.944, 'trg_tokens_per_sec': 11946.128262973045, 'trg_real_tokens_per_step': 1687.84, 'trg_real_tokens_per_sec': 9918.203908900798, 'samples_per_step': 55.953, 'samples_per_sec': 328.79494698237175, 'this_step_loss': 151939.203125}
I1209 15:06:49.913034 140177781462848 callbacks.py:220] Update 58000	TrainingLoss=3.19	Speed 0.170 secs/step 5.9 steps/sec
I1209 15:06:49.939933 140177781462848 callbacks.py:238] {'step': 58000, 'lr': 0.0002595171, 'loss': 3.1870245933532715, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 11944.167074853773, 'src_real_tokens_per_step': 1652.248, 'src_real_tokens_per_sec': 9707.232572038156, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 11944.167074853773, 'trg_real_tokens_per_step': 1689.104, 'trg_real_tokens_per_sec': 9923.767719107505, 'samples_per_step': 56.041, 'samples_per_sec': 329.2502218611191, 'this_step_loss': 244259.484375}
I1209 15:06:50.943507 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-58000	Elapsed 1.00s
I1209 15:09:37.471695 140177781462848 callbacks.py:220] Update 59000	TrainingLoss=3.18	Speed 0.166 secs/step 6.0 steps/sec
I1209 15:09:37.478106 140177781462848 callbacks.py:238] {'step': 59000, 'lr': 0.00025730842, 'loss': 3.1810286045074463, 'src_tokens_per_step': 2032.848, 'src_tokens_per_sec': 12212.456440081263, 'src_real_tokens_per_step': 1653.664, 'src_real_tokens_per_sec': 9934.48578867212, 'trg_tokens_per_step': 2032.848, 'trg_tokens_per_sec': 12212.456440081263, 'trg_real_tokens_per_step': 1687.152, 'trg_real_tokens_per_sec': 10135.66695975104, 'samples_per_step': 55.613, 'samples_per_sec': 334.09843726743924, 'this_step_loss': 105842.765625}
I1209 15:12:27.048656 140177781462848 callbacks.py:220] Update 60000	TrainingLoss=3.18	Speed 0.169 secs/step 5.9 steps/sec
I1209 15:12:27.053369 140177781462848 callbacks.py:238] {'step': 60000, 'lr': 0.00025515517, 'loss': 3.1754302978515625, 'src_tokens_per_step': 2032.912, 'src_tokens_per_sec': 11994.037628050297, 'src_real_tokens_per_step': 1650.792, 'src_real_tokens_per_sec': 9739.556539626115, 'trg_tokens_per_step': 2032.912, 'trg_tokens_per_sec': 11994.037628050297, 'trg_real_tokens_per_step': 1687.08, 'trg_real_tokens_per_sec': 9953.653183970135, 'samples_per_step': 55.978, 'samples_per_sec': 330.2662576358443, 'this_step_loss': 96746.6875}
I1209 15:12:27.754957 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-60000	Elapsed 0.70s
I1209 15:12:35.523287 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=76.81 (Best 76.81)  step=60000	Elapsed 7.77s  FromSTART 11997.07s
I1209 15:12:35.570690 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.12 (Best 7.12)  step=60000	Elapsed 7.77s  FromSTART 11997.07s
I1209 15:14:15.220440 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 15:14:15.224711 140177781462848 seq_generation_validator.py:179] Sample 1159
I1209 15:14:15.225011 140177781462848 seq_generation_validator.py:181]   Data: It is less common there for large groups of people with disabilities to actively take part in social life, in this respect with The Tap Tap we are a step ahead!
I1209 15:14:15.225250 140177781462848 seq_generation_validator.py:182]   Reference: Es ist aber nicht gerade üblich, dass sich dort eine größere Gruppe behinderter Menschen aktiv am gesellschaftlichen Geschehen beteiligt, darin sind wir mit der Band The Tap Tap dem Westen ein wenig voraus!
I1209 15:14:15.225461 140177781462848 seq_generation_validator.py:183]   Hypothesis: Es ist weniger üblich, dass große Gruppen von Menschen mit Behinderungen aktiv an dem sozialen Leben teilnehmen, in diesem Zusammenhang mit dem Tap Tap sind wir ein Schritt voraus!
I1209 15:14:15.225641 140177781462848 seq_generation_validator.py:179] Sample 1926
I1209 15:14:15.225738 140177781462848 seq_generation_validator.py:181]   Data: Then she tells of her husband who was in the army.
I1209 15:14:15.225807 140177781462848 seq_generation_validator.py:182]   Reference: Dann erzählt sie von ihrem Mann, der in der Armee war.
I1209 15:14:15.225873 140177781462848 seq_generation_validator.py:183]   Hypothesis: Dann sagt sie von ihrem Mann, der in der Armee war.
I1209 15:14:15.225935 140177781462848 seq_generation_validator.py:179] Sample 696
I1209 15:14:15.226001 140177781462848 seq_generation_validator.py:181]   Data: "I am given the opportunity to shoot, and when I am certain that the weapon is good, we begin to negotiate," describes the expert.
I1209 15:14:15.226065 140177781462848 seq_generation_validator.py:182]   Reference: "Ich kann dann dort schießen, und wenn ich davon überzeugt bin, dass die Waffe einen guten Schuss hat, verhandeln wir miteinander", beschreibt der Experte das Vorgehen.
I1209 15:14:15.226129 140177781462848 seq_generation_validator.py:183]   Hypothesis: "Ich habe die Gelegenheit gegeben, zu spielen, und wenn ich sicher bin, dass die Waffe gut ist, wir beginnen zu verhandeln", beschreibt den Experte.
I1209 15:14:15.226192 140177781462848 seq_generation_validator.py:179] Sample 1832
I1209 15:14:15.226253 140177781462848 seq_generation_validator.py:181]   Data: In this bright shining galaxy with one small black hole, there exists no dust - only gas.
I1209 15:14:15.226324 140177781462848 seq_generation_validator.py:182]   Reference: In dieser hell strahlenden Galaxie mit einem kleinen Schwarzen Loch existiert noch kein Staub, sondern nur Gas.
I1209 15:14:15.226388 140177781462848 seq_generation_validator.py:183]   Hypothesis: In diesem hellen Schlamm mit einem kleinen schwarz-Loch besteht kein Staub - nur Gas.
I1209 15:14:15.226451 140177781462848 seq_generation_validator.py:179] Sample 1272
I1209 15:14:15.226515 140177781462848 seq_generation_validator.py:181]   Data: The proof is the existence of the almost omnipotent, and in all states, omnipresent lobby.
I1209 15:14:15.226577 140177781462848 seq_generation_validator.py:182]   Reference: Der Beweis dafür ist die Existenz einer praktisch allmächtigen und in allen Staaten allgegenwärtigen Lobby.
I1209 15:14:15.226640 140177781462848 seq_generation_validator.py:183]   Hypothesis: Der Beweis ist die Existenz des fast omnipozalen, und in allen Staaten, der alltäglichen Lobby.
I1209 15:14:16.094083 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.87s
I1209 15:14:17.568100 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.47s
I1209 15:14:17.568309 140177781462848 training_utils.py:359] Evaluating bleu at step=60000 with bad count=0 (early_stop_patience=0).
I1209 15:14:17.568376 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.78 (Best 17.78)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:14:17.569452 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.93 (Best 17.93)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:14:17.570013 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.78 (Best 17.78)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:14:17.570485 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.71 (Best 46.71)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:14:17.570940 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.25 (Best 18.25)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:14:17.571418 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.41 (Best 18.41)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:14:17.571870 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.25 (Best 18.25)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:14:17.572320 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.03 (Best 48.03)  step=60000	Elapsed 89.48s  FromSTART 12080.98s
I1209 15:17:02.132162 140177781462848 callbacks.py:220] Update 61000	TrainingLoss=3.17	Speed 0.164 secs/step 6.1 steps/sec
I1209 15:17:02.716173 140177781462848 callbacks.py:238] {'step': 61000, 'lr': 0.0002530551, 'loss': 3.1698660850524902, 'src_tokens_per_step': 2032.976, 'src_tokens_per_sec': 12359.287369164575, 'src_real_tokens_per_step': 1649.728, 'src_real_tokens_per_sec': 10029.367013165494, 'trg_tokens_per_step': 2032.976, 'trg_tokens_per_sec': 12359.287369164575, 'trg_real_tokens_per_step': 1688.552, 'trg_real_tokens_per_sec': 10265.393888455928, 'samples_per_step': 55.995, 'samples_per_sec': 340.41636312301296, 'this_step_loss': 199324.203125}
I1209 15:19:51.195238 140177781462848 callbacks.py:220] Update 62000	TrainingLoss=3.16	Speed 0.168 secs/step 5.9 steps/sec
I1209 15:19:51.203669 140177781462848 callbacks.py:238] {'step': 62000, 'lr': 0.00025100604, 'loss': 3.164576292037964, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 12072.169545029965, 'src_real_tokens_per_step': 1650.864, 'src_real_tokens_per_sec': 9803.005461773904, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 12072.169545029965, 'trg_real_tokens_per_step': 1688.048, 'trg_real_tokens_per_sec': 10023.807996138094, 'samples_per_step': 55.87, 'samples_per_sec': 331.76198351245654, 'this_step_loss': 176650.390625}
I1209 15:19:52.229433 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-62000	Elapsed 1.02s
I1209 15:22:41.290061 140177781462848 callbacks.py:220] Update 63000	TrainingLoss=3.16	Speed 0.169 secs/step 5.9 steps/sec
I1209 15:22:41.299690 140177781462848 callbacks.py:238] {'step': 63000, 'lr': 0.00024900597, 'loss': 3.1592495441436768, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 12029.733075260827, 'src_real_tokens_per_step': 1652.912, 'src_real_tokens_per_sec': 9781.049011714935, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 12029.733075260827, 'trg_real_tokens_per_step': 1687.496, 'trg_real_tokens_per_sec': 9985.698623443295, 'samples_per_step': 55.859, 'samples_per_sec': 330.5436809372698, 'this_step_loss': 215800.890625}
I1209 15:25:31.028960 140177781462848 callbacks.py:220] Update 64000	TrainingLoss=3.15	Speed 0.170 secs/step 5.9 steps/sec
I1209 15:25:31.103678 140177781462848 callbacks.py:238] {'step': 64000, 'lr': 0.00024705296, 'loss': 3.154043197631836, 'src_tokens_per_step': 2032.848, 'src_tokens_per_sec': 11982.315853336266, 'src_real_tokens_per_step': 1649.536, 'src_real_tokens_per_sec': 9722.941097144936, 'trg_tokens_per_step': 2032.848, 'trg_tokens_per_sec': 11982.315853336266, 'trg_real_tokens_per_step': 1689.584, 'trg_real_tokens_per_sec': 9958.997991361528, 'samples_per_step': 55.941, 'samples_per_sec': 329.73578504220876, 'this_step_loss': 218640.09375}
I1209 15:25:32.155449 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-64000	Elapsed 1.04s
I1209 15:28:23.050045 140177781462848 callbacks.py:220] Update 65000	TrainingLoss=3.15	Speed 0.171 secs/step 5.9 steps/sec
I1209 15:28:23.075259 140177781462848 callbacks.py:238] {'step': 65000, 'lr': 0.00024514517, 'loss': 3.148930549621582, 'src_tokens_per_step': 2033.16, 'src_tokens_per_sec': 11902.093711813097, 'src_real_tokens_per_step': 1651.288, 'src_real_tokens_per_sec': 9666.619705872841, 'trg_tokens_per_step': 2033.16, 'trg_tokens_per_sec': 11902.093711813097, 'trg_real_tokens_per_step': 1687.432, 'trg_real_tokens_per_sec': 9878.206238718152, 'samples_per_step': 56.167, 'samples_per_sec': 328.8009293471277, 'this_step_loss': 40400.95703125}
I1209 15:31:12.043077 140177781462848 callbacks.py:220] Update 66000	TrainingLoss=3.14	Speed 0.169 secs/step 5.9 steps/sec
I1209 15:31:12.054216 140177781462848 callbacks.py:238] {'step': 66000, 'lr': 0.00024328091, 'loss': 3.144076108932495, 'src_tokens_per_step': 2032.96, 'src_tokens_per_sec': 12037.039760536032, 'src_real_tokens_per_step': 1649.408, 'src_real_tokens_per_sec': 9766.050329247117, 'trg_tokens_per_step': 2032.96, 'trg_tokens_per_sec': 12037.039760536032, 'trg_real_tokens_per_step': 1689.816, 'trg_real_tokens_per_sec': 10005.303783640582, 'samples_per_step': 55.816, 'samples_per_sec': 330.48334019069694, 'this_step_loss': 98875.578125}
I1209 15:31:12.858484 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-66000	Elapsed 0.80s
I1209 15:34:03.012434 140177781462848 callbacks.py:220] Update 67000	TrainingLoss=3.14	Speed 0.170 secs/step 5.9 steps/sec
I1209 15:34:03.018393 140177781462848 callbacks.py:238] {'step': 67000, 'lr': 0.00024145858, 'loss': 3.139254570007324, 'src_tokens_per_step': 2035.112, 'src_tokens_per_sec': 11965.43453248423, 'src_real_tokens_per_step': 1651.088, 'src_real_tokens_per_sec': 9707.566645654058, 'trg_tokens_per_step': 2035.112, 'trg_tokens_per_sec': 11965.43453248423, 'trg_real_tokens_per_step': 1690.808, 'trg_real_tokens_per_sec': 9941.100259347199, 'samples_per_step': 56.023, 'samples_per_sec': 329.3870503507247, 'this_step_loss': 214842.953125}
I1209 15:36:51.861230 140177781462848 callbacks.py:220] Update 68000	TrainingLoss=3.13	Speed 0.169 secs/step 5.9 steps/sec
I1209 15:36:52.051545 140177781462848 callbacks.py:238] {'step': 68000, 'lr': 0.00023967656, 'loss': 3.134533166885376, 'src_tokens_per_step': 2035.296, 'src_tokens_per_sec': 12059.788660987859, 'src_real_tokens_per_step': 1649.896, 'src_real_tokens_per_sec': 9776.16871089474, 'trg_tokens_per_step': 2035.296, 'trg_tokens_per_sec': 12059.788660987859, 'trg_real_tokens_per_step': 1688.68, 'trg_real_tokens_per_sec': 10005.976485011013, 'samples_per_step': 56.144, 'samples_per_sec': 332.67140238201335, 'this_step_loss': 222714.296875}
I1209 15:36:52.767698 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-68000	Elapsed 0.71s
I1209 15:39:44.363411 140177781462848 callbacks.py:220] Update 69000	TrainingLoss=3.13	Speed 0.172 secs/step 5.8 steps/sec
I1209 15:39:44.605757 140177781462848 callbacks.py:238] {'step': 69000, 'lr': 0.00023793345, 'loss': 3.1299400329589844, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 11864.014389896063, 'src_real_tokens_per_step': 1654.608, 'src_real_tokens_per_sec': 9646.601145602604, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 11864.014389896063, 'trg_real_tokens_per_step': 1689.352, 'trg_real_tokens_per_sec': 9849.163631824607, 'samples_per_step': 55.729, 'samples_per_sec': 324.9080357663492, 'this_step_loss': 175099.640625}
I1209 15:42:34.033056 140177781462848 callbacks.py:220] Update 70000	TrainingLoss=3.13	Speed 0.169 secs/step 5.9 steps/sec
I1209 15:42:34.192518 140177781462848 callbacks.py:238] {'step': 70000, 'lr': 0.00023622779, 'loss': 3.1254231929779053, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 12016.960878002637, 'src_real_tokens_per_step': 1652.48, 'src_real_tokens_per_sec': 9758.241626280505, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 12016.960878002637, 'trg_real_tokens_per_step': 1689.64, 'trg_real_tokens_per_sec': 9977.67923450123, 'samples_per_step': 56.095, 'samples_per_sec': 331.2527619252305, 'this_step_loss': 76551.203125}
I1209 15:42:35.405985 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-70000	Elapsed 1.21s
I1209 15:42:42.802672 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=75.97 (Best 75.97)  step=70000	Elapsed 7.40s  FromSTART 13804.35s
I1209 15:42:42.804206 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.97 (Best 6.97)  step=70000	Elapsed 7.40s  FromSTART 13804.35s
I1209 15:44:12.723000 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 15:44:12.728301 140177781462848 seq_generation_validator.py:179] Sample 1102
I1209 15:44:12.728403 140177781462848 seq_generation_validator.py:181]   Data: According to meteorologists the conditions for this were perfect - rain and melting snow during the day, with a clear night and freezing temperatures.
I1209 15:44:12.728456 140177781462848 seq_generation_validator.py:182]   Reference: Nach Angaben von Meteorologen gab es hierfür in der Nacht ideale Bedingungen - tagsüber Regen und schmelzender Schnee, in der Nacht wolkenloser Himmel und Frost.
I1209 15:44:12.728505 140177781462848 seq_generation_validator.py:183]   Hypothesis: Nach Meteorologen waren die Bedingungen dafür perfekt - Regen- und Schmelzen während des Tages, mit einer klaren Nacht und Einfrieren von Temperaturen.
I1209 15:44:12.728552 140177781462848 seq_generation_validator.py:179] Sample 968
I1209 15:44:12.728598 140177781462848 seq_generation_validator.py:181]   Data: The biggest risk was for Tereshkova.
I1209 15:44:12.728643 140177781462848 seq_generation_validator.py:182]   Reference: Das größte Risiko gab es bei Tereschkowa.
I1209 15:44:12.728688 140177781462848 seq_generation_validator.py:183]   Hypothesis: Das größte Risiko war für Tereshkova.
I1209 15:44:12.728731 140177781462848 seq_generation_validator.py:179] Sample 2576
I1209 15:44:12.728775 140177781462848 seq_generation_validator.py:181]   Data: "I'll go on working two more years because I want to open a shop or study Spanish or French" he vows in more than acceptable English.
I1209 15:44:12.728819 140177781462848 seq_generation_validator.py:182]   Reference: "Ich werde hier noch zwei Jahre arbeiten, weil ich einen Laden aufmachen oder Spanisch oder Französisch lernen möchte", verspricht er in einem mehr als nur annehmbaren Englisch.
I1209 15:44:12.728864 140177781462848 seq_generation_validator.py:183]   Hypothesis: "Ich werde zwei weitere Jahre arbeiten, weil ich einen Shop öffnen möchte oder Spanisch oder Französisch", so er in mehr als akzeptablen Englisch.
I1209 15:44:12.728909 140177781462848 seq_generation_validator.py:179] Sample 1447
I1209 15:44:12.728954 140177781462848 seq_generation_validator.py:181]   Data: But without the support of the Parliament you will be left with just slogans.
I1209 15:44:12.728998 140177781462848 seq_generation_validator.py:182]   Reference: Aber ohne Unterstützung im Parlament bleiben Ihnen nur Schlagwörter.
I1209 15:44:12.729041 140177781462848 seq_generation_validator.py:183]   Hypothesis: Aber ohne die Unterstützung des Parlaments werden Sie mit nur langsameren Bildern bleiben.
I1209 15:44:12.729085 140177781462848 seq_generation_validator.py:179] Sample 296
I1209 15:44:12.729129 140177781462848 seq_generation_validator.py:181]   Data: It was a terrific season.
I1209 15:44:12.729172 140177781462848 seq_generation_validator.py:182]   Reference: Wir haben die Saison mit einer sehr starken Leistung beendet.
I1209 15:44:12.729216 140177781462848 seq_generation_validator.py:183]   Hypothesis: Es war eine schreckliche Saison.
I1209 15:44:13.613002 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.88s
I1209 15:44:14.823467 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.21s
I1209 15:44:14.823665 140177781462848 training_utils.py:359] Evaluating bleu at step=70000 with bad count=0 (early_stop_patience=0).
I1209 15:44:14.823729 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.01 (Best 18.01)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:44:14.824831 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.13 (Best 18.13)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:44:14.825477 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.01 (Best 18.01)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:44:14.826009 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.94 (Best 46.94)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:44:14.826487 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.53 (Best 18.53)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:44:14.826959 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.67 (Best 18.67)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:44:14.827427 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.53 (Best 18.53)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:44:14.827877 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.27 (Best 48.27)  step=70000	Elapsed 79.58s  FromSTART 13878.31s
I1209 15:47:00.526575 140177781462848 callbacks.py:220] Update 71000	TrainingLoss=3.12	Speed 0.166 secs/step 6.0 steps/sec
I1209 15:47:00.533432 140177781462848 callbacks.py:238] {'step': 71000, 'lr': 0.00023455832, 'loss': 3.121019124984741, 'src_tokens_per_step': 2035.088, 'src_tokens_per_sec': 12287.31257727733, 'src_real_tokens_per_step': 1652.152, 'src_real_tokens_per_sec': 9975.248268956377, 'trg_tokens_per_step': 2035.088, 'trg_tokens_per_sec': 12287.31257727733, 'trg_real_tokens_per_step': 1689.968, 'trg_real_tokens_per_sec': 10203.571079774541, 'samples_per_step': 56.069, 'samples_per_sec': 338.52950284968637, 'this_step_loss': 88078.90625}
I1209 15:49:51.340650 140177781462848 callbacks.py:220] Update 72000	TrainingLoss=3.12	Speed 0.171 secs/step 5.9 steps/sec
I1209 15:49:51.347917 140177781462848 callbacks.py:238] {'step': 72000, 'lr': 0.00023292375, 'loss': 3.1167402267456055, 'src_tokens_per_step': 2034.96, 'src_tokens_per_sec': 11919.249465469564, 'src_real_tokens_per_step': 1652.064, 'src_real_tokens_per_sec': 9676.535631620038, 'trg_tokens_per_step': 2034.96, 'trg_tokens_per_sec': 11919.249465469564, 'trg_real_tokens_per_step': 1690.616, 'trg_real_tokens_per_sec': 9902.343954826774, 'samples_per_step': 55.763, 'samples_per_sec': 326.61728384979523, 'this_step_loss': 237263.171875}
I1209 15:49:52.124921 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-72000	Elapsed 0.77s
I1209 15:52:42.664166 140177781462848 callbacks.py:220] Update 73000	TrainingLoss=3.11	Speed 0.170 secs/step 5.9 steps/sec
I1209 15:52:42.932272 140177781462848 callbacks.py:238] {'step': 73000, 'lr': 0.00023132288, 'loss': 3.1124823093414307, 'src_tokens_per_step': 2035.072, 'src_tokens_per_sec': 11938.260176165993, 'src_real_tokens_per_step': 1654.312, 'src_real_tokens_per_sec': 9704.623260775794, 'trg_tokens_per_step': 2035.072, 'trg_tokens_per_sec': 11938.260176165993, 'trg_real_tokens_per_step': 1688.072, 'trg_real_tokens_per_sec': 9902.668176900317, 'samples_per_step': 56.046, 'samples_per_sec': 328.7803723079082, 'this_step_loss': 207102.578125}
I1209 15:55:34.862131 140177781462848 callbacks.py:220] Update 74000	TrainingLoss=3.11	Speed 0.172 secs/step 5.8 steps/sec
I1209 15:55:35.174657 140177781462848 callbacks.py:238] {'step': 74000, 'lr': 0.00022975456, 'loss': 3.1084461212158203, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 11841.555642277966, 'src_real_tokens_per_step': 1651.84, 'src_real_tokens_per_sec': 9612.081553856378, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 11841.555642277966, 'trg_real_tokens_per_step': 1687.52, 'trg_real_tokens_per_sec': 9819.704005087488, 'samples_per_step': 55.913, 'samples_per_sec': 325.35857947547686, 'this_step_loss': 69040.25}
I1209 15:55:36.751801 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-74000	Elapsed 1.57s
I1209 15:58:29.068771 140177781462848 callbacks.py:220] Update 75000	TrainingLoss=3.10	Speed 0.172 secs/step 5.8 steps/sec
I1209 15:58:29.075027 140177781462848 callbacks.py:238] {'step': 75000, 'lr': 0.00022821774, 'loss': 3.1045851707458496, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 11814.77620927533, 'src_real_tokens_per_step': 1649.336, 'src_real_tokens_per_sec': 9575.50501901748, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 11814.77620927533, 'trg_real_tokens_per_step': 1690.592, 'trg_real_tokens_per_sec': 9815.023852696359, 'samples_per_step': 55.968, 'samples_per_sec': 324.93189071503343, 'this_step_loss': 95142.3515625}
I1209 16:01:35.609812 140177781462848 callbacks.py:220] Update 76000	TrainingLoss=3.10	Speed 0.186 secs/step 5.4 steps/sec
I1209 16:01:35.626366 140177781462848 callbacks.py:238] {'step': 76000, 'lr': 0.00022671133, 'loss': 3.1007046699523926, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 10913.720993217694, 'src_real_tokens_per_step': 1651.84, 'src_real_tokens_per_sec': 8859.074689739235, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 10913.720993217694, 'trg_real_tokens_per_step': 1689.784, 'trg_real_tokens_per_sec': 9062.57425993215, 'samples_per_step': 56.025, 'samples_per_sec': 300.4707837881639, 'this_step_loss': 166106.375}
I1209 16:01:36.635318 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-76000	Elapsed 1.00s
I1209 16:04:36.913789 140177781462848 callbacks.py:220] Update 77000	TrainingLoss=3.10	Speed 0.180 secs/step 5.5 steps/sec
I1209 16:04:36.920516 140177781462848 callbacks.py:238] {'step': 77000, 'lr': 0.00022523437, 'loss': 3.096933603286743, 'src_tokens_per_step': 2035.168, 'src_tokens_per_sec': 11293.442073296883, 'src_real_tokens_per_step': 1652.072, 'src_real_tokens_per_sec': 9167.586868954173, 'trg_tokens_per_step': 2035.168, 'trg_tokens_per_sec': 11293.442073296883, 'trg_real_tokens_per_step': 1686.464, 'trg_real_tokens_per_sec': 9358.433059433204, 'samples_per_step': 55.734, 'samples_per_sec': 309.27604036282435, 'this_step_loss': 45037.25390625}
I1209 16:07:31.612087 140177781462848 callbacks.py:220] Update 78000	TrainingLoss=3.09	Speed 0.175 secs/step 5.7 steps/sec
I1209 16:07:31.736501 140177781462848 callbacks.py:238] {'step': 78000, 'lr': 0.0002237859, 'loss': 3.0930628776550293, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 11654.233767372043, 'src_real_tokens_per_step': 1652.272, 'src_real_tokens_per_sec': 9462.353040029002, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 11654.233767372043, 'trg_real_tokens_per_step': 1688.928, 'trg_real_tokens_per_sec': 9672.277321887743, 'samples_per_step': 55.921, 'samples_per_sec': 320.2525034325231, 'this_step_loss': 48698.41015625}
I1209 16:07:38.283634 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-78000	Elapsed 6.54s
I1209 16:10:33.305356 140177781462848 callbacks.py:220] Update 79000	TrainingLoss=3.09	Speed 0.175 secs/step 5.7 steps/sec
I1209 16:10:33.311070 140177781462848 callbacks.py:238] {'step': 79000, 'lr': 0.00022236501, 'loss': 3.089237689971924, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 11632.071846664143, 'src_real_tokens_per_step': 1652.536, 'src_real_tokens_per_sec': 9445.719730913877, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 11632.071846664143, 'trg_real_tokens_per_step': 1691.272, 'trg_real_tokens_per_sec': 9667.13058035781, 'samples_per_step': 55.997, 'samples_per_sec': 320.07288662515333, 'this_step_loss': 104180.078125}
I1209 16:13:29.702458 140177781462848 callbacks.py:220] Update 80000	TrainingLoss=3.09	Speed 0.176 secs/step 5.7 steps/sec
I1209 16:13:30.052281 140177781462848 callbacks.py:238] {'step': 80000, 'lr': 0.00022097088, 'loss': 3.0856287479400635, 'src_tokens_per_step': 2035.2, 'src_tokens_per_sec': 11543.155309260035, 'src_real_tokens_per_step': 1651.216, 'src_real_tokens_per_sec': 9365.29222540051, 'trg_tokens_per_step': 2035.2, 'trg_tokens_per_sec': 11543.155309260035, 'trg_real_tokens_per_step': 1686.704, 'trg_real_tokens_per_sec': 9566.571458701916, 'samples_per_step': 56.071, 'samples_per_sec': 318.02096174602957, 'this_step_loss': 91099.4140625}
I1209 16:13:31.071684 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-80000	Elapsed 1.01s
I1209 16:13:40.450103 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=75.22 (Best 75.22)  step=80000	Elapsed 9.38s  FromSTART 15662.00s
I1209 16:13:40.451376 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.84 (Best 6.84)  step=80000	Elapsed 9.38s  FromSTART 15662.00s
I1209 16:15:39.259164 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 16:15:39.272629 140177781462848 seq_generation_validator.py:179] Sample 2701
I1209 16:15:39.272760 140177781462848 seq_generation_validator.py:181]   Data: According to Guardia, it is important to remind children the "don't talk to strangers" lesson.
I1209 16:15:39.272816 140177781462848 seq_generation_validator.py:182]   Reference: Laut Guardia müssen die Kinder an die Lektion "Du sollst nicht mit Fremden sprechen" erinnert werden.
I1209 16:15:39.272865 140177781462848 seq_generation_validator.py:183]   Hypothesis: In der Regel ist es wichtig, Kinder daran zu erinnern, dass "nicht mit der Lehre der Würden spricht.
I1209 16:15:39.272911 140177781462848 seq_generation_validator.py:179] Sample 1068
I1209 16:15:39.272960 140177781462848 seq_generation_validator.py:181]   Data: On some roads in Eastern Bohemia, there might be a risk of black ice, at higher altitudes and in the mountains there might be a layer of compacted snow, according to the Road and Motorway Directorate.
I1209 16:15:39.273006 140177781462848 seq_generation_validator.py:182]   Reference: Auf einigen Straßen Ostböhmens besteht Gefahr wegen Eisglätte, in höheren Lagen und in den Bergen liegt auf den Straßen mitunter noch eine ausgefahrene Schneeschicht, teilt die Straßen- und Autobahndirektion auf ihren Internetseiten mit.
I1209 16:15:39.273053 140177781462848 seq_generation_validator.py:183]   Hypothesis: Auf einigen Straßen in Ostböhmen könnte es ein Risiko von schwarzes Eis geben, bei höheren Alben und in den Bergen könnte es eine Schicht von comptierten Schnee geben, nach der Straßen- und Motorstraße.
I1209 16:15:39.273099 140177781462848 seq_generation_validator.py:179] Sample 2252
I1209 16:15:39.273144 140177781462848 seq_generation_validator.py:181]   Data: That would do more to create a Palestinian state than the imminent bid for virtual statehood at the UN.
I1209 16:15:39.273189 140177781462848 seq_generation_validator.py:182]   Reference: Damit wäre mehr für die Schaffung eines palästinensischen Staates getan als mit dem bevorstehenden Antrag auf UN-Beobachterstatus.
I1209 16:15:39.273233 140177781462848 seq_generation_validator.py:183]   Hypothesis: Dies würde mehr tun, um einen palästinensischen Staat zu schaffen, als das bevorstehende Angebot für virtuelle Staatsbürgerschaft an der UN.
I1209 16:15:39.273277 140177781462848 seq_generation_validator.py:179] Sample 404
I1209 16:15:39.273329 140177781462848 seq_generation_validator.py:181]   Data: And ourselves, with our 88 soldiers killed, plus the wounded, the maimed.
I1209 16:15:39.273373 140177781462848 seq_generation_validator.py:182]   Reference: Und uns selbst mit unseren 88 getöteten Soldaten, sowie den Verletzten und Versehrten.
I1209 16:15:39.273418 140177781462848 seq_generation_validator.py:183]   Hypothesis: Und wir selbst, mit unseren 88 Soldaten getötet, plus der verwundeten, die Verzweiflung.
I1209 16:15:39.273462 140177781462848 seq_generation_validator.py:179] Sample 1284
I1209 16:15:39.273508 140177781462848 seq_generation_validator.py:181]   Data: The current state of affairs, when we consider the demise of those systems in favour of liberalised democracy as an interlude, can expect its next cycle.
I1209 16:15:39.273552 140177781462848 seq_generation_validator.py:182]   Reference: Vom heutigen Zustand aus gesehen, kann der Untergang dieser Regimes zugunsten liberaler Demokratien als gewisse Pause angesehen werden, bei der sich die nächste Runde schon abzeichnet.
I1209 16:15:39.273604 140177781462848 seq_generation_validator.py:183]   Hypothesis: Der gegenwärtige Stand der Angelegenheiten, wenn wir die Entführung dieser Systeme zugunsten der liberalisierten Demokratie als Interlude betrachten, kann von seinem nächsten Zyklus erwarten.
I1209 16:15:40.061709 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.79s
I1209 16:15:41.411451 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.35s
I1209 16:15:41.411675 140177781462848 training_utils.py:359] Evaluating bleu at step=80000 with bad count=0 (early_stop_patience=0).
I1209 16:15:41.411756 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.42 (Best 18.42)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:15:41.412940 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.52 (Best 18.52)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:15:41.413616 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.42 (Best 18.42)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:15:41.414202 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.32 (Best 47.32)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:15:41.414791 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.93 (Best 18.93)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:15:41.415377 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=19.03 (Best 19.03)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:15:41.415956 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.93 (Best 18.93)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:15:41.416532 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.66 (Best 48.66)  step=80000	Elapsed 108.83s  FromSTART 15765.21s
I1209 16:18:42.731549 140177781462848 callbacks.py:220] Update 81000	TrainingLoss=3.08	Speed 0.181 secs/step 5.5 steps/sec
I1209 16:18:42.740274 140177781462848 callbacks.py:238] {'step': 81000, 'lr': 0.00021960262, 'loss': 3.082155227661133, 'src_tokens_per_step': 2034.88, 'src_tokens_per_sec': 11227.301798960723, 'src_real_tokens_per_step': 1654.0, 'src_real_tokens_per_sec': 9125.824213457814, 'trg_tokens_per_step': 2034.88, 'trg_tokens_per_sec': 11227.301798960723, 'trg_real_tokens_per_step': 1688.608, 'trg_real_tokens_per_sec': 9316.771326141821, 'samples_per_step': 55.682, 'samples_per_sec': 307.22136871448487, 'this_step_loss': 132714.484375}
I1209 16:21:49.923635 140177781462848 callbacks.py:220] Update 82000	TrainingLoss=3.08	Speed 0.187 secs/step 5.3 steps/sec
I1209 16:21:49.930768 140177781462848 callbacks.py:238] {'step': 82000, 'lr': 0.00021825948, 'loss': 3.0786168575286865, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 10876.475096907563, 'src_real_tokens_per_step': 1654.272, 'src_real_tokens_per_sec': 8841.42238556071, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 10876.475096907563, 'trg_real_tokens_per_step': 1689.04, 'trg_real_tokens_per_sec': 9027.243443706635, 'samples_per_step': 55.955, 'samples_per_sec': 299.05710160363566, 'this_step_loss': 97935.75}
I1209 16:21:50.693210 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-82000	Elapsed 0.76s
I1209 16:24:44.472991 140177781462848 callbacks.py:220] Update 83000	TrainingLoss=3.08	Speed 0.174 secs/step 5.8 steps/sec
I1209 16:24:44.479263 140177781462848 callbacks.py:238] {'step': 83000, 'lr': 0.00021694068, 'loss': 3.075260639190674, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 11714.73152987823, 'src_real_tokens_per_step': 1651.776, 'src_real_tokens_per_sec': 9508.916406297247, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 11714.73152987823, 'trg_real_tokens_per_step': 1689.264, 'trg_real_tokens_per_sec': 9724.72669669938, 'samples_per_step': 55.857, 'samples_per_sec': 321.5566418851862, 'this_step_loss': 89834.875}
I1209 16:27:42.874960 140177781462848 callbacks.py:220] Update 84000	TrainingLoss=3.07	Speed 0.178 secs/step 5.6 steps/sec
I1209 16:27:42.883125 140177781462848 callbacks.py:238] {'step': 84000, 'lr': 0.00021564549, 'loss': 3.072037935256958, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 11412.257450559213, 'src_real_tokens_per_step': 1653.824, 'src_real_tokens_per_sec': 9274.590205991151, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 11412.257450559213, 'trg_real_tokens_per_step': 1687.168, 'trg_real_tokens_per_sec': 9461.582253408875, 'samples_per_step': 55.619, 'samples_per_sec': 311.90950951674535, 'this_step_loss': 181265.125}
I1209 16:27:43.564128 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-84000	Elapsed 0.67s
I1209 16:30:54.676761 140177781462848 callbacks.py:220] Update 85000	TrainingLoss=3.07	Speed 0.191 secs/step 5.2 steps/sec
I1209 16:30:54.686213 140177781462848 callbacks.py:238] {'step': 85000, 'lr': 0.00021437323, 'loss': 3.068734884262085, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 10652.183312987429, 'src_real_tokens_per_step': 1650.272, 'src_real_tokens_per_sec': 8638.295210775776, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 10652.183312987429, 'trg_real_tokens_per_step': 1690.768, 'trg_real_tokens_per_sec': 8850.270208143225, 'samples_per_step': 55.989, 'samples_per_sec': 293.07260291401957, 'this_step_loss': 188627.125}
I1209 16:33:52.541227 140177781462848 callbacks.py:220] Update 86000	TrainingLoss=3.07	Speed 0.178 secs/step 5.6 steps/sec
I1209 16:33:52.574200 140177781462848 callbacks.py:238] {'step': 86000, 'lr': 0.00021312323, 'loss': 3.065478563308716, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 11447.163133597658, 'src_real_tokens_per_step': 1650.32, 'src_real_tokens_per_sec': 9283.101198324792, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 11447.163133597658, 'trg_real_tokens_per_step': 1689.072, 'trg_real_tokens_per_sec': 9501.082400538595, 'samples_per_step': 55.921, 'samples_per_sec': 314.55735985234423, 'this_step_loss': 210964.703125}
I1209 16:33:53.484474 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-86000	Elapsed 0.90s
I1209 16:36:53.550695 140177781462848 callbacks.py:220] Update 87000	TrainingLoss=3.06	Speed 0.180 secs/step 5.6 steps/sec
I1209 16:36:53.557452 140177781462848 callbacks.py:238] {'step': 87000, 'lr': 0.00021189486, 'loss': 3.062232494354248, 'src_tokens_per_step': 2034.912, 'src_tokens_per_sec': 11306.926126721226, 'src_real_tokens_per_step': 1652.656, 'src_real_tokens_per_sec': 9182.93238473339, 'trg_tokens_per_step': 2034.912, 'trg_tokens_per_sec': 11306.926126721226, 'trg_real_tokens_per_step': 1688.8, 'trg_real_tokens_per_sec': 9383.765412365155, 'samples_per_step': 55.861, 'samples_per_sec': 310.3899335031561, 'this_step_loss': 80024.5234375}
I1209 16:40:02.400270 140177781462848 callbacks.py:220] Update 88000	TrainingLoss=3.06	Speed 0.189 secs/step 5.3 steps/sec
I1209 16:40:02.405659 140177781462848 callbacks.py:238] {'step': 88000, 'lr': 0.00021068745, 'loss': 3.0590410232543945, 'src_tokens_per_step': 2035.104, 'src_tokens_per_sec': 10781.274042668332, 'src_real_tokens_per_step': 1649.056, 'src_real_tokens_per_sec': 8736.125843055917, 'trg_tokens_per_step': 2035.104, 'trg_tokens_per_sec': 10781.274042668332, 'trg_real_tokens_per_step': 1689.328, 'trg_real_tokens_per_sec': 8949.472909469398, 'samples_per_step': 55.845, 'samples_per_sec': 295.84741070373457, 'this_step_loss': 95801.890625}
I1209 16:40:03.123421 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-88000	Elapsed 0.71s
I1209 16:43:01.726208 140177781462848 callbacks.py:220] Update 89000	TrainingLoss=3.06	Speed 0.179 secs/step 5.6 steps/sec
I1209 16:43:01.745811 140177781462848 callbacks.py:238] {'step': 89000, 'lr': 0.00020950047, 'loss': 3.056030511856079, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 11398.21640758505, 'src_real_tokens_per_step': 1650.96, 'src_real_tokens_per_sec': 9247.428607503016, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 11398.21640758505, 'trg_real_tokens_per_step': 1690.88, 'trg_real_tokens_per_sec': 9471.030239287868, 'samples_per_step': 55.791, 'samples_per_sec': 312.49896390051896, 'this_step_loss': 154822.03125}
I1209 16:46:05.126751 140177781462848 callbacks.py:220] Update 90000	TrainingLoss=3.05	Speed 0.183 secs/step 5.5 steps/sec
I1209 16:46:05.341473 140177781462848 callbacks.py:238] {'step': 90000, 'lr': 0.00020833334, 'loss': 3.052952527999878, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 11101.703554540929, 'src_real_tokens_per_step': 1652.016, 'src_real_tokens_per_sec': 9012.627325055868, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 11101.703554540929, 'trg_real_tokens_per_step': 1689.632, 'trg_real_tokens_per_sec': 9217.842643466403, 'samples_per_step': 55.846, 'samples_per_sec': 304.6696797095609, 'this_step_loss': 211228.921875}
I1209 16:46:06.407704 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-90000	Elapsed 1.06s
I1209 16:46:14.506070 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=74.77 (Best 74.77)  step=90000	Elapsed 8.10s  FromSTART 17616.05s
I1209 16:46:14.547355 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.76 (Best 6.76)  step=90000	Elapsed 8.10s  FromSTART 17616.05s
I1209 16:48:21.198553 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 16:48:21.540629 140177781462848 seq_generation_validator.py:179] Sample 2353
I1209 16:48:21.540773 140177781462848 seq_generation_validator.py:181]   Data: Rather, our goal should be to feel comfortable away from the constant chatter of activity and technology.
I1209 16:48:21.540828 140177781462848 seq_generation_validator.py:182]   Reference: Stattdessen sollte unser Ziel darin bestehen, uns auch abseits der ständigen Aktivitäts- und Technologieberieselung wohlzufühlen.
I1209 16:48:21.540879 140177781462848 seq_generation_validator.py:183]   Hypothesis: Vielmehr sollte unser Ziel darin bestehen, sich von dem ständigen Chat der Aktivität und der Technik zu fühlen.
I1209 16:48:21.540926 140177781462848 seq_generation_validator.py:179] Sample 1837
I1209 16:48:21.540972 140177781462848 seq_generation_validator.py:181]   Data: For a period of 30 years, namely from 1947 until 1975, the Hale telescope in the Palomar Observatory near San Diego was the largest telescope in the world.
I1209 16:48:21.541017 140177781462848 seq_generation_validator.py:182]   Reference: 30 Jahre lang, nämlich von 1947 bis 1975, war das Hale-Teleskop im Palomar-Observatorium nahe San Diego das größte Fernrohr der Welt.
I1209 16:48:21.541061 140177781462848 seq_generation_validator.py:183]   Hypothesis: Für einen Zeitraum von 30 Jahren, nämlich von 1947 bis 1975, war das Hale Teleskop im Palomar Observservatorium in der Nähe von San Diego das größte Teleskop in der Welt.
I1209 16:48:21.541106 140177781462848 seq_generation_validator.py:179] Sample 1587
I1209 16:48:21.541150 140177781462848 seq_generation_validator.py:181]   Data: I believe that the German foreign policy is motivated by the economic policy, that is, by export and its relations with certain regions such as Russia, China or the Near East.
I1209 16:48:21.541194 140177781462848 seq_generation_validator.py:182]   Reference: Ich glaube, dass die deutsche Außenpolitik von der Wirtschaftspolitik motiviert ist, also von den Exporten und seinen Beziehungen zu bestimmten Regionen, etwa Russland, China oder dem Nahen Osten.
I1209 16:48:21.541241 140177781462848 seq_generation_validator.py:183]   Hypothesis: Meiner Ansicht nach ist die deutsche Außenpolitik von der Wirtschaftspolitik, nämlich durch den Export und die Beziehungen zu bestimmten Regionen wie Russland, China oder den Nahen Osten.
I1209 16:48:21.541287 140177781462848 seq_generation_validator.py:179] Sample 233
I1209 16:48:21.541361 140177781462848 seq_generation_validator.py:181]   Data: Other highway officials admitted having their palms greased by inflating invoices by 30 to 40%, and by false add-ons.
I1209 16:48:21.541407 140177781462848 seq_generation_validator.py:182]   Reference: Andere Beamte des Straßenbauamtes haben zugegeben, dass sie sich schmieren ließen, indem sie die Rechnungen mit falschen Zusatzleistungen künstlich um 30-40% aufblähten.
I1209 16:48:21.541451 140177781462848 seq_generation_validator.py:183]   Hypothesis: Weitere Autobahnbeamte haben ihre Palmen mit ihren Palmen vertriebenen, indem sie die Rechnungsabschlüsse um 30 bis 40% und falsche Ergänzungen zerstreuen.
I1209 16:48:21.541496 140177781462848 seq_generation_validator.py:179] Sample 2884
I1209 16:48:21.541539 140177781462848 seq_generation_validator.py:181]   Data: I remember him saying that if Mincy had given Bayamon one championship, Gausse would help get another.
I1209 16:48:21.541586 140177781462848 seq_generation_validator.py:182]   Reference: Ich weiß noch, wie er sagte, dass, wenn Mincy eine Meisterschaft für Bayamon ermöglicht hatte, Gausse für eine weitere sorgen würde.
I1209 16:48:21.541661 140177781462848 seq_generation_validator.py:183]   Hypothesis: Ich erinnere mich daran, dass Gausse, wenn Mincy Bayamon eine Meisterschaft gegeben hatte, dann würde Gausse helfen, einen anderen zu erhalten.
I1209 16:48:22.313482 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.77s
I1209 16:48:23.833739 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.52s
I1209 16:48:23.833992 140177781462848 training_utils.py:359] Evaluating bleu at step=90000 with bad count=1 (early_stop_patience=0).
I1209 16:48:23.834076 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.22 (Best 18.42)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:48:23.835355 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.40 (Best 18.52)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:48:23.836189 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.22 (Best 18.42)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:48:23.836925 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.38 (Best 47.32)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:48:23.837622 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.68 (Best 18.93)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:48:23.838313 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.86 (Best 19.03)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:48:23.838989 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.68 (Best 18.93)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:48:23.839640 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.70 (Best 48.66)  step=90000	Elapsed 115.84s  FromSTART 17726.31s
I1209 16:51:22.114347 140177781462848 callbacks.py:220] Update 91000	TrainingLoss=3.05	Speed 0.178 secs/step 5.6 steps/sec
I1209 16:51:22.123069 140177781462848 callbacks.py:238] {'step': 91000, 'lr': 0.0002071855, 'loss': 3.0499796867370605, 'src_tokens_per_step': 2034.96, 'src_tokens_per_sec': 11419.519372991006, 'src_real_tokens_per_step': 1650.464, 'src_real_tokens_per_sec': 9261.855575748037, 'trg_tokens_per_step': 2034.96, 'trg_tokens_per_sec': 11419.519372991006, 'trg_real_tokens_per_step': 1684.64, 'trg_real_tokens_per_sec': 9453.639932242191, 'samples_per_step': 56.029, 'samples_per_sec': 314.41613149610464, 'this_step_loss': 52392.65234375}
I1209 16:54:25.141458 140177781462848 callbacks.py:220] Update 92000	TrainingLoss=3.05	Speed 0.183 secs/step 5.5 steps/sec
I1209 16:54:25.427683 140177781462848 callbacks.py:238] {'step': 92000, 'lr': 0.00020605639, 'loss': 3.0471394062042236, 'src_tokens_per_step': 2034.768, 'src_tokens_per_sec': 11122.719102339419, 'src_real_tokens_per_step': 1650.4, 'src_real_tokens_per_sec': 9021.635688442602, 'trg_tokens_per_step': 2034.768, 'trg_tokens_per_sec': 11122.719102339419, 'trg_real_tokens_per_step': 1689.952, 'trg_real_tokens_per_sec': 9237.840084194713, 'samples_per_step': 55.752, 'samples_per_sec': 304.7589874588294, 'this_step_loss': 44130.13671875}
I1209 16:54:26.292122 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-92000	Elapsed 0.86s
I1209 16:57:17.389540 140177781462848 callbacks.py:220] Update 93000	TrainingLoss=3.04	Speed 0.171 secs/step 5.8 steps/sec
I1209 16:57:17.397064 140177781462848 callbacks.py:238] {'step': 93000, 'lr': 0.00020494558, 'loss': 3.0441930294036865, 'src_tokens_per_step': 2035.168, 'src_tokens_per_sec': 11900.00638623156, 'src_real_tokens_per_step': 1649.312, 'src_real_tokens_per_sec': 9643.834480931475, 'trg_tokens_per_step': 2035.168, 'trg_tokens_per_sec': 11900.00638623156, 'trg_real_tokens_per_step': 1690.992, 'trg_real_tokens_per_sec': 9887.545204654594, 'samples_per_step': 56.106, 'samples_per_sec': 328.0622328505106, 'this_step_loss': 125486.5}
I1209 17:00:12.119005 140177781462848 callbacks.py:220] Update 94000	TrainingLoss=3.04	Speed 0.175 secs/step 5.7 steps/sec
I1209 17:00:12.166488 140177781462848 callbacks.py:238] {'step': 94000, 'lr': 0.00020385251, 'loss': 3.041468620300293, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 11652.337284556808, 'src_real_tokens_per_step': 1649.824, 'src_real_tokens_per_sec': 9446.944685419707, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 11652.337284556808, 'trg_real_tokens_per_step': 1687.376, 'trg_real_tokens_per_sec': 9661.968631505399, 'samples_per_step': 55.759, 'samples_per_sec': 319.27780703536706, 'this_step_loss': 90337.2109375}
I1209 17:00:12.969278 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-94000	Elapsed 0.79s
I1209 17:03:07.449370 140177781462848 callbacks.py:220] Update 95000	TrainingLoss=3.04	Speed 0.174 secs/step 5.7 steps/sec
I1209 17:03:07.469191 140177781462848 callbacks.py:238] {'step': 95000, 'lr': 0.00020277678, 'loss': 3.0387141704559326, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 11668.446713245483, 'src_real_tokens_per_step': 1652.176, 'src_real_tokens_per_sec': 9473.193459048996, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 11668.446713245483, 'trg_real_tokens_per_step': 1687.488, 'trg_real_tokens_per_sec': 9675.664265685782, 'samples_per_step': 55.976, 'samples_per_sec': 320.95338333429765, 'this_step_loss': 161102.1875}
I1209 17:06:12.356382 140177781462848 callbacks.py:220] Update 96000	TrainingLoss=3.04	Speed 0.185 secs/step 5.4 steps/sec
I1209 17:06:12.363840 140177781462848 callbacks.py:238] {'step': 96000, 'lr': 0.00020171789, 'loss': 3.035886764526367, 'src_tokens_per_step': 2035.232, 'src_tokens_per_sec': 11012.752007600648, 'src_real_tokens_per_step': 1652.688, 'src_real_tokens_per_sec': 8942.785436715567, 'trg_tokens_per_step': 2035.232, 'trg_tokens_per_sec': 11012.752007600648, 'trg_real_tokens_per_step': 1688.912, 'trg_real_tokens_per_sec': 9138.795488013566, 'samples_per_step': 55.985, 'samples_per_sec': 302.93790641338296, 'this_step_loss': 104776.84375}
I1209 17:06:13.234325 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-96000	Elapsed 0.86s
I1209 17:09:12.007081 140177781462848 callbacks.py:220] Update 97000	TrainingLoss=3.03	Speed 0.179 secs/step 5.6 steps/sec
I1209 17:09:12.013847 140177781462848 callbacks.py:238] {'step': 97000, 'lr': 0.0002006754, 'loss': 3.033125877380371, 'src_tokens_per_step': 2035.072, 'src_tokens_per_sec': 11388.204115732695, 'src_real_tokens_per_step': 1653.472, 'src_real_tokens_per_sec': 9252.78154072621, 'trg_tokens_per_step': 2035.072, 'trg_tokens_per_sec': 11388.204115732695, 'trg_real_tokens_per_step': 1686.736, 'trg_real_tokens_per_sec': 9438.925923679606, 'samples_per_step': 55.899, 'samples_per_sec': 312.809188994464, 'this_step_loss': 52175.3671875}
I1209 17:12:12.988583 140177781462848 callbacks.py:220] Update 98000	TrainingLoss=3.03	Speed 0.181 secs/step 5.5 steps/sec
I1209 17:12:12.998214 140177781462848 callbacks.py:238] {'step': 98000, 'lr': 0.00019964892, 'loss': 3.0305392742156982, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 11249.676106750678, 'src_real_tokens_per_step': 1650.608, 'src_real_tokens_per_sec': 9124.684217070164, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 11249.676106750678, 'trg_real_tokens_per_step': 1690.8, 'trg_real_tokens_per_sec': 9346.868592798673, 'samples_per_step': 55.916, 'samples_per_sec': 309.10782128869806, 'this_step_loss': 55866.94921875}
I1209 17:12:13.798410 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-98000	Elapsed 0.79s
I1209 17:15:14.275064 140177781462848 callbacks.py:220] Update 99000	TrainingLoss=3.03	Speed 0.180 secs/step 5.5 steps/sec
I1209 17:15:14.280938 140177781462848 callbacks.py:238] {'step': 99000, 'lr': 0.00019863802, 'loss': 3.02789306640625, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 11280.070050875707, 'src_real_tokens_per_step': 1652.56, 'src_real_tokens_per_sec': 9160.300938819504, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 11280.070050875707, 'trg_real_tokens_per_step': 1687.664, 'trg_real_tokens_per_sec': 9354.885827813743, 'samples_per_step': 56.063, 'samples_per_sec': 310.76266612591246, 'this_step_loss': 98833.9609375}
I1209 17:18:18.041437 140177781462848 callbacks.py:220] Update 100000	TrainingLoss=3.03	Speed 0.184 secs/step 5.4 steps/sec
I1209 17:18:18.053244 140177781462848 callbacks.py:238] {'step': 100000, 'lr': 0.00019764237, 'loss': 3.0252463817596436, 'src_tokens_per_step': 2034.912, 'src_tokens_per_sec': 11078.187460351775, 'src_real_tokens_per_step': 1651.008, 'src_real_tokens_per_sec': 8988.190212913612, 'trg_tokens_per_step': 2034.912, 'trg_tokens_per_sec': 11078.187460351775, 'trg_real_tokens_per_step': 1688.736, 'trg_real_tokens_per_sec': 9193.5837908689, 'samples_per_step': 55.913, 'samples_per_sec': 304.3938487122042, 'this_step_loss': 93368.921875}
I1209 17:18:18.860337 140177781462848 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_dy-1209/ckpt-100000	Elapsed 0.80s
I1209 17:18:25.130087 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=74.58 (Best 74.58)  step=100000	Elapsed 6.27s  FromSTART 19546.68s
I1209 17:18:25.132941 140177781462848 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.73 (Best 6.73)  step=100000	Elapsed 6.27s  FromSTART 19546.68s
I1209 17:20:17.698792 140177781462848 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 17:20:17.709143 140177781462848 seq_generation_validator.py:179] Sample 183
I1209 17:20:17.709357 140177781462848 seq_generation_validator.py:181]   Data: This morning, it is his son who will finish the beer at the feet of the deceased.
I1209 17:20:17.709467 140177781462848 seq_generation_validator.py:182]   Reference: Heute Morgen wird sein Sohn das Bier zu Füßen des Verstorbenen austrinken.
I1209 17:20:17.709548 140177781462848 seq_generation_validator.py:183]   Hypothesis: Heute Morgen ist es sein Sohn, der das Bier auf den Füßen des Verstorbenen beenden wird.
I1209 17:20:17.709626 140177781462848 seq_generation_validator.py:179] Sample 1650
I1209 17:20:17.709704 140177781462848 seq_generation_validator.py:181]   Data: "This harms business and we must fight against it," challenges Pattloch.
I1209 17:20:17.709780 140177781462848 seq_generation_validator.py:182]   Reference: "Das ist geschäftsschädigend, man muss es bekämpfen", fordert Pattloch.
I1209 17:20:17.709852 140177781462848 seq_generation_validator.py:183]   Hypothesis: "Diese Geschäftsleute und wir müssen gegen ihn kämpfen".
I1209 17:20:17.709923 140177781462848 seq_generation_validator.py:179] Sample 1015
I1209 17:20:17.709995 140177781462848 seq_generation_validator.py:181]   Data: This kind of joint audit could contribute to curtailing these efforts to specify our law, to reduce and perfect boundaries, when it does not have such a positive impact.
I1209 17:20:17.710077 140177781462848 seq_generation_validator.py:182]   Reference: Gerade eine solche gemeinsame Kontrolle könnte dazu beitragen, dass wir aufhören, uns ständig um eine Perfektionierung der Gesetze und die Senkung und Austarierung von Schwellenwerten zu bemühen, wenn sich dies augenscheinlich nicht besonders positiv auswirkt.
I1209 17:20:17.710157 140177781462848 seq_generation_validator.py:183]   Hypothesis: Eine solche gemeinsame Prüfung könnte dazu beitragen, diese Bemühungen, unser Gesetz festzulegen, zu reduzieren, die Grenzen zu reduzieren, wenn sie keine so positiven Auswirkungen hat.
I1209 17:20:17.710232 140177781462848 seq_generation_validator.py:179] Sample 1954
I1209 17:20:17.710321 140177781462848 seq_generation_validator.py:181]   Data: This is at least suggested by one newly added article: in all issues affecting Sharia law, the Al Ashar University must be consulted, the country's most important Islamic institution, which has great influence throughout the whole of Sunni Islam.
I1209 17:20:17.710397 140177781462848 seq_generation_validator.py:182]   Reference: Das legt zumindest ein neu hinzugeführter Artikel nahe: In allen die Scharia betreffenden Angelegenheiten müsse die Al-Ashar-Universität zurate gezogen werden, die wichtigste islamische Institution des Landes mit großer Strahlkraft in den gesamten sunnitischen Islam.
I1209 17:20:17.710470 140177781462848 seq_generation_validator.py:183]   Hypothesis: Das ist zumindest von einem neuen Artikel vorgeschlagen worden: In allen Fragen, die das Scharia-Gesetz betreffen, muss die Al AShar-Universität konsultiert werden, die wichtigste islamische Institution des Landes, die über den gesamten sunnitischen Islam großen Einfluß hat.
I1209 17:20:17.710547 140177781462848 seq_generation_validator.py:179] Sample 2804
I1209 17:20:17.710630 140177781462848 seq_generation_validator.py:181]   Data: Some people think they're going to die and don't want to know anything.
I1209 17:20:17.710702 140177781462848 seq_generation_validator.py:182]   Reference: Einige denken, dass sie sterben werden, und möchten nichts hören.
I1209 17:20:17.710751 140177781462848 seq_generation_validator.py:183]   Hypothesis: Einige denken, sie werden sterben und wollen nichts wissen.
I1209 17:20:18.482574 140177781462848 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.77s
I1209 17:20:19.884118 140177781462848 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.40s
I1209 17:20:19.884298 140177781462848 training_utils.py:359] Evaluating bleu at step=100000 with bad count=2 (early_stop_patience=0).
I1209 17:20:19.884362 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.16 (Best 18.42)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.885447 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.29 (Best 18.52)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.886037 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.16 (Best 18.42)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.886634 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.06 (Best 47.32)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.887126 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.69 (Best 18.93)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.887616 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.83 (Best 19.03)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.888084 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.69 (Best 18.93)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.888556 140177781462848 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.38 (Best 48.66)  step=100000	Elapsed 101.93s  FromSTART 19643.00s
I1209 17:20:19.898374 140177781462848 trainer.py:315] {'loss': [3.0252463817596436], 'src_tokens': [203368768.0], 'src_real_tokens': [165164288.0], 'trg_tokens': [203368768.0], 'trg_real_tokens': [168868384.0], 'samples': [5591388.0], 'this_step_loss': [93368.921875]}
