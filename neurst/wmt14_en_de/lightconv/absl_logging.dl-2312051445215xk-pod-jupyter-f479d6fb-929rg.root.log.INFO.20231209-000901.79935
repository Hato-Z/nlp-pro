I1209 00:09:01.973133 140215084762944 configurable.py:222] loading configurations from wmt14_en_de/training_args.yml
I1209 00:09:02.007557 140215084762944 configurable.py:222] loading configurations from wmt14_en_de/translation_bpe.yml
I1209 00:09:02.018716 140215084762944 configurable.py:222] loading configurations from wmt14_en_de/validation_args.yml
I1209 00:09:02.023809 140215084762944 hparams_sets.py:50] matched the pre-defined hyper-parameters set: lightweight_conv_toy
I1209 00:09:09.923604 140215084762944 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I1209 00:09:09.929236 140215084762944 training_utils.py:132] Using distribution strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f8608a42860> with num_replicas_in_sync=1
I1209 00:09:09.929378 140215084762944 flags_core.py:514] ==========================================================================
I1209 00:09:09.929431 140215084762944 flags_core.py:515] Parsed all matched flags: 
I1209 00:09:09.929765 140215084762944 flags_core.py:520]  distribution_strategy: mirrored     # (default: mirrored) The distribution strategy.
I1209 00:09:09.929820 140215084762944 flags_core.py:520]  dtype: float16     # (default: float16) The computation type of the whole model.
I1209 00:09:09.929871 140215084762944 flags_core.py:520]  enable_check_numerics: None     # (default: None) Whether to open the tf.debugging.enable_check_numerics. Note that this may lower down the training speed.
I1209 00:09:09.929927 140215084762944 flags_core.py:520]  enable_xla: True     # (default: None) Whether to enable XLA for training.
I1209 00:09:09.929974 140215084762944 flags_core.py:520]  hparams_set: lightweight_conv_toy     # (default: None) A string indicating a set of pre-defined hyper-parameters, e.g. transformer_base, transformer_big or transformer_768_16e_3d.
I1209 00:09:09.930024 140215084762944 flags_core.py:520]  model_dir: ./wmt14_en_de/benchmark_lg3-1208     # (default: None) The path to the checkpoint for saving and loading.
I1209 00:09:09.930070 140215084762944 flags_core.py:520]  enable_quant: False     # (default: False) Whether to enable quantization for finetuning.
I1209 00:09:09.930115 140215084762944 flags_core.py:520]  quant_params: None     # (default: None) A dict of parameters for quantization.
I1209 00:09:09.930166 140215084762944 flags_core.py:523]  entry.class: trainer
I1209 00:09:09.930216 140215084762944 flags_core.py:527]  entry.params:
I1209 00:09:09.930348 140215084762944 flags_core.py:535]    criterion.class: label_smoothed_cross_entropy
I1209 00:09:09.930465 140215084762944 flags_core.py:510]    criterion.params: {"label_smoothing": 0.1}     # The criterion for training or evaluation.
I1209 00:09:09.930514 140215084762944 flags_core.py:535]    optimizer.class: Adam
I1209 00:09:09.930578 140215084762944 flags_core.py:510]    optimizer.params: {"epsilon": 1e-09, "beta_1": 0.9, "beta_2": 0.98}     # The optimizer for training.
I1209 00:09:09.930626 140215084762944 flags_core.py:535]    lr_schedule.class: noam
I1209 00:09:09.930681 140215084762944 flags_core.py:510]    lr_schedule.params: {"dmodel": 256, "warmup_steps": 3000, "initial_factor": 1.0}     # The learning schedule for training.
I1209 00:09:09.930728 140215084762944 flags_core.py:535]    validator.class: SeqGenerationValidator
I1209 00:09:09.930803 140215084762944 flags_core.py:510]    validator.params: {"eval_dataset.params": {"src_file": "/root/neurst/wmt14_en_de/newstest2013.en.txt", "trg_file": "/root/neurst/wmt14_en_de/newstest2013.de.txt"}, "eval_search_method.params": {"beam_size": 4, "length_penalty": 0.6, "maximum_decode_length": 160, "extra_decode_length": 50}, "eval_steps": 10000, "eval_start_at": 10000, "eval_criterion.class": "label_smoothed_cross_entropy", "eval_criterion.params": {}, "eval_dataset.class": "ParallelTextDataset", "eval_batch_size": 64, "eval_metric.class": "bleu", "eval_metric.params": {}, "eval_search_method.class": "beam_search", "eval_auto_average_checkpoints": true, "eval_top_checkpoints_to_keep": 10}     # The validation process while training.
I1209 00:09:09.930859 140215084762944 flags_core.py:535]    pruning_schedule.class: PolynomialDecay
I1209 00:09:09.930911 140215084762944 flags_core.py:510]    pruning_schedule.params: {}     # The schedule for weight weight_pruning.
I1209 00:09:09.930960 140215084762944 flags_core.py:510]    train_steps: 100000     # (default: 10000000) The maximum steps for training loop.
I1209 00:09:09.931008 140215084762944 flags_core.py:510]    summary_steps: 1000     # (default: 200) Doing summary(logging & tensorboard) this every steps.
I1209 00:09:09.931054 140215084762944 flags_core.py:510]    save_checkpoint_steps: 2000     # (default: 1000) Saving checkpoints this every steps.
I1209 00:09:09.931115 140215084762944 flags_core.py:523]  task.class: translation
I1209 00:09:09.931165 140215084762944 flags_core.py:527]  task.params:
I1209 00:09:09.931264 140215084762944 flags_core.py:510]    batch_size: 2048     # The number of samples per update.
I1209 00:09:09.931320 140215084762944 flags_core.py:535]    src_data_pipeline.class: TextDataPipeline
I1209 00:09:09.931378 140215084762944 flags_core.py:510]    src_data_pipeline.params: {"language": "en", "tokenizer": "moses", "subtokenizer": "bpe", "subtokenizer_codes": "/root/neurst/wmt14_en_de/codes.bpe", "vocab_path": "/root/neurst/wmt14_en_de/vocab.en"}     # The source side data pipeline.
I1209 00:09:09.931428 140215084762944 flags_core.py:535]    trg_data_pipeline.class: TextDataPipeline
I1209 00:09:09.931483 140215084762944 flags_core.py:510]    trg_data_pipeline.params: {"language": "de", "tokenizer": "moses", "subtokenizer": "bpe", "subtokenizer_codes": "/root/neurst/wmt14_en_de/codes.bpe", "vocab_path": "/root/neurst/wmt14_en_de/vocab.de"}     # The target side data pipeline.
I1209 00:09:09.931530 140215084762944 flags_core.py:510]    max_src_len: 128     # The maximum source length of training data.
I1209 00:09:09.931575 140215084762944 flags_core.py:510]    max_trg_len: 128     # The maximum target length of training data.
I1209 00:09:09.931622 140215084762944 flags_core.py:510]    batch_by_tokens: True     # Whether to batch the data by word tokens.
I1209 00:09:09.931673 140215084762944 flags_core.py:523]  model.class: LightConvolutionModel
I1209 00:09:09.931722 140215084762944 flags_core.py:527]  model.params:
I1209 00:09:09.931856 140215084762944 flags_core.py:510]    encoder.params: {}     # The encoder.
I1209 00:09:09.931910 140215084762944 flags_core.py:510]    decoder.params: {}     # The decoder.
I1209 00:09:09.931956 140215084762944 flags_core.py:510]    modality.share_source_target_embedding: False     # (default: False) Whether to share source and target embedding table.
I1209 00:09:09.932005 140215084762944 flags_core.py:510]    modality.share_embedding_and_softmax_weights: True     # (default: False) Whether to share the target embedding table and softmax weights.
I1209 00:09:09.932059 140215084762944 flags_core.py:510]    modality.dim: 256     # The default embedding dimension for both source and target side.
I1209 00:09:09.932105 140215084762944 flags_core.py:510]    modality.timing: sinusoids     # The arbitrary parameters for positional encoding of both source and target side.
I1209 00:09:09.932151 140215084762944 flags_core.py:510]    encoder.num_layers: 5     # The number of stacking layers of the encoder.
I1209 00:09:09.932200 140215084762944 flags_core.py:510]    encoder.conv_kernel_size_list: [3, 7, 15, 15, 15]     # A list of kernel sizes for each encoder layers. The length of the list must be equal to `encoder.num_layers`.
I1209 00:09:09.932245 140215084762944 flags_core.py:510]    encoder.num_conv_heads: 4     # The number of heads of encoder convolution shared weights.
I1209 00:09:09.932294 140215084762944 flags_core.py:510]    encoder.conv_hidden_size: 256     # The number of hidden units of the encoder convolution layer.
I1209 00:09:09.932339 140215084762944 flags_core.py:510]    encoder.conv_type: lightweight     # (default: lightweight) The type of encoder conv layer, one of lightweight or dynamic.
I1209 00:09:09.932388 140215084762944 flags_core.py:510]    encoder.filter_size: 1024     # The number of the filter size of encoder ffn.
I1209 00:09:09.932433 140215084762944 flags_core.py:510]    encoder.ffn_activation: relu     # (default: relu) The activation function of encoder ffn layer.
I1209 00:09:09.932483 140215084762944 flags_core.py:510]    encoder.conv_weight_dropout_rate: 0.1     # (default: 0.0) The dropout rate of encoder convolution weights.
I1209 00:09:09.932529 140215084762944 flags_core.py:510]    encoder.glu_after_proj: True     # (default: False) Whether to apply glu activation after input projection in encoder convolution layer.
I1209 00:09:09.932575 140215084762944 flags_core.py:510]    encoder.ffn_dropout_rate: 0.1     # (default: 0.0) The dropout rate of encoder ffn layer.
I1209 00:09:09.932621 140215084762944 flags_core.py:510]    encoder.layer_postprocess_dropout_rate: 0.1     # (default: 0.0) The dropout rate for each layer's post process in encoder.
I1209 00:09:09.932667 140215084762944 flags_core.py:510]    decoder.num_layers: 5     # The number of stacking layers of the decoder.
I1209 00:09:09.932713 140215084762944 flags_core.py:510]    decoder.conv_kernel_size_list: [3, 7, 15, 15, 15]     # A list of kernel sizes for each decoder layers. The length of the list must be equal to `decoder.num_layers`.
I1209 00:09:09.932758 140215084762944 flags_core.py:510]    decoder.num_conv_heads: 4     # The number of heads of decoder convolution shared weights.
I1209 00:09:09.932802 140215084762944 flags_core.py:510]    decoder.conv_hidden_size: 256     # The number of hidden units of the decoder convolution layer.
I1209 00:09:09.932847 140215084762944 flags_core.py:510]    decoder.num_attention_heads: 4     # The number of heads of decoder's encoder-decoder attention.
I1209 00:09:09.932891 140215084762944 flags_core.py:510]    decoder.conv_type: lightweight     # (default: lightweight) The type of decoder conv layer, one of lightweight or dynamic.
I1209 00:09:09.932935 140215084762944 flags_core.py:510]    decoder.filter_size: 1024     # The number of the filter size of decoder ffn.
I1209 00:09:09.932980 140215084762944 flags_core.py:510]    decoder.ffn_activation: relu     # (default: relu) The activation function of decoder ffn layer.
I1209 00:09:09.933025 140215084762944 flags_core.py:510]    decoder.attention_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder's encoder-decoder attention.
I1209 00:09:09.933071 140215084762944 flags_core.py:510]    decoder.conv_weight_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder convolution weights.
I1209 00:09:09.933117 140215084762944 flags_core.py:510]    decoder.glu_after_proj: True     # (default: False) Whether to apply glu activation after input projection in decoder convolution layer.
I1209 00:09:09.933161 140215084762944 flags_core.py:510]    decoder.attention_type: dot_product     # (default: dot_product) The type of the attention function of decoder's encoder-decoder attention.
I1209 00:09:09.933208 140215084762944 flags_core.py:510]    decoder.ffn_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder ffn layer.
I1209 00:09:09.933254 140215084762944 flags_core.py:510]    decoder.layer_postprocess_dropout_rate: 0.1     # (default: 0.0) The dropout rate for each layer's post process in decoder.
I1209 00:09:09.933309 140215084762944 flags_core.py:523]  dataset.class: ParallelTextDataset
I1209 00:09:09.933360 140215084762944 flags_core.py:527]  dataset.params:
I1209 00:09:09.933425 140215084762944 flags_core.py:510]    src_file: /root/neurst/wmt14_en_de/train.en.tok.bpe.txt     # The source text file
I1209 00:09:09.933471 140215084762944 flags_core.py:510]    trg_file: /root/neurst/wmt14_en_de/train.de.tok.bpe.txt     # The target text file
I1209 00:09:09.933516 140215084762944 flags_core.py:510]    data_is_processed: True     # Whether the text data is already processed.
I1209 00:09:09.933562 140215084762944 flags_core.py:545] 
I1209 00:09:09.933603 140215084762944 flags_core.py:546] Other flags:
I1209 00:09:09.933650 140215084762944 flags_core.py:548]  config_paths: ['wmt14_en_de/training_args.yml,wmt14_en_de/translation_bpe.yml,wmt14_en_de/validation_args.yml']
I1209 00:09:09.933692 140215084762944 flags_core.py:563] ==========================================================================
I1209 00:09:09.933744 140215084762944 training_utils.py:74] Using float16 as computation dtype.
I1209 00:09:10.003745 140215084762944 device_compatibility_check.py:124] Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100S-PCIE-32GB, compute capability 7.0
I1209 00:09:10.004550 140215084762944 registry.py:39] Creating task: <class 'neurst.tasks.translation.Translation'>
I1209 00:09:10.004642 140215084762944 registry.py:41]   (task) arguments: 
I1209 00:09:10.004693 140215084762944 registry.py:51]     batch_size: 2048
I1209 00:09:10.004739 140215084762944 registry.py:51]     src_data_pipeline.class: TextDataPipeline
I1209 00:09:10.004783 140215084762944 registry.py:44]     src_data_pipeline.params:
I1209 00:09:10.004826 140215084762944 registry.py:49]       language: en
I1209 00:09:10.004868 140215084762944 registry.py:49]       tokenizer: moses
I1209 00:09:10.004909 140215084762944 registry.py:49]       subtokenizer: bpe
I1209 00:09:10.004951 140215084762944 registry.py:49]       subtokenizer_codes: /root/neurst/wmt14_en_de/codes.bpe
I1209 00:09:10.004992 140215084762944 registry.py:49]       vocab_path: /root/neurst/wmt14_en_de/vocab.en
I1209 00:09:10.005033 140215084762944 registry.py:51]     trg_data_pipeline.class: TextDataPipeline
I1209 00:09:10.005074 140215084762944 registry.py:44]     trg_data_pipeline.params:
I1209 00:09:10.005116 140215084762944 registry.py:49]       language: de
I1209 00:09:10.005157 140215084762944 registry.py:49]       tokenizer: moses
I1209 00:09:10.005198 140215084762944 registry.py:49]       subtokenizer: bpe
I1209 00:09:10.005239 140215084762944 registry.py:49]       subtokenizer_codes: /root/neurst/wmt14_en_de/codes.bpe
I1209 00:09:10.005280 140215084762944 registry.py:49]       vocab_path: /root/neurst/wmt14_en_de/vocab.de
I1209 00:09:10.005327 140215084762944 registry.py:51]     max_src_len: 128
I1209 00:09:10.005369 140215084762944 registry.py:51]     max_trg_len: 128
I1209 00:09:10.005411 140215084762944 registry.py:51]     batch_by_tokens: True
I1209 00:09:10.005452 140215084762944 registry.py:51]     shuffle_buffer: 0
I1209 00:09:10.005494 140215084762944 registry.py:51]     batch_size_per_gpu: None
I1209 00:09:10.005536 140215084762944 registry.py:51]     cache_dataset: None
I1209 00:09:10.005577 140215084762944 registry.py:51]     truncate_src: None
I1209 00:09:10.005618 140215084762944 registry.py:51]     truncate_trg: None
I1209 00:09:10.005659 140215084762944 registry.py:51]     target_begin_of_sentence: bos
I1209 00:09:10.005700 140215084762944 registry.py:51]     gpu_efficient_level: 0
I1209 00:09:10.005742 140215084762944 registry.py:51]     auto_scaling_batch_size: None
I1209 00:09:11.544044 140215084762944 registry.py:39] Creating dataset: <class 'neurst.data.datasets.parallel_text_dataset.ParallelTextDataset'>
I1209 00:09:11.544207 140215084762944 registry.py:41]   (dataset) arguments: 
I1209 00:09:11.544261 140215084762944 registry.py:51]     src_file: /root/neurst/wmt14_en_de/train.en.tok.bpe.txt
I1209 00:09:11.544314 140215084762944 registry.py:51]     trg_file: /root/neurst/wmt14_en_de/train.de.tok.bpe.txt
I1209 00:09:11.544361 140215084762944 registry.py:51]     data_is_processed: True
I1209 00:09:11.544404 140215084762944 registry.py:51]     raw_trg_file: None
I1209 00:09:11.544447 140215084762944 registry.py:51]     src_lang: None
I1209 00:09:11.544490 140215084762944 registry.py:51]     trg_lang: None
I1209 00:09:11.544682 140215084762944 registry.py:39] Creating model: <class 'neurst.models.light_convolution_model.LightConvolutionModel'>
I1209 00:09:11.544734 140215084762944 registry.py:41]   (model) arguments: 
I1209 00:09:11.544778 140215084762944 registry.py:44]     encoder.params:
I1209 00:09:11.544826 140215084762944 registry.py:44]     decoder.params:
I1209 00:09:11.544870 140215084762944 registry.py:51]     modality.share_source_target_embedding: False
I1209 00:09:11.544913 140215084762944 registry.py:51]     modality.share_embedding_and_softmax_weights: True
I1209 00:09:11.544955 140215084762944 registry.py:51]     modality.dim: 256
I1209 00:09:11.544996 140215084762944 registry.py:51]     modality.timing: sinusoids
I1209 00:09:11.545038 140215084762944 registry.py:51]     encoder.num_layers: 5
I1209 00:09:11.545084 140215084762944 registry.py:51]     encoder.conv_kernel_size_list: [3, 7, 15, 15, 15]
I1209 00:09:11.545126 140215084762944 registry.py:51]     encoder.num_conv_heads: 4
I1209 00:09:11.545167 140215084762944 registry.py:51]     encoder.conv_hidden_size: 256
I1209 00:09:11.545207 140215084762944 registry.py:51]     encoder.conv_type: lightweight
I1209 00:09:11.545251 140215084762944 registry.py:51]     encoder.filter_size: 1024
I1209 00:09:11.545298 140215084762944 registry.py:51]     encoder.ffn_activation: relu
I1209 00:09:11.545344 140215084762944 registry.py:51]     encoder.conv_weight_dropout_rate: 0.1
I1209 00:09:11.545385 140215084762944 registry.py:51]     encoder.glu_after_proj: True
I1209 00:09:11.545427 140215084762944 registry.py:51]     encoder.ffn_dropout_rate: 0.1
I1209 00:09:11.545469 140215084762944 registry.py:51]     encoder.layer_postprocess_dropout_rate: 0.1
I1209 00:09:11.545510 140215084762944 registry.py:51]     decoder.num_layers: 5
I1209 00:09:11.545553 140215084762944 registry.py:51]     decoder.conv_kernel_size_list: [3, 7, 15, 15, 15]
I1209 00:09:11.545594 140215084762944 registry.py:51]     decoder.num_conv_heads: 4
I1209 00:09:11.545634 140215084762944 registry.py:51]     decoder.conv_hidden_size: 256
I1209 00:09:11.545675 140215084762944 registry.py:51]     decoder.num_attention_heads: 4
I1209 00:09:11.545716 140215084762944 registry.py:51]     decoder.conv_type: lightweight
I1209 00:09:11.545757 140215084762944 registry.py:51]     decoder.filter_size: 1024
I1209 00:09:11.545798 140215084762944 registry.py:51]     decoder.ffn_activation: relu
I1209 00:09:11.545839 140215084762944 registry.py:51]     decoder.attention_dropout_rate: 0.1
I1209 00:09:11.545881 140215084762944 registry.py:51]     decoder.conv_weight_dropout_rate: 0.1
I1209 00:09:11.545922 140215084762944 registry.py:51]     decoder.glu_after_proj: True
I1209 00:09:11.545963 140215084762944 registry.py:51]     decoder.attention_type: dot_product
I1209 00:09:11.546005 140215084762944 registry.py:51]     decoder.ffn_dropout_rate: 0.1
I1209 00:09:11.546046 140215084762944 registry.py:51]     decoder.layer_postprocess_dropout_rate: 0.1
I1209 00:09:11.546088 140215084762944 registry.py:51]     encoder.class: None
I1209 00:09:11.546129 140215084762944 registry.py:51]     decoder.class: None
I1209 00:09:11.546170 140215084762944 registry.py:51]     modality.source.dim: None
I1209 00:09:11.546211 140215084762944 registry.py:51]     modality.target.dim: None
I1209 00:09:11.546252 140215084762944 registry.py:51]     modality.source.timing: None
I1209 00:09:11.546298 140215084762944 registry.py:51]     modality.target.timing: None
I1209 00:09:11.546345 140215084762944 registry.py:51]     encoder.layer_postprocess_epsilon: 1e-06
I1209 00:09:11.546388 140215084762944 registry.py:51]     decoder.layer_postprocess_epsilon: 1e-06
I1209 00:09:11.546429 140215084762944 registry.py:53]   (model) extra args: 
I1209 00:09:11.546475 140215084762944 registry.py:55]     - {'language': 'en', 'vocab_size': 42293, 'eos_id': 42292, 'bos_id': 42291, 'unk_id': 42290, 'pad_id': 42292, 'padding_mode': 2}
I1209 00:09:11.546520 140215084762944 registry.py:55]     - {'language': 'de', 'vocab_size': 43629, 'eos_id': 43628, 'bos_id': 43627, 'unk_id': 43626, 'pad_id': 43628, 'padding_mode': 2}
I1209 00:09:11.546562 140215084762944 registry.py:57]   (model) extra k-v args: 
I1209 00:09:11.546604 140215084762944 registry.py:59]     name: None
I1209 00:09:16.540466 140215084762944 registry.py:39] Creating entry: <class 'neurst.exps.trainer.Trainer'>
I1209 00:09:16.541313 140215084762944 registry.py:41]   (entry) arguments: 
I1209 00:09:16.541391 140215084762944 registry.py:51]     criterion.class: label_smoothed_cross_entropy
I1209 00:09:16.541440 140215084762944 registry.py:44]     criterion.params:
I1209 00:09:16.541493 140215084762944 registry.py:49]       label_smoothing: 0.1
I1209 00:09:16.541538 140215084762944 registry.py:51]     optimizer.class: Adam
I1209 00:09:16.541580 140215084762944 registry.py:44]     optimizer.params:
I1209 00:09:16.541627 140215084762944 registry.py:49]       epsilon: 1e-09
I1209 00:09:16.541670 140215084762944 registry.py:49]       beta_1: 0.9
I1209 00:09:16.541714 140215084762944 registry.py:49]       beta_2: 0.98
I1209 00:09:16.541756 140215084762944 registry.py:51]     lr_schedule.class: noam
I1209 00:09:16.541798 140215084762944 registry.py:44]     lr_schedule.params:
I1209 00:09:16.541840 140215084762944 registry.py:49]       dmodel: 256
I1209 00:09:16.541882 140215084762944 registry.py:49]       warmup_steps: 3000
I1209 00:09:16.541925 140215084762944 registry.py:49]       initial_factor: 1.0
I1209 00:09:16.541967 140215084762944 registry.py:51]     validator.class: SeqGenerationValidator
I1209 00:09:16.542009 140215084762944 registry.py:44]     validator.params:
I1209 00:09:16.542057 140215084762944 registry.py:49]       eval_dataset.params: {'src_file': '/root/neurst/wmt14_en_de/newstest2013.en.txt', 'trg_file': '/root/neurst/wmt14_en_de/newstest2013.de.txt'}
I1209 00:09:16.542106 140215084762944 registry.py:49]       eval_search_method.params: {'beam_size': 4, 'length_penalty': 0.6, 'maximum_decode_length': 160, 'extra_decode_length': 50}
I1209 00:09:16.542153 140215084762944 registry.py:49]       eval_steps: 10000
I1209 00:09:16.542197 140215084762944 registry.py:49]       eval_start_at: 10000
I1209 00:09:16.542240 140215084762944 registry.py:49]       eval_criterion.class: label_smoothed_cross_entropy
I1209 00:09:16.542284 140215084762944 registry.py:49]       eval_criterion.params: {}
I1209 00:09:16.542333 140215084762944 registry.py:49]       eval_dataset.class: ParallelTextDataset
I1209 00:09:16.542375 140215084762944 registry.py:49]       eval_batch_size: 64
I1209 00:09:16.542416 140215084762944 registry.py:49]       eval_metric.class: bleu
I1209 00:09:16.542459 140215084762944 registry.py:49]       eval_metric.params: {}
I1209 00:09:16.542501 140215084762944 registry.py:49]       eval_search_method.class: beam_search
I1209 00:09:16.542545 140215084762944 registry.py:49]       eval_auto_average_checkpoints: True
I1209 00:09:16.542587 140215084762944 registry.py:49]       eval_top_checkpoints_to_keep: 10
I1209 00:09:16.542629 140215084762944 registry.py:51]     pruning_schedule.class: PolynomialDecay
I1209 00:09:16.542670 140215084762944 registry.py:44]     pruning_schedule.params:
I1209 00:09:16.542713 140215084762944 registry.py:51]     train_steps: 100000
I1209 00:09:16.542754 140215084762944 registry.py:51]     summary_steps: 1000
I1209 00:09:16.542796 140215084762944 registry.py:51]     save_checkpoint_steps: 2000
I1209 00:09:16.542839 140215084762944 registry.py:51]     tb_log_dir: None
I1209 00:09:16.542881 140215084762944 registry.py:51]     train_epochs: None
I1209 00:09:16.542922 140215084762944 registry.py:51]     checkpoints_max_to_keep: 8
I1209 00:09:16.542964 140215084762944 registry.py:51]     initial_global_step: None
I1209 00:09:16.543005 140215084762944 registry.py:51]     pretrain_model: None
I1209 00:09:16.543047 140215084762944 registry.py:51]     pretrain_variable_pattern: None
I1209 00:09:16.543089 140215084762944 registry.py:51]     update_cycle: 1
I1209 00:09:16.543131 140215084762944 registry.py:51]     clip_value: None
I1209 00:09:16.543173 140215084762944 registry.py:51]     clip_norm: None
I1209 00:09:16.543214 140215084762944 registry.py:51]     experimental_count_batch_num: None
I1209 00:09:16.543256 140215084762944 registry.py:51]     freeze_variables: None
I1209 00:09:16.543308 140215084762944 registry.py:51]     pruning_variable_pattern: None
I1209 00:09:16.543358 140215084762944 registry.py:51]     nopruning_variable_pattern: None
I1209 00:09:16.543401 140215084762944 registry.py:51]     optimizer_controller: None
I1209 00:09:16.543442 140215084762944 registry.py:51]     optimizer_controller_args: None
I1209 00:09:16.543485 140215084762944 registry.py:57]   (entry) extra k-v args: 
I1209 00:09:16.543532 140215084762944 registry.py:59]     strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f8608a42860>
I1209 00:09:16.543579 140215084762944 registry.py:59]     model: <neurst.models.light_convolution_model.LightConvolutionModel object at 0x7f84d47b1be0>
I1209 00:09:16.543629 140215084762944 registry.py:59]     task: <neurst.tasks.translation.Translation object at 0x7f8608a3e1d0>
I1209 00:09:16.543672 140215084762944 registry.py:59]     model_dir: ./wmt14_en_de/benchmark_lg3-1208
I1209 00:09:16.543720 140215084762944 registry.py:59]     custom_dataset: <neurst.data.datasets.parallel_text_dataset.ParallelTextDataset object at 0x7f8608a3e2e8>
I1209 00:09:16.543951 140215084762944 registry.py:39] Creating criterion: <class 'neurst.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropy'>
I1209 00:09:16.544005 140215084762944 registry.py:41]   (criterion) arguments: 
I1209 00:09:16.544050 140215084762944 registry.py:51]     label_smoothing: 0.1
I1209 00:09:16.544106 140215084762944 label_smoothed_cross_entropy.py:38] Using LabelSmoothedCrossEntropy with label_smoothing=0.1
I1209 00:09:16.544175 140215084762944 registry.py:39] Creating optimizer: <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>
I1209 00:09:16.544220 140215084762944 registry.py:57]   (optimizer) extra k-v args: 
I1209 00:09:16.544265 140215084762944 registry.py:59]     epsilon: 1e-09
I1209 00:09:16.544319 140215084762944 registry.py:59]     beta_1: 0.9
I1209 00:09:16.544363 140215084762944 registry.py:59]     beta_2: 0.98
I1209 00:09:16.544620 140215084762944 registry.py:39] Creating validator: <class 'neurst.training.seq_generation_validator.SeqGenerationValidator'>
I1209 00:09:16.544676 140215084762944 registry.py:41]   (validator) arguments: 
I1209 00:09:16.544720 140215084762944 registry.py:44]     eval_dataset.params:
I1209 00:09:16.544764 140215084762944 registry.py:49]       src_file: /root/neurst/wmt14_en_de/newstest2013.en.txt
I1209 00:09:16.544806 140215084762944 registry.py:49]       trg_file: /root/neurst/wmt14_en_de/newstest2013.de.txt
I1209 00:09:16.544848 140215084762944 registry.py:44]     eval_search_method.params:
I1209 00:09:16.544891 140215084762944 registry.py:49]       beam_size: 4
I1209 00:09:16.544934 140215084762944 registry.py:49]       length_penalty: 0.6
I1209 00:09:16.544976 140215084762944 registry.py:49]       maximum_decode_length: 160
I1209 00:09:16.545017 140215084762944 registry.py:49]       extra_decode_length: 50
I1209 00:09:16.545059 140215084762944 registry.py:51]     eval_steps: 10000
I1209 00:09:16.545101 140215084762944 registry.py:51]     eval_start_at: 10000
I1209 00:09:16.545142 140215084762944 registry.py:51]     eval_criterion.class: label_smoothed_cross_entropy
I1209 00:09:16.545183 140215084762944 registry.py:44]     eval_criterion.params:
I1209 00:09:16.545224 140215084762944 registry.py:51]     eval_dataset.class: ParallelTextDataset
I1209 00:09:16.545266 140215084762944 registry.py:51]     eval_batch_size: 64
I1209 00:09:16.545317 140215084762944 registry.py:51]     eval_metric.class: bleu
I1209 00:09:16.545359 140215084762944 registry.py:44]     eval_metric.params:
I1209 00:09:16.545401 140215084762944 registry.py:51]     eval_search_method.class: beam_search
I1209 00:09:16.545443 140215084762944 registry.py:51]     eval_auto_average_checkpoints: True
I1209 00:09:16.545485 140215084762944 registry.py:51]     eval_top_checkpoints_to_keep: 10
I1209 00:09:16.545526 140215084762944 registry.py:51]     eval_on_begin: False
I1209 00:09:16.545567 140215084762944 registry.py:51]     eval_task_args: None
I1209 00:09:16.545608 140215084762944 registry.py:51]     eval_estop_patience: 0
I1209 00:09:16.545655 140215084762944 registry.py:51]     eval_best_checkpoint_path: None
I1209 00:09:16.545697 140215084762944 registry.py:51]     eval_best_avg_checkpoint_path: None
I1209 00:09:16.545837 140215084762944 registry.py:39] Creating pruning_schedule: <class 'neurst.sparsity.pruning_schedule.PolynomialDecay'>
I1209 00:09:16.545896 140215084762944 registry.py:41]   (pruning_schedule) arguments: 
I1209 00:09:16.545943 140215084762944 registry.py:51]     target_sparsity: 0.0
I1209 00:09:16.545986 140215084762944 registry.py:51]     begin_pruning_step: 0
I1209 00:09:16.546028 140215084762944 registry.py:51]     end_pruning_step: -1
I1209 00:09:16.546070 140215084762944 registry.py:51]     pruning_frequency: 100
I1209 00:09:16.546111 140215084762944 registry.py:51]     initial_sparsity: 0.0
I1209 00:09:16.546151 140215084762944 registry.py:51]     polynomial_power: 3
I1209 00:09:16.546318 140215084762944 translation.py:70] Creating training dataset with GPU efficient level=0.
I1209 00:09:17.948511 140215084762944 dataset_utils.py:330] Filtering empty data and datas exceeded max length={'feature': 128, 'label': 128}
I1209 00:09:18.239493 140215084762944 dataset_utils.py:452] The global batch size is 2048 tokens.
I1209 00:09:18.239692 140215084762944 dataset_utils.py:497] The details of batching logic:
I1209 00:09:18.239754 140215084762944 dataset_utils.py:503]    - batch=256, bucket boundary={'feature': 8, 'label': 8}
I1209 00:09:18.239804 140215084762944 dataset_utils.py:503]    - batch=128, bucket boundary={'feature': 16, 'label': 16}
I1209 00:09:18.239851 140215084762944 dataset_utils.py:503]    - batch=85, bucket boundary={'feature': 24, 'label': 24}
I1209 00:09:18.239896 140215084762944 dataset_utils.py:503]    - batch=64, bucket boundary={'feature': 32, 'label': 32}
I1209 00:09:18.239940 140215084762944 dataset_utils.py:503]    - batch=51, bucket boundary={'feature': 40, 'label': 40}
I1209 00:09:18.239984 140215084762944 dataset_utils.py:503]    - batch=42, bucket boundary={'feature': 48, 'label': 48}
I1209 00:09:18.240028 140215084762944 dataset_utils.py:503]    - batch=36, bucket boundary={'feature': 56, 'label': 56}
I1209 00:09:18.240072 140215084762944 dataset_utils.py:503]    - batch=32, bucket boundary={'feature': 64, 'label': 64}
I1209 00:09:18.240115 140215084762944 dataset_utils.py:503]    - batch=28, bucket boundary={'feature': 72, 'label': 72}
I1209 00:09:18.240159 140215084762944 dataset_utils.py:503]    - batch=25, bucket boundary={'feature': 80, 'label': 80}
I1209 00:09:18.240203 140215084762944 dataset_utils.py:503]    - batch=23, bucket boundary={'feature': 88, 'label': 88}
I1209 00:09:18.240246 140215084762944 dataset_utils.py:503]    - batch=21, bucket boundary={'feature': 96, 'label': 96}
I1209 00:09:18.240297 140215084762944 dataset_utils.py:503]    - batch=19, bucket boundary={'feature': 104, 'label': 104}
I1209 00:09:18.240345 140215084762944 dataset_utils.py:503]    - batch=18, bucket boundary={'feature': 112, 'label': 112}
I1209 00:09:18.240389 140215084762944 dataset_utils.py:503]    - batch=17, bucket boundary={'feature': 120, 'label': 120}
I1209 00:09:18.240433 140215084762944 dataset_utils.py:503]    - batch=16, bucket boundary={'feature': 128, 'label': 128}
I1209 00:09:18.240476 140215084762944 dataset_utils.py:506]   Total 16 input shapes are compiled.
I1209 00:09:24.820643 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:24.823759 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:25.832927 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:25.835545 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:26.867909 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:26.870938 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:27.880811 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:27.884014 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:28.955334 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:29.017057 140215084762944 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1209 00:09:30.392779 140215084762944 trainer.py:155] No checkpoint restored from model_dir=./wmt14_en_de/benchmark_lg3-1208
I1209 00:09:30.393023 140215084762944 registry.py:39] Creating lr_schedule: <class 'neurst.optimizers.schedules.noam_schedule.NoamSchedule'>
I1209 00:09:30.393083 140215084762944 registry.py:41]   (lr_schedule) arguments: 
I1209 00:09:30.393134 140215084762944 registry.py:51]     dmodel: 256
I1209 00:09:30.393179 140215084762944 registry.py:51]     warmup_steps: 3000
I1209 00:09:30.393225 140215084762944 registry.py:51]     initial_factor: 1.0
I1209 00:09:30.393269 140215084762944 registry.py:51]     end_factor: None
I1209 00:09:30.393324 140215084762944 registry.py:51]     start_decay_at: 0
I1209 00:09:30.393367 140215084762944 registry.py:51]     decay_steps: None
I1209 00:09:30.393887 140215084762944 noam_schedule.py:39] Initialize NoamSchedule from global step=0. The result learning rate will be scaled by 1.0
I1209 00:09:30.394111 140215084762944 revised_dynamic_loss_scale.py:50] Using RevisedDynamaicLossScale under FP16 to ensure tf.reduce_all behaviour, especially under XLA
I1209 00:09:30.501827 140215084762944 model_utils.py:80] variable name  | # parameters
I1209 00:09:30.503436 140215084762944 model_utils.py:132]  SequenceToSequence (--/30.60m params)
I1209 00:09:30.503519 140215084762944 model_utils.py:132]    SequenceToSequence/input_symbol_modality_posenc_wrapper (--/10.83m params)
I1209 00:09:30.503575 140215084762944 model_utils.py:132]      SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality (--/10.83m params)
I1209 00:09:30.503627 140215084762944 model_utils.py:132]        SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality/emb (--/10.83m params)
I1209 00:09:30.503680 140215084762944 model_utils.py:137]          SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality/emb/weights (42293x256, 10.83m params)
I1209 00:09:30.503731 140215084762944 model_utils.py:132]    SequenceToSequence/target_symbol_modality_posenc_wrapper (--/11.21m params)
I1209 00:09:30.503778 140215084762944 model_utils.py:132]      SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality (--/11.21m params)
I1209 00:09:30.503825 140215084762944 model_utils.py:132]        SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared (--/11.21m params)
I1209 00:09:30.503874 140215084762944 model_utils.py:137]          SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared/weights (43629x256, 11.17m params)
I1209 00:09:30.503922 140215084762944 model_utils.py:137]          SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared/bias (43629, 43.63k params)
I1209 00:09:30.503970 140215084762944 model_utils.py:132]    SequenceToSequence/LightConvolutionEncoder (--/3.62m params)
I1209 00:09:30.504016 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_0 (--/723.98k params)
I1209 00:09:30.504068 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper (--/197.90k params)
I1209 00:09:30.504114 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv (--/197.39k params)
I1209 00:09:30.504162 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x3, 12 params)
I1209 00:09:30.504209 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.504257 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.504309 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.504356 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.504404 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.504451 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.504497 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.504547 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.504594 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.504640 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.504686 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.504732 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.504780 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.504828 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.504874 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.504922 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.504970 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.505016 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.505063 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.505109 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_0/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.505159 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_1 (--/724.00k params)
I1209 00:09:30.505206 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper (--/197.92k params)
I1209 00:09:30.505251 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv (--/197.40k params)
I1209 00:09:30.505301 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x7, 28 params)
I1209 00:09:30.505348 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.505395 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.505442 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.505489 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.505536 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.505582 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.505628 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.505674 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.505720 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.505766 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.505811 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.505856 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.505904 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.505952 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.505998 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.506046 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.506093 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.506138 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.506185 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.506234 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_1/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.506281 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_2 (--/724.03k params)
I1209 00:09:30.506331 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper (--/197.95k params)
I1209 00:09:30.506377 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv (--/197.44k params)
I1209 00:09:30.506423 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x15, 60 params)
I1209 00:09:30.506469 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.506516 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.506562 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.506608 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.506658 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.506726 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.506792 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.506858 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.506930 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.507002 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.507068 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.507134 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.507204 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.507277 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.507356 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.507426 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.507494 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.507564 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.507636 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.507709 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_2/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.507776 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_3 (--/724.03k params)
I1209 00:09:30.507844 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper (--/197.95k params)
I1209 00:09:30.507915 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv (--/197.44k params)
I1209 00:09:30.507987 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x15, 60 params)
I1209 00:09:30.508054 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.508122 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.508194 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.508266 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.508344 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.508412 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.508478 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.508550 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.508621 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.508688 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.508754 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.508821 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.508895 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.508966 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.509032 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.509100 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.509172 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.509242 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.509322 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.509390 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_3/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.509460 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/layer_4 (--/724.03k params)
I1209 00:09:30.509531 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper (--/197.95k params)
I1209 00:09:30.509600 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv (--/197.44k params)
I1209 00:09:30.509668 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x15, 60 params)
I1209 00:09:30.509735 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.509807 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.509879 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.509947 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.510015 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.510082 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.510152 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.510223 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.510295 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.510363 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.510432 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.510502 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.510574 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.510643 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.510709 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.510783 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.510855 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.510926 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.510994 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.511063 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionEncoder/layer_4/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.511134 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionEncoder/output_ln (--/512 params)
I1209 00:09:30.511203 140215084762944 model_utils.py:137]        SequenceToSequence/LightConvolutionEncoder/output_ln/gamma (256, 256 params)
I1209 00:09:30.511270 140215084762944 model_utils.py:137]        SequenceToSequence/LightConvolutionEncoder/output_ln/beta (256, 256 params)
I1209 00:09:30.511344 140215084762944 model_utils.py:132]    SequenceToSequence/LightConvolutionDecoder (--/4.94m params)
I1209 00:09:30.511414 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_0 (--/987.66k params)
I1209 00:09:30.511484 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper (--/197.90k params)
I1209 00:09:30.511550 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv (--/197.39k params)
I1209 00:09:30.511616 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x3, 12 params)
I1209 00:09:30.511684 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.511758 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.511829 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.511895 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.511963 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.512034 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.512105 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.512175 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.512241 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.512314 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 00:09:30.512386 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 00:09:30.512456 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 00:09:30.512528 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.512600 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 00:09:30.512669 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 00:09:30.512742 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.512814 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 00:09:30.512880 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 00:09:30.512948 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 00:09:30.513019 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 00:09:30.513090 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.513159 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.513226 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.513299 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.513371 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.513442 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.513511 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.513580 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.513647 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.513722 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.513793 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.513859 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.513926 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.513995 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_0/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.514067 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_1 (--/987.68k params)
I1209 00:09:30.514136 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper (--/197.92k params)
I1209 00:09:30.514206 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv (--/197.40k params)
I1209 00:09:30.514275 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x7, 28 params)
I1209 00:09:30.514352 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.514427 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.514495 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.514561 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.514631 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.514703 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.514772 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.514839 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.514906 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.514976 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 00:09:30.515046 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 00:09:30.515115 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 00:09:30.515183 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.515251 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 00:09:30.515327 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 00:09:30.515403 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.515471 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 00:09:30.515537 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 00:09:30.515607 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 00:09:30.515683 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 00:09:30.515753 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.515820 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.515886 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.515955 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.516025 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.516094 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.516162 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.516231 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.516309 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.516385 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.516452 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.516518 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.516586 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.516656 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_1/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.516728 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_2 (--/987.71k params)
I1209 00:09:30.516793 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper (--/197.95k params)
I1209 00:09:30.516858 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv (--/197.44k params)
I1209 00:09:30.516928 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x15, 60 params)
I1209 00:09:30.516998 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.517069 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.517135 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.517201 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.517280 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.517360 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.517426 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.517493 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.517560 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.517632 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 00:09:30.517701 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 00:09:30.517767 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 00:09:30.517837 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.517908 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 00:09:30.517979 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 00:09:30.518050 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.518117 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 00:09:30.518184 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 00:09:30.518259 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 00:09:30.518336 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 00:09:30.518403 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.518471 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.518539 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.518611 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.518679 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.518744 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.518817 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.518891 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.518962 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.519031 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.519098 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.519165 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.519237 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.519312 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_2/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.519381 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_3 (--/987.71k params)
I1209 00:09:30.519447 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper (--/197.95k params)
I1209 00:09:30.519516 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv (--/197.44k params)
I1209 00:09:30.519588 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x15, 60 params)
I1209 00:09:30.519654 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.519723 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.519793 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.519865 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.519934 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.520002 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.520069 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.520141 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.520211 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.520278 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 00:09:30.520350 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 00:09:30.520426 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 00:09:30.520500 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.520568 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 00:09:30.520635 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 00:09:30.520705 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.520778 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 00:09:30.520846 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 00:09:30.520915 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 00:09:30.520982 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 00:09:30.521053 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.521123 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.521190 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.521257 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.521334 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.521405 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.521474 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.521543 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.521611 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.521684 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.521755 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.521821 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.521888 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.521964 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_3/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.522036 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/layer_4 (--/987.71k params)
I1209 00:09:30.522101 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper (--/197.95k params)
I1209 00:09:30.522165 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv (--/197.44k params)
I1209 00:09:30.522236 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/conv_shared_weight (4x15, 60 params)
I1209 00:09:30.522321 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1 (--/131.58k params)
I1209 00:09:30.522392 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/kernel (256x512, 131.07k params)
I1209 00:09:30.522459 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense1/bias (512, 512 params)
I1209 00:09:30.522526 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2 (--/65.79k params)
I1209 00:09:30.522600 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/kernel (256x256, 65.54k params)
I1209 00:09:30.522671 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/light_conv/dense2/bias (256, 256 params)
I1209 00:09:30.522736 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.522804 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.522873 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/light_conv_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.522947 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper (--/263.68k params)
I1209 00:09:30.523015 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1209 00:09:30.523082 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1209 00:09:30.523151 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.523224 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1209 00:09:30.523300 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1209 00:09:30.523370 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1209 00:09:30.523438 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1209 00:09:30.523518 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1209 00:09:30.523592 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1209 00:09:30.523660 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1209 00:09:30.523727 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.523798 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.523870 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.523937 140215084762944 model_utils.py:132]        SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper (--/526.08k params)
I1209 00:09:30.524003 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn (--/525.57k params)
I1209 00:09:30.524072 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1209 00:09:30.524147 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1209 00:09:30.524216 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1209 00:09:30.524283 140215084762944 model_utils.py:132]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1209 00:09:30.524363 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1209 00:09:30.524435 140215084762944 model_utils.py:137]              SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1209 00:09:30.524503 140215084762944 model_utils.py:132]          SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ln (--/512 params)
I1209 00:09:30.524570 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1209 00:09:30.524637 140215084762944 model_utils.py:137]            SequenceToSequence/LightConvolutionDecoder/layer_4/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1209 00:09:30.524709 140215084762944 model_utils.py:132]      SequenceToSequence/LightConvolutionDecoder/output_ln (--/512 params)
I1209 00:09:30.524779 140215084762944 model_utils.py:137]        SequenceToSequence/LightConvolutionDecoder/output_ln/gamma (256, 256 params)
I1209 00:09:30.524845 140215084762944 model_utils.py:137]        SequenceToSequence/LightConvolutionDecoder/output_ln/beta (256, 256 params)
I1209 00:09:30.731124 140215084762944 checkpoints.py:166] Creates checkpoint manager for directory: ./wmt14_en_de/benchmark_lg3-1208
I1209 00:09:30.741312 140215084762944 registry.py:39] Creating criterion: <class 'neurst.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropy'>
I1209 00:09:30.741475 140215084762944 registry.py:41]   (criterion) arguments: 
I1209 00:09:30.741532 140215084762944 registry.py:51]     label_smoothing: 0.0
I1209 00:09:30.741643 140215084762944 registry.py:39] Creating dataset: <class 'neurst.data.datasets.parallel_text_dataset.ParallelTextDataset'>
I1209 00:09:30.741704 140215084762944 registry.py:41]   (dataset) arguments: 
I1209 00:09:30.741751 140215084762944 registry.py:51]     src_file: /root/neurst/wmt14_en_de/newstest2013.en.txt
I1209 00:09:30.741794 140215084762944 registry.py:51]     trg_file: /root/neurst/wmt14_en_de/newstest2013.de.txt
I1209 00:09:30.741842 140215084762944 registry.py:51]     raw_trg_file: None
I1209 00:09:30.741895 140215084762944 registry.py:51]     data_is_processed: None
I1209 00:09:30.741938 140215084762944 registry.py:51]     src_lang: None
I1209 00:09:30.741981 140215084762944 registry.py:51]     trg_lang: None
I1209 00:09:32.765377 140215084762944 seq2seq.py:231] Creating evaluation dataset.
I1209 00:09:32.765657 140215084762944 dataset_utils.py:450] The global batch size is 64 samples.
I1209 00:09:32.772929 140215084762944 registry.py:39] Creating search_method: <class 'neurst.layers.search.beam_search.BeamSearch'>
I1209 00:09:32.773057 140215084762944 registry.py:41]   (search_method) arguments: 
I1209 00:09:32.773111 140215084762944 registry.py:51]     beam_size: 4
I1209 00:09:32.773164 140215084762944 registry.py:51]     length_penalty: 0.6
I1209 00:09:32.773209 140215084762944 registry.py:51]     top_k: 1
I1209 00:09:32.773252 140215084762944 registry.py:51]     maximum_decode_length: 160
I1209 00:09:32.773301 140215084762944 registry.py:51]     minimum_decode_length: 0
I1209 00:09:32.773343 140215084762944 registry.py:51]     extra_decode_length: 50
I1209 00:09:32.773387 140215084762944 registry.py:51]     padded_decode: None
I1209 00:09:32.773429 140215084762944 registry.py:51]     ensemble_weights: average
I1209 00:09:32.773471 140215084762944 registry.py:51]     enable_unk: None
I1209 00:09:37.129432 140215084762944 seq2seq.py:223] Creating test dataset.
I1209 00:09:37.129688 140215084762944 dataset_utils.py:450] The global batch size is 64 samples.
I1209 00:09:37.371222 140215084762944 checkpoints.py:209] Creates custom keep-best checkpoint manager for directory: ./wmt14_en_de/benchmark_lg3-1208/best
I1209 00:09:37.372214 140215084762944 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_lg3-1208/best
I1209 00:09:37.720453 140215084762944 checkpoints.py:271] Create checkpoint manager for averaged checkpoint of the latest 10 checkpoints to dir: ./wmt14_en_de/benchmark_lg3-1208/best_avg
I1209 00:09:37.721647 140215084762944 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_lg3-1208/best_avg
I1209 00:09:37.742208 140215084762944 trainer.py:306] Training for 100000 steps.
I1209 00:09:38.387460 140215084762944 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_lg3-1208
W1209 00:09:38.728005 140215084762944 deprecation.py:323] From /root/.local/conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
I1209 00:20:04.928045 140215084762944 callbacks.py:220] Update 1000	TrainingLoss=6.78	Speed 0.626 secs/step 1.6 steps/sec
I1209 00:20:04.930794 140215084762944 callbacks.py:238] {'step': 1000, 'lr': 0.0003803629, 'loss': 6.776112079620361, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 3245.6073600619784, 'src_real_tokens_per_step': 1652.174, 'src_real_tokens_per_sec': 2637.73689791189, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 3245.6073600619784, 'trg_real_tokens_per_step': 1689.269, 'trg_real_tokens_per_sec': 2696.959988353963, 'samples_per_step': 55.584, 'samples_per_sec': 88.7412389576004, 'this_step_loss': 192731.015625}
I1209 00:22:31.094583 140215084762944 callbacks.py:220] Update 2000	TrainingLoss=6.09	Speed 0.146 secs/step 6.8 steps/sec
I1209 00:22:31.096400 140215084762944 callbacks.py:238] {'step': 2000, 'lr': 0.0007607258, 'loss': 6.093723773956299, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 13925.531944154307, 'src_real_tokens_per_step': 1653.658, 'src_real_tokens_per_sec': 11326.601057552778, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 13925.531944154307, 'trg_real_tokens_per_step': 1688.407, 'trg_real_tokens_per_sec': 11564.611613634448, 'samples_per_step': 55.958, 'samples_per_sec': 383.27994178877276, 'this_step_loss': 166730.421875}
I1209 00:22:31.701296 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-2000	Elapsed 0.60s
I1209 00:25:01.722187 140215084762944 callbacks.py:220] Update 3000	TrainingLoss=5.65	Speed 0.150 secs/step 6.7 steps/sec
I1209 00:25:01.726236 140215084762944 callbacks.py:238] {'step': 3000, 'lr': 0.0011410887, 'loss': 5.650871753692627, 'src_tokens_per_step': 2033.16, 'src_tokens_per_sec': 13559.143653731422, 'src_real_tokens_per_step': 1651.425, 'src_real_tokens_per_sec': 11013.353011255098, 'trg_tokens_per_step': 2033.16, 'trg_tokens_per_sec': 13559.143653731422, 'trg_real_tokens_per_step': 1689.757, 'trg_real_tokens_per_sec': 11268.989111972618, 'samples_per_step': 55.79, 'samples_per_sec': 372.0634994007732, 'this_step_loss': 307707.09375}
I1209 00:27:35.094724 140215084762944 callbacks.py:220] Update 4000	TrainingLoss=5.31	Speed 0.153 secs/step 6.5 steps/sec
I1209 00:27:35.099648 140215084762944 callbacks.py:238] {'step': 4000, 'lr': 0.0009882118, 'loss': 5.305970191955566, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 13261.691336522172, 'src_real_tokens_per_step': 1650.945, 'src_real_tokens_per_sec': 10769.804363528707, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 13261.691336522172, 'trg_real_tokens_per_step': 1688.576, 'trg_real_tokens_per_sec': 11015.287107050719, 'samples_per_step': 56.014, 'samples_per_sec': 365.40273698923767, 'this_step_loss': 268617.21875}
I1209 00:27:35.814226 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-4000	Elapsed 0.70s
I1209 00:30:10.552839 140215084762944 callbacks.py:220] Update 5000	TrainingLoss=5.03	Speed 0.155 secs/step 6.5 steps/sec
I1209 00:30:10.559176 140215084762944 callbacks.py:238] {'step': 5000, 'lr': 0.0008838835, 'loss': 5.0274434089660645, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 13144.402279141203, 'src_real_tokens_per_step': 1650.244, 'src_real_tokens_per_sec': 10669.685683590307, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 13144.402279141203, 'trg_real_tokens_per_step': 1689.368, 'trg_real_tokens_per_sec': 10922.642690363116, 'samples_per_step': 55.759, 'samples_per_sec': 360.51093294768043, 'this_step_loss': 523989.0625}
I1209 00:32:46.567046 140215084762944 callbacks.py:220] Update 6000	TrainingLoss=4.80	Speed 0.156 secs/step 6.4 steps/sec
I1209 00:32:46.582148 140215084762944 callbacks.py:238] {'step': 6000, 'lr': 0.00080687157, 'loss': 4.804910182952881, 'src_tokens_per_step': 2032.928, 'src_tokens_per_sec': 13037.560355792919, 'src_real_tokens_per_step': 1651.805, 'src_real_tokens_per_sec': 10593.344861943227, 'trg_tokens_per_step': 2032.928, 'trg_tokens_per_sec': 13037.560355792919, 'trg_real_tokens_per_step': 1688.948, 'trg_real_tokens_per_sec': 10831.550103002044, 'samples_per_step': 55.921, 'samples_per_sec': 358.6321860175549, 'this_step_loss': 528579.625}
I1209 00:32:47.276917 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-6000	Elapsed 0.69s
I1209 00:35:20.105792 140215084762944 callbacks.py:220] Update 7000	TrainingLoss=4.63	Speed 0.153 secs/step 6.5 steps/sec
I1209 00:35:20.118585 140215084762944 callbacks.py:238] {'step': 7000, 'lr': 0.0007470179, 'loss': 4.627692699432373, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 13309.363311073188, 'src_real_tokens_per_step': 1648.645, 'src_real_tokens_per_sec': 10792.696438892839, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 13309.363311073188, 'trg_real_tokens_per_step': 1690.892, 'trg_real_tokens_per_sec': 11069.262374223918, 'samples_per_step': 55.928, 'samples_per_sec': 366.12729025011373, 'this_step_loss': 963085.875}
I1209 00:37:55.290646 140215084762944 callbacks.py:220] Update 8000	TrainingLoss=4.48	Speed 0.155 secs/step 6.4 steps/sec
I1209 00:37:55.294420 140215084762944 callbacks.py:238] {'step': 8000, 'lr': 0.00069877127, 'loss': 4.481411933898926, 'src_tokens_per_step': 2032.984, 'src_tokens_per_sec': 13108.32591121485, 'src_real_tokens_per_step': 1651.918, 'src_real_tokens_per_sec': 10651.278870174192, 'trg_tokens_per_step': 2032.984, 'trg_tokens_per_sec': 13108.32591121485, 'trg_real_tokens_per_step': 1689.622, 'trg_real_tokens_per_sec': 10894.38767976465, 'samples_per_step': 55.829, 'samples_per_sec': 359.9756453062168, 'this_step_loss': 934744.1875}
I1209 00:37:56.043938 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-8000	Elapsed 0.74s
I1209 00:40:29.769234 140215084762944 callbacks.py:220] Update 9000	TrainingLoss=4.36	Speed 0.154 secs/step 6.5 steps/sec
I1209 00:40:29.787469 140215084762944 callbacks.py:238] {'step': 9000, 'lr': 0.0006588078, 'loss': 4.358987808227539, 'src_tokens_per_step': 2032.896, 'src_tokens_per_sec': 13230.816204362172, 'src_real_tokens_per_step': 1650.892, 'src_real_tokens_per_sec': 10744.597178238275, 'trg_tokens_per_step': 2032.896, 'trg_tokens_per_sec': 13230.816204362172, 'trg_real_tokens_per_step': 1686.96, 'trg_real_tokens_per_sec': 10979.340656930217, 'samples_per_step': 55.944, 'samples_per_sec': 364.10361461522746, 'this_step_loss': 1014050.3125}
I1209 00:43:04.728881 140215084762944 callbacks.py:220] Update 10000	TrainingLoss=4.26	Speed 0.155 secs/step 6.5 steps/sec
I1209 00:43:04.734578 140215084762944 callbacks.py:238] {'step': 10000, 'lr': 0.000625, 'loss': 4.256591320037842, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 13128.136221864312, 'src_real_tokens_per_step': 1650.865, 'src_real_tokens_per_sec': 10660.408282034194, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 13128.136221864312, 'trg_real_tokens_per_step': 1687.753, 'trg_real_tokens_per_sec': 10898.61136993519, 'samples_per_step': 55.714, 'samples_per_sec': 359.77138471362167, 'this_step_loss': 959372.5625}
I1209 00:43:05.574365 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-10000	Elapsed 0.83s
I1209 00:43:36.021492 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=94.79 (Best 94.79)  step=10000	Elapsed 30.45s  FromSTART 2043.25s
I1209 00:43:36.024179 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=11.28 (Best 11.28)  step=10000	Elapsed 30.45s  FromSTART 2043.25s
I1209 00:48:06.019697 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 00:48:06.025164 140215084762944 seq_generation_validator.py:179] Sample 125
I1209 00:48:06.025311 140215084762944 seq_generation_validator.py:181]   Data: Which is why a huge particle accelerator like the one at CERN - the Large Hadron Collider is a ring with a 27km circumference! - is needed to achieve such energy levels.
I1209 00:48:06.025377 140215084762944 seq_generation_validator.py:182]   Reference: Deshalb ist ein riesiger Teilchenbeschleuniger wie das CERN erforderlich - der Large Hadron Collider ist ein Ring mit einem Umfang von 27 km! - um derartige Energien zu erreichen.
I1209 00:48:06.025428 140215084762944 seq_generation_validator.py:183]   Hypothesis: Aus diesem Grund ist ein riesiges Angebot wie das, was man im Jahre 1974 N - der groe Hadron Collider ist eine Verbindung mit einem 27km-Schlamm. - ist es erforderlich, solche Energieniveau zu erreichen.
I1209 00:48:06.025481 140215084762944 seq_generation_validator.py:179] Sample 1801
I1209 00:48:06.025528 140215084762944 seq_generation_validator.py:181]   Data: This image section from an infrared recording by the Spitzer telescope shows a "family portrait" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured "new-borns" in the star delivery room.
I1209 00:48:06.025575 140215084762944 seq_generation_validator.py:182]   Reference: Dieser Bildausschnitt einer Infrarot-Aufnahme des Spitzer-Teleskops zeigt ein "Familienportrt" ungezhlter Sternengenerationen: Die ltesten Sterne sind als blaue Punkte zu erkennen, schwerer zu identifizieren sind die pinkfarbenen "Neugeborenen" im Sternenkreissaal.
I1209 00:48:06.025627 140215084762944 seq_generation_validator.py:183]   Hypothesis: Dieses Bild von einem Infrarot von der KresesesTelesesesesesesesesesesesesesesesesesesooodots.
I1209 00:48:06.025674 140215084762944 seq_generation_validator.py:179] Sample 1805
I1209 00:48:06.025721 140215084762944 seq_generation_validator.py:181]   Data: In the huge Trifid Nebula, 5,400 light years away from the Earth, new stars are created from gas and dust.
I1209 00:48:06.025766 140215084762944 seq_generation_validator.py:182]   Reference: Im riesigen Trifid-Nebel, 5400 Lichtjahre von der Erde entfernt, entstehen aus Gas und Staub neue Sterne.
I1209 00:48:06.025811 140215084762944 seq_generation_validator.py:183]   Hypothesis: In der riesigen Trifid Dekard, 5.400 Licht Jahre von der Erde entfernt, werden neue Sterne aus Gas und Staub geschaffen.
I1209 00:48:06.025856 140215084762944 seq_generation_validator.py:179] Sample 173
I1209 00:48:06.025902 140215084762944 seq_generation_validator.py:181]   Data: This is palliative care, given when there is nothing else that can be done.
I1209 00:48:06.025947 140215084762944 seq_generation_validator.py:182]   Reference: Die Palliativpflege wird dann eingesetzt, wenn es keine anderen Mglichkeiten mehr gibt.
I1209 00:48:06.025992 140215084762944 seq_generation_validator.py:183]   Hypothesis: Dies ist Palliative Sorgfalt, wenn es nichts anderes gibt.
I1209 00:48:06.026036 140215084762944 seq_generation_validator.py:179] Sample 1218
I1209 00:48:06.026082 140215084762944 seq_generation_validator.py:181]   Data: The first reference to the company figures in the November issue of the magazine Problems in 1954, in which four in-depth articles are on the subject of astronautics.
I1209 00:48:06.026126 140215084762944 seq_generation_validator.py:182]   Reference: Die erste Erwhnung dieser Gesellschaft erfolgt 1954 in der Novemberausgabe der Zeitschrift Problemy, in der vier umfangreiche Artikel der Raumfahrt gewidmet sind.
I1209 00:48:06.026171 140215084762944 seq_generation_validator.py:183]   Hypothesis: Der erste Hinweis auf die Unternehmenszahlen im November-Problem der Ausstellung-Probleme im November, in denen vier im Bereich der Apropautiktikalitt vier unterschiedliche Artikeln sind.
I1209 00:48:06.995584 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.97s
I1209 00:48:07.885587 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 0.89s
I1209 00:48:07.885804 140215084762944 training_utils.py:359] Evaluating bleu at step=10000 with bad count=0 (early_stop_patience=0).
I1209 00:48:07.885881 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=13.71 (Best 13.71)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:48:07.887237 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=13.77 (Best 13.77)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:48:07.887858 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=13.71 (Best 13.71)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:48:07.888360 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=41.71 (Best 41.71)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:48:07.888843 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=14.10 (Best 14.10)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:48:07.889335 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=14.17 (Best 14.17)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:48:07.889813 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=14.10 (Best 14.10)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:48:07.890302 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=43.08 (Best 43.08)  step=10000	Elapsed 257.61s  FromSTART 2295.89s
I1209 00:50:38.512795 140215084762944 callbacks.py:220] Update 11000	TrainingLoss=4.17	Speed 0.151 secs/step 6.6 steps/sec
I1209 00:50:38.517244 140215084762944 callbacks.py:238] {'step': 11000, 'lr': 0.00059591414, 'loss': 4.1672444343566895, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 13503.747247610228, 'src_real_tokens_per_step': 1650.973, 'src_real_tokens_per_sec': 10966.261600945209, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 13503.747247610228, 'trg_real_tokens_per_step': 1690.334, 'trg_real_tokens_per_sec': 11227.709258099387, 'samples_per_step': 55.928, 'samples_per_sec': 371.4906778110021, 'this_step_loss': 401403.0}
I1209 00:53:12.039721 140215084762944 callbacks.py:220] Update 12000	TrainingLoss=4.09	Speed 0.153 secs/step 6.5 steps/sec
I1209 00:53:12.044971 140215084762944 callbacks.py:238] {'step': 12000, 'lr': 0.00057054433, 'loss': 4.090338706970215, 'src_tokens_per_step': 2033.024, 'src_tokens_per_sec': 13249.880913654788, 'src_real_tokens_per_step': 1647.188, 'src_real_tokens_per_sec': 10735.261778710534, 'trg_tokens_per_step': 2033.024, 'trg_tokens_per_sec': 13249.880913654788, 'trg_real_tokens_per_step': 1688.254, 'trg_real_tokens_per_sec': 11002.90230317072, 'samples_per_step': 55.985, 'samples_per_sec': 364.87251648330925, 'this_step_loss': 388102.21875}
I1209 00:53:12.725263 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-12000	Elapsed 0.67s
I1209 00:55:47.920696 140215084762944 callbacks.py:220] Update 13000	TrainingLoss=4.02	Speed 0.155 secs/step 6.4 steps/sec
I1209 00:55:47.958575 140215084762944 callbacks.py:238] {'step': 13000, 'lr': 0.0005481613, 'loss': 4.021828651428223, 'src_tokens_per_step': 2033.032, 'src_tokens_per_sec': 13106.034303319919, 'src_real_tokens_per_step': 1652.124, 'src_real_tokens_per_sec': 10650.49336032985, 'trg_tokens_per_step': 2033.032, 'trg_tokens_per_sec': 13106.034303319919, 'trg_real_tokens_per_step': 1689.804, 'trg_real_tokens_per_sec': 10893.399213532897, 'samples_per_step': 55.956, 'samples_per_sec': 360.7229278617205, 'this_step_loss': 492045.09375}
I1209 00:58:22.237872 140215084762944 callbacks.py:220] Update 14000	TrainingLoss=3.96	Speed 0.154 secs/step 6.5 steps/sec
I1209 00:58:22.241210 140215084762944 callbacks.py:238] {'step': 14000, 'lr': 0.0005282214, 'loss': 3.9609596729278564, 'src_tokens_per_step': 2033.152, 'src_tokens_per_sec': 13185.54397193604, 'src_real_tokens_per_step': 1654.684, 'src_real_tokens_per_sec': 10731.076004971106, 'trg_tokens_per_step': 2033.152, 'trg_tokens_per_sec': 13185.54397193604, 'trg_real_tokens_per_step': 1685.324, 'trg_real_tokens_per_sec': 10929.784742586455, 'samples_per_step': 55.951, 'samples_per_sec': 362.857460127818, 'this_step_loss': 354630.375}
I1209 00:58:23.235499 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-14000	Elapsed 0.99s
I1209 01:00:56.898044 140215084762944 callbacks.py:220] Update 15000	TrainingLoss=3.91	Speed 0.154 secs/step 6.5 steps/sec
I1209 01:00:56.902171 140215084762944 callbacks.py:238] {'step': 15000, 'lr': 0.00051031035, 'loss': 3.906301259994507, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 13236.5955385844, 'src_real_tokens_per_step': 1651.502, 'src_real_tokens_per_sec': 10753.050762622737, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 13236.5955385844, 'trg_real_tokens_per_step': 1689.692, 'trg_real_tokens_per_sec': 11001.708656239918, 'samples_per_step': 55.966, 'samples_per_sec': 364.39873459489854, 'this_step_loss': 805571.3125}
I1209 01:03:30.991133 140215084762944 callbacks.py:220] Update 16000	TrainingLoss=3.86	Speed 0.154 secs/step 6.5 steps/sec
I1209 01:03:30.994071 140215084762944 callbacks.py:238] {'step': 16000, 'lr': 0.0004941059, 'loss': 3.8573975563049316, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 13201.95444799136, 'src_real_tokens_per_step': 1651.122, 'src_real_tokens_per_sec': 10721.682094200125, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 13201.95444799136, 'trg_real_tokens_per_step': 1689.034, 'trg_real_tokens_per_sec': 10967.866453414837, 'samples_per_step': 55.867, 'samples_per_sec': 362.77647173054345, 'this_step_loss': 422821.25}
I1209 01:03:31.693561 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-16000	Elapsed 0.69s
I1209 01:06:03.282002 140215084762944 callbacks.py:220] Update 17000	TrainingLoss=3.81	Speed 0.152 secs/step 6.6 steps/sec
I1209 01:06:03.298626 140215084762944 callbacks.py:238] {'step': 17000, 'lr': 0.00047935313, 'loss': 3.8128268718719482, 'src_tokens_per_step': 2033.04, 'src_tokens_per_sec': 13418.353039296322, 'src_real_tokens_per_step': 1653.008, 'src_real_tokens_per_sec': 10910.087809773117, 'trg_tokens_per_step': 2033.04, 'trg_tokens_per_sec': 13418.353039296322, 'trg_real_tokens_per_step': 1687.93, 'trg_real_tokens_per_sec': 11140.577974668202, 'samples_per_step': 55.956, 'samples_per_sec': 369.3175553195535, 'this_step_loss': 363949.40625}
I1209 01:08:39.160520 140215084762944 callbacks.py:220] Update 18000	TrainingLoss=3.77	Speed 0.156 secs/step 6.4 steps/sec
I1209 01:08:39.167769 140215084762944 callbacks.py:238] {'step': 18000, 'lr': 0.0004658475, 'loss': 3.77132248878479, 'src_tokens_per_step': 2032.856, 'src_tokens_per_sec': 13049.650247753872, 'src_real_tokens_per_step': 1651.538, 'src_real_tokens_per_sec': 10601.829775879321, 'trg_tokens_per_step': 2032.856, 'trg_tokens_per_sec': 13049.650247753872, 'trg_real_tokens_per_step': 1689.164, 'trg_real_tokens_per_sec': 10843.364906858587, 'samples_per_step': 55.868, 'samples_per_sec': 358.6372374833797, 'this_step_loss': 837375.4375}
I1209 01:08:40.225414 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-18000	Elapsed 1.05s
I1209 01:11:12.854249 140215084762944 callbacks.py:220] Update 19000	TrainingLoss=3.73	Speed 0.153 secs/step 6.6 steps/sec
I1209 01:11:12.958686 140215084762944 callbacks.py:238] {'step': 19000, 'lr': 0.00045342266, 'loss': 3.734067678451538, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 13326.762988403128, 'src_real_tokens_per_step': 1651.486, 'src_real_tokens_per_sec': 10825.85464863056, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 13326.762988403128, 'trg_real_tokens_per_step': 1688.274, 'trg_real_tokens_per_sec': 11067.008095171324, 'samples_per_step': 55.975, 'samples_per_sec': 366.9284595552706, 'this_step_loss': 408546.28125}
I1209 01:13:45.972990 140215084762944 callbacks.py:220] Update 20000	TrainingLoss=3.70	Speed 0.153 secs/step 6.5 steps/sec
I1209 01:13:45.976542 140215084762944 callbacks.py:238] {'step': 20000, 'lr': 0.00044194175, 'loss': 3.7000892162323, 'src_tokens_per_step': 2033.048, 'src_tokens_per_sec': 13293.956947134371, 'src_real_tokens_per_step': 1652.616, 'src_real_tokens_per_sec': 10806.339030925692, 'trg_tokens_per_step': 2033.048, 'trg_tokens_per_sec': 13293.956947134371, 'trg_real_tokens_per_step': 1688.95, 'trg_real_tokens_per_sec': 11043.924484745365, 'samples_per_step': 55.698, 'samples_per_sec': 364.2052789907027, 'this_step_loss': 200226.453125}
I1209 01:13:46.838701 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-20000	Elapsed 0.86s
I1209 01:15:57.628184 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=85.03 (Best 85.03)  step=20000	Elapsed 130.79s  FromSTART 3984.86s
I1209 01:15:57.635827 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=8.78 (Best 8.78)  step=20000	Elapsed 130.79s  FromSTART 3984.86s
I1209 01:17:49.685206 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 01:17:49.687972 140215084762944 seq_generation_validator.py:179] Sample 188
I1209 01:17:49.688088 140215084762944 seq_generation_validator.py:181]   Data: 53% of patients admitted to the Victor-Gadbois home come from their homes, 47% from hospital.
I1209 01:17:49.688152 140215084762944 seq_generation_validator.py:182]   Reference: 53% der ins Heim Victor-Gadbois eingewiesenen Patienten kommen von ihrer Wohnsttte, 47% kommen aus dem Krankenhaus.
I1209 01:17:49.688205 140215084762944 seq_generation_validator.py:183]   Hypothesis: 53% der Patienten, die in der Heimat von Christopher-Gadbois kommen, kommen aus ihrer Heimat, 47% aus Krankenhaus.
I1209 01:17:49.688252 140215084762944 seq_generation_validator.py:179] Sample 691
I1209 01:17:49.688306 140215084762944 seq_generation_validator.py:181]   Data: The formulas are still the same, according to him.
I1209 01:17:49.688353 140215084762944 seq_generation_validator.py:182]   Reference: Die Schemas haben sich ihm zufolge nicht verndert.
I1209 01:17:49.688399 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die Formulierung sind immer noch dasselbe, gem ihm.
I1209 01:17:49.688444 140215084762944 seq_generation_validator.py:179] Sample 539
I1209 01:17:49.688490 140215084762944 seq_generation_validator.py:181]   Data: Stars are glimmering,
I1209 01:17:49.688536 140215084762944 seq_generation_validator.py:182]   Reference: Sterne funkeln.
I1209 01:17:49.688580 140215084762944 seq_generation_validator.py:183]   Hypothesis: Stars sind schwinken.
I1209 01:17:49.688624 140215084762944 seq_generation_validator.py:179] Sample 1515
I1209 01:17:49.688670 140215084762944 seq_generation_validator.py:181]   Data: It is part of a revitalisation project, which also includes an underground car park, a supermarket, a post office and a small number of adjacent apartment buildings and terraced houses, with a total of 50 dwellings.
I1209 01:17:49.688717 140215084762944 seq_generation_validator.py:182]   Reference: Er ist Teil eines Revitalisierungsprojekts, zu dem auch Tiefgarage, Supermarkt, Post und ein paar angrenzende Wohn- und Reihenhuser mit insgesamt 50 Wohnungen gehren.
I1209 01:17:49.688762 140215084762944 seq_generation_validator.py:183]   Hypothesis: Es ist Teil eines Revitalisierung-Projekts, das auch einen unterirdischen Parkplatz, ein Superort und eine kleine Zahl der benachbarten Wohnung und der Wohngebude, die insgesamt 50 Einwohner sind.
I1209 01:17:49.688808 140215084762944 seq_generation_validator.py:179] Sample 209
I1209 01:17:49.688854 140215084762944 seq_generation_validator.py:181]   Data: But this does not necessarily mean "I want you to euthanise me," it means "I want to be relieved."
I1209 01:17:49.688899 140215084762944 seq_generation_validator.py:182]   Reference: Aber das bedeutet nicht zwangslufig, "ich will, dass Sie mich einschlfern", sondern "ich mchte, dass meine Schmerzen gelindert werden".
I1209 01:17:49.688944 140215084762944 seq_generation_validator.py:183]   Hypothesis: Aber das bedeutet nicht unbedingt "Ich mchte Ihnen eudankbar mich", es bedeutet, "Ich mchte".
I1209 01:17:50.697942 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.01s
I1209 01:17:51.861407 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.16s
I1209 01:17:51.861662 140215084762944 training_utils.py:359] Evaluating bleu at step=20000 with bad count=0 (early_stop_patience=0).
I1209 01:17:51.861749 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=15.90 (Best 15.90)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:17:51.863287 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=16.05 (Best 16.05)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:17:51.863930 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=15.90 (Best 15.90)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:17:51.864443 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=44.92 (Best 44.92)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:17:51.864937 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=16.36 (Best 16.36)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:17:51.865442 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=16.53 (Best 16.53)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:17:51.865927 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=16.36 (Best 16.36)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:17:51.866449 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=46.26 (Best 46.26)  step=20000	Elapsed 100.66s  FromSTART 4080.55s
I1209 01:20:21.145362 140215084762944 callbacks.py:220] Update 21000	TrainingLoss=3.67	Speed 0.149 secs/step 6.7 steps/sec
I1209 01:20:21.148739 140215084762944 callbacks.py:238] {'step': 21000, 'lr': 0.00043129097, 'loss': 3.6675288677215576, 'src_tokens_per_step': 2032.84, 'src_tokens_per_sec': 13625.267773431202, 'src_real_tokens_per_step': 1652.48, 'src_real_tokens_per_sec': 11075.875371519447, 'trg_tokens_per_step': 2032.84, 'trg_tokens_per_sec': 13625.267773431202, 'trg_real_tokens_per_step': 1688.708, 'trg_real_tokens_per_sec': 11318.696351476485, 'samples_per_step': 55.936, 'samples_per_sec': 374.915378571185, 'this_step_loss': 220131.296875}
I1209 01:22:57.818907 140215084762944 callbacks.py:220] Update 22000	TrainingLoss=3.64	Speed 0.157 secs/step 6.4 steps/sec
I1209 01:22:57.828435 140215084762944 callbacks.py:238] {'step': 22000, 'lr': 0.0004213749, 'loss': 3.6382358074188232, 'src_tokens_per_step': 2033.152, 'src_tokens_per_sec': 12984.913160634098, 'src_real_tokens_per_step': 1651.616, 'src_real_tokens_per_sec': 10548.19823343943, 'trg_tokens_per_step': 2033.152, 'trg_tokens_per_sec': 12984.913160634098, 'trg_real_tokens_per_step': 1690.204, 'trg_real_tokens_per_sec': 10794.644061908011, 'samples_per_step': 56.121, 'samples_per_sec': 358.4219534436905, 'this_step_loss': 411641.90625}
I1209 01:22:59.013612 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-22000	Elapsed 1.18s
I1209 01:25:30.095019 140215084762944 callbacks.py:220] Update 23000	TrainingLoss=3.61	Speed 0.151 secs/step 6.6 steps/sec
I1209 01:25:30.097989 140215084762944 callbacks.py:238] {'step': 23000, 'lr': 0.00041211277, 'loss': 3.6109979152679443, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 13463.250548989232, 'src_real_tokens_per_step': 1650.164, 'src_real_tokens_per_sec': 10928.318146229032, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 13463.250548989232, 'trg_real_tokens_per_step': 1689.18, 'trg_real_tokens_per_sec': 11186.704137435527, 'samples_per_step': 55.675, 'samples_per_sec': 368.7112994776891, 'this_step_loss': 363205.09375}
I1209 01:28:02.765866 140215084762944 callbacks.py:220] Update 24000	TrainingLoss=3.59	Speed 0.153 secs/step 6.6 steps/sec
I1209 01:28:02.771156 140215084762944 callbacks.py:238] {'step': 24000, 'lr': 0.00040343578, 'loss': 3.5852766036987305, 'src_tokens_per_step': 2033.032, 'src_tokens_per_sec': 13324.434551944249, 'src_real_tokens_per_step': 1652.608, 'src_real_tokens_per_sec': 10831.146354813638, 'trg_tokens_per_step': 2033.032, 'trg_tokens_per_sec': 13324.434551944249, 'trg_real_tokens_per_step': 1686.52, 'trg_real_tokens_per_sec': 11053.404649088166, 'samples_per_step': 55.998, 'samples_per_sec': 367.0093171380352, 'this_step_loss': 321762.21875}
I1209 01:28:03.825591 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-24000	Elapsed 1.05s
I1209 01:30:37.394577 140215084762944 callbacks.py:220] Update 25000	TrainingLoss=3.56	Speed 0.153 secs/step 6.5 steps/sec
I1209 01:30:37.398886 140215084762944 callbacks.py:238] {'step': 25000, 'lr': 0.00039528473, 'loss': 3.5614140033721924, 'src_tokens_per_step': 2032.864, 'src_tokens_per_sec': 13244.624225741589, 'src_real_tokens_per_step': 1654.028, 'src_real_tokens_per_sec': 10776.411662981343, 'trg_tokens_per_step': 2032.864, 'trg_tokens_per_sec': 13244.624225741589, 'trg_real_tokens_per_step': 1686.928, 'trg_real_tokens_per_sec': 10990.76350207481, 'samples_per_step': 55.844, 'samples_per_sec': 363.8378146606528, 'this_step_loss': 344905.90625}
I1209 01:33:11.686278 140215084762944 callbacks.py:220] Update 26000	TrainingLoss=3.54	Speed 0.154 secs/step 6.5 steps/sec
I1209 01:33:11.690244 140215084762944 callbacks.py:238] {'step': 26000, 'lr': 0.00038760857, 'loss': 3.539050340652466, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 13183.593445051903, 'src_real_tokens_per_step': 1652.84, 'src_real_tokens_per_sec': 10718.374981170407, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 13183.593445051903, 'trg_real_tokens_per_step': 1688.652, 'trg_real_tokens_per_sec': 10950.609465346537, 'samples_per_step': 55.713, 'samples_per_sec': 361.2889483107541, 'this_step_loss': 757383.0625}
I1209 01:33:12.512484 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-26000	Elapsed 0.82s
I1209 01:35:43.942674 140215084762944 callbacks.py:220] Update 27000	TrainingLoss=3.52	Speed 0.151 secs/step 6.6 steps/sec
I1209 01:35:43.947393 140215084762944 callbacks.py:238] {'step': 27000, 'lr': 0.00038036288, 'loss': 3.5177178382873535, 'src_tokens_per_step': 2033.024, 'src_tokens_per_sec': 13432.157456921514, 'src_real_tokens_per_step': 1653.868, 'src_real_tokens_per_sec': 10927.079753590646, 'trg_tokens_per_step': 2033.024, 'trg_tokens_per_sec': 13432.157456921514, 'trg_real_tokens_per_step': 1690.04, 'trg_real_tokens_per_sec': 11166.067586263436, 'samples_per_step': 55.955, 'samples_per_sec': 369.69380120551614, 'this_step_loss': 753678.8125}
I1209 01:38:19.246122 140215084762944 callbacks.py:220] Update 28000	TrainingLoss=3.50	Speed 0.155 secs/step 6.4 steps/sec
I1209 01:38:19.249643 140215084762944 callbacks.py:238] {'step': 28000, 'lr': 0.00037350896, 'loss': 3.4976346492767334, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 13098.194699610007, 'src_real_tokens_per_step': 1651.12, 'src_real_tokens_per_sec': 10637.737839948173, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 13098.194699610007, 'trg_real_tokens_per_step': 1688.58, 'trg_real_tokens_per_sec': 10879.082902381224, 'samples_per_step': 55.723, 'samples_per_sec': 359.008833794898, 'this_step_loss': 1454251.375}
I1209 01:38:20.089304 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-28000	Elapsed 0.83s
I1209 01:40:54.582643 140215084762944 callbacks.py:220] Update 29000	TrainingLoss=3.48	Speed 0.154 secs/step 6.5 steps/sec
I1209 01:40:54.587242 140215084762944 callbacks.py:238] {'step': 29000, 'lr': 0.00036701263, 'loss': 3.4790735244750977, 'src_tokens_per_step': 2032.888, 'src_tokens_per_sec': 13165.034692872652, 'src_real_tokens_per_step': 1653.516, 'src_real_tokens_per_sec': 10708.211915865517, 'trg_tokens_per_step': 2032.888, 'trg_tokens_per_sec': 13165.034692872652, 'trg_real_tokens_per_step': 1685.924, 'trg_real_tokens_per_sec': 10918.086952919508, 'samples_per_step': 55.619, 'samples_per_sec': 360.19006683244925, 'this_step_loss': 679932.625}
I1209 01:43:29.059874 140215084762944 callbacks.py:220] Update 30000	TrainingLoss=3.46	Speed 0.154 secs/step 6.5 steps/sec
I1209 01:43:29.065178 140215084762944 callbacks.py:238] {'step': 30000, 'lr': 0.00036084393, 'loss': 3.4609508514404297, 'src_tokens_per_step': 2032.96, 'src_tokens_per_sec': 13167.626926112482, 'src_real_tokens_per_step': 1651.564, 'src_real_tokens_per_sec': 10697.297830059635, 'trg_tokens_per_step': 2032.96, 'trg_tokens_per_sec': 13167.626926112482, 'trg_real_tokens_per_step': 1686.668, 'trg_real_tokens_per_sec': 10924.668941942924, 'samples_per_step': 55.93, 'samples_per_sec': 362.26259935142406, 'this_step_loss': 811731.0625}
I1209 01:43:29.826415 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-30000	Elapsed 0.75s
I1209 01:43:32.372024 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=81.35 (Best 81.35)  step=30000	Elapsed 2.55s  FromSTART 5639.60s
I1209 01:43:32.373718 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=8.00 (Best 8.00)  step=30000	Elapsed 2.55s  FromSTART 5639.60s
I1209 01:45:21.267241 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 01:45:21.269737 140215084762944 seq_generation_validator.py:179] Sample 1182
I1209 01:45:21.269875 140215084762944 seq_generation_validator.py:181]   Data: Some people come to us with a romantic idea and their head in the clouds, and when they find out they have to go to rehearsals twice a week, attend practice sessions and put up with a lot of time travelling to concerts, their enthusiasm quickly disappears.
I1209 01:45:21.269944 140215084762944 seq_generation_validator.py:182]   Reference: Manche kommen zu uns mit romantischen Vorstellungen, und wenn sie dann feststellen, dass sie zweimal die Woche Probe haben, Vorbereitungsphasen absolvieren mssen und viel Zeit auf Konzertreisen verbringen, dann erlischt die Begeisterung bei vielen schnell.
I1209 01:45:21.270004 140215084762944 seq_generation_validator.py:183]   Hypothesis: Einige Menschen kommen zu uns mit einer romantischen Vorstellung und ihrem Kopf in den Wolken, und wenn sie sich aussuchen mssen, um zweimal eine Woche zu reproduzieren, an der Praxis zu sitzen und mit viel Zeit nach Konzerte zu reisen.
I1209 01:45:21.270063 140215084762944 seq_generation_validator.py:179] Sample 2215
I1209 01:45:21.270118 140215084762944 seq_generation_validator.py:181]   Data: Even if the ceasefire agreed on November 21st holds, this week's fighting has strengthened the hawks on both sides.
I1209 01:45:21.270171 140215084762944 seq_generation_validator.py:182]   Reference: Selbst wenn der am 21.November vereinbarte Waffenstillstand anhlt, haben die dieswchigen Kmpfe die Hartgesottenen auf beiden Seiten gestrkt.
I1209 01:45:21.270220 140215084762944 seq_generation_validator.py:183]   Hypothesis: Auch wenn der Waffenstillstand am 21. November vereinbart wurde, hat diese Woche die Hawks auf beiden Seiten gestrkt.
I1209 01:45:21.270267 140215084762944 seq_generation_validator.py:179] Sample 2346
I1209 01:45:21.270330 140215084762944 seq_generation_validator.py:181]   Data: If you're bored and use that energy to play guitar and cook, it will make you happy.
I1209 01:45:21.270378 140215084762944 seq_generation_validator.py:182]   Reference: Wenn einem langweilig ist und man diese Energie verwendet, um Gitarre zu spielen und zu kochen, wird man glcklich.
I1209 01:45:21.270425 140215084762944 seq_generation_validator.py:183]   Hypothesis: Wenn Sie sich wagt und nutzen, um die Energie zu spielen und zu kochen, werden Sie glcklich machen.
I1209 01:45:21.270470 140215084762944 seq_generation_validator.py:179] Sample 132
I1209 01:45:21.270515 140215084762944 seq_generation_validator.py:181]   Data: A number of people, moreover, fervently hope that some will be found, because the slightest difference could open a door to a "new physics" and plug certain holes in the Model.
I1209 01:45:21.270561 140215084762944 seq_generation_validator.py:182]   Reference: Es gibt brigens zahlreiche Physiker, die sehnlichst hoffen, dass solche Abweichungen gefunden werden, da der geringste Unterschied eine Tr zu einer "neuen Physik" ffnen und einige Lcher des Modells stopfen knnte.
I1209 01:45:21.270609 140215084762944 seq_generation_validator.py:183]   Hypothesis: Eine Reihe von Menschen, die auch hoffen, dass einige gefunden werden, weil der geringste Unterschied zu einer "neuen physik" ffnen und bestimmte Lcher in dem Modell aufzeigt.
I1209 01:45:21.270655 140215084762944 seq_generation_validator.py:179] Sample 882
I1209 01:45:21.270705 140215084762944 seq_generation_validator.py:181]   Data: - Your work is marked by a recognizable style.
I1209 01:45:21.270754 140215084762944 seq_generation_validator.py:182]   Reference: - Deine Arbeiten haben einen unverkennbaren Stil.
I1209 01:45:21.270802 140215084762944 seq_generation_validator.py:183]   Hypothesis: - Ihre Arbeit ist von einem klaren Stil gekennzeichnet.
I1209 01:45:28.329153 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 7.06s
I1209 01:45:29.885098 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.55s
I1209 01:45:29.885366 140215084762944 training_utils.py:359] Evaluating bleu at step=30000 with bad count=0 (early_stop_patience=0).
I1209 01:45:29.885452 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=16.57 (Best 16.57)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:45:29.886917 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=16.72 (Best 16.72)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:45:29.887664 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=16.57 (Best 16.57)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:45:29.888233 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=45.62 (Best 45.62)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:45:29.888787 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=17.03 (Best 17.03)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:45:29.889329 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=17.19 (Best 17.19)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:45:29.889848 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=17.03 (Best 17.03)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:45:29.890385 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=46.95 (Best 46.95)  step=30000	Elapsed 97.64s  FromSTART 5732.27s
I1209 01:48:01.550544 140215084762944 callbacks.py:220] Update 31000	TrainingLoss=3.44	Speed 0.152 secs/step 6.6 steps/sec
I1209 01:48:02.196316 140215084762944 callbacks.py:238] {'step': 31000, 'lr': 0.00035497616, 'loss': 3.443824529647827, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 13411.57408437244, 'src_real_tokens_per_step': 1651.36, 'src_real_tokens_per_sec': 10893.96169781744, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 13411.57408437244, 'trg_real_tokens_per_step': 1689.58, 'trg_real_tokens_per_sec': 11146.097644001544, 'samples_per_step': 56.063, 'samples_per_sec': 369.8455664814087, 'this_step_loss': 649669.0625}
I1209 01:50:35.586807 140215084762944 callbacks.py:220] Update 32000	TrainingLoss=3.43	Speed 0.153 secs/step 6.5 steps/sec
I1209 01:50:35.596671 140215084762944 callbacks.py:238] {'step': 32000, 'lr': 0.00034938563, 'loss': 3.4278669357299805, 'src_tokens_per_step': 2032.968, 'src_tokens_per_sec': 13260.409839132544, 'src_real_tokens_per_step': 1650.292, 'src_real_tokens_per_sec': 10764.33484159206, 'trg_tokens_per_step': 2032.968, 'trg_tokens_per_sec': 13260.409839132544, 'trg_real_tokens_per_step': 1689.66, 'trg_real_tokens_per_sec': 11021.119903898485, 'samples_per_step': 55.93, 'samples_per_sec': 364.81377095098554, 'this_step_loss': 411917.125}
I1209 01:50:36.270506 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-32000	Elapsed 0.67s
I1209 01:53:08.209473 140215084762944 callbacks.py:220] Update 33000	TrainingLoss=3.41	Speed 0.152 secs/step 6.6 steps/sec
I1209 01:53:09.004643 140215084762944 callbacks.py:238] {'step': 33000, 'lr': 0.00034405116, 'loss': 3.4128963947296143, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 13387.298951135414, 'src_real_tokens_per_step': 1652.38, 'src_real_tokens_per_sec': 10880.960200963473, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 13387.298951135414, 'trg_real_tokens_per_step': 1686.908, 'trg_real_tokens_per_sec': 11108.327872938968, 'samples_per_step': 55.821, 'samples_per_sec': 367.5825653772026, 'this_step_loss': 366504.5625}
I1209 01:55:44.036978 140215084762944 callbacks.py:220] Update 34000	TrainingLoss=3.40	Speed 0.155 secs/step 6.5 steps/sec
I1209 01:55:44.043443 140215084762944 callbacks.py:238] {'step': 34000, 'lr': 0.00033895386, 'loss': 3.3983113765716553, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 13120.676070361109, 'src_real_tokens_per_step': 1650.52, 'src_real_tokens_per_sec': 10651.704724052586, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 13120.676070361109, 'trg_real_tokens_per_step': 1688.272, 'trg_real_tokens_per_sec': 10895.338946444579, 'samples_per_step': 55.93, 'samples_per_sec': 360.9467593341863, 'this_step_loss': 795438.875}
I1209 01:55:45.000077 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-34000	Elapsed 0.95s
I1209 01:58:17.440706 140215084762944 callbacks.py:220] Update 35000	TrainingLoss=3.38	Speed 0.152 secs/step 6.6 steps/sec
I1209 01:58:17.448869 140215084762944 callbacks.py:238] {'step': 35000, 'lr': 0.00033407655, 'loss': 3.3842053413391113, 'src_tokens_per_step': 2032.976, 'src_tokens_per_sec': 13342.589199191927, 'src_real_tokens_per_step': 1652.0, 'src_real_tokens_per_sec': 10842.21228241999, 'trg_tokens_per_step': 2032.976, 'trg_tokens_per_sec': 13342.589199191927, 'trg_real_tokens_per_step': 1689.452, 'trg_real_tokens_per_sec': 11088.012848038146, 'samples_per_step': 56.197, 'samples_per_sec': 368.82554699464663, 'this_step_loss': 463465.75}
I1209 02:00:51.695415 140215084762944 callbacks.py:220] Update 36000	TrainingLoss=3.37	Speed 0.154 secs/step 6.5 steps/sec
I1209 02:00:54.285125 140215084762944 callbacks.py:238] {'step': 36000, 'lr': 0.0003294039, 'loss': 3.3712046146392822, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 13187.063045915815, 'src_real_tokens_per_step': 1650.252, 'src_real_tokens_per_sec': 10704.408657608425, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 13187.063045915815, 'trg_real_tokens_per_step': 1685.516, 'trg_real_tokens_per_sec': 10933.14964195621, 'samples_per_step': 55.694, 'samples_per_sec': 361.26078670217856, 'this_step_loss': 172079.734375}
I1209 02:00:55.888971 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-36000	Elapsed 1.60s
I1209 02:03:22.881692 140215084762944 callbacks.py:220] Update 37000	TrainingLoss=3.36	Speed 0.147 secs/step 6.8 steps/sec
I1209 02:03:22.907530 140215084762944 callbacks.py:238] {'step': 37000, 'lr': 0.00032492203, 'loss': 3.3584988117218018, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 13838.29713078487, 'src_real_tokens_per_step': 1652.804, 'src_real_tokens_per_sec': 11249.834169635744, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 13838.29713078487, 'trg_real_tokens_per_step': 1685.672, 'trg_real_tokens_per_sec': 11473.550683806565, 'samples_per_step': 56.036, 'samples_per_sec': 381.40983899464703, 'this_step_loss': 146486.75}
I1209 02:05:57.333132 140215084762944 callbacks.py:220] Update 38000	TrainingLoss=3.35	Speed 0.154 secs/step 6.5 steps/sec
I1209 02:05:57.343764 140215084762944 callbacks.py:238] {'step': 38000, 'lr': 0.00032061824, 'loss': 3.3464648723602295, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 13172.712091049696, 'src_real_tokens_per_step': 1650.216, 'src_real_tokens_per_sec': 10692.48413971651, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 13172.712091049696, 'trg_real_tokens_per_step': 1688.944, 'trg_real_tokens_per_sec': 10943.420093411627, 'samples_per_step': 56.013, 'samples_per_sec': 362.9331639724381, 'this_step_loss': 344857.59375}
I1209 02:05:58.143137 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-38000	Elapsed 0.79s
I1209 02:08:30.348028 140215084762944 callbacks.py:220] Update 39000	TrainingLoss=3.33	Speed 0.152 secs/step 6.6 steps/sec
I1209 02:08:30.355917 140215084762944 callbacks.py:238] {'step': 39000, 'lr': 0.00031648105, 'loss': 3.3348965644836426, 'src_tokens_per_step': 2033.04, 'src_tokens_per_sec': 13363.670465663821, 'src_real_tokens_per_step': 1651.396, 'src_real_tokens_per_sec': 10855.030866247282, 'trg_tokens_per_step': 2033.04, 'trg_tokens_per_sec': 13363.670465663821, 'trg_real_tokens_per_step': 1687.732, 'trg_real_tokens_per_sec': 11093.876304625455, 'samples_per_step': 56.055, 'samples_per_sec': 368.463260906222, 'this_step_loss': 417237.5625}
I1209 02:11:02.916113 140215084762944 callbacks.py:220] Update 40000	TrainingLoss=3.32	Speed 0.152 secs/step 6.6 steps/sec
I1209 02:11:02.947359 140215084762944 callbacks.py:238] {'step': 40000, 'lr': 0.0003125, 'loss': 3.323765754699707, 'src_tokens_per_step': 2033.128, 'src_tokens_per_sec': 13333.569741686251, 'src_real_tokens_per_step': 1651.392, 'src_real_tokens_per_sec': 10830.085662517433, 'trg_tokens_per_step': 2033.128, 'trg_tokens_per_sec': 13333.569741686251, 'trg_real_tokens_per_step': 1688.524, 'trg_real_tokens_per_sec': 11073.603095580325, 'samples_per_step': 55.937, 'samples_per_sec': 366.8435487783867, 'this_step_loss': 742398.5625}
I1209 02:11:03.640170 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-40000	Elapsed 0.69s
I1209 02:11:06.200866 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=78.58 (Best 78.58)  step=40000	Elapsed 2.56s  FromSTART 7293.43s
I1209 02:11:06.202459 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.45 (Best 7.45)  step=40000	Elapsed 2.56s  FromSTART 7293.43s
I1209 02:12:47.183054 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 02:12:47.189972 140215084762944 seq_generation_validator.py:179] Sample 1516
I1209 02:12:47.190092 140215084762944 seq_generation_validator.py:181]   Data: At the beginning of November, the Bcherberg was awarded second place in the "Best Library of NL 2012" competition.
I1209 02:12:47.190153 140215084762944 seq_generation_validator.py:182]   Reference: Anfang November wurde der Bcherberg mit dem zweiten Preis "Best Library of NL 2012" ausgezeichnet.
I1209 02:12:47.190204 140215084762944 seq_generation_validator.py:183]   Hypothesis: Zu Beginn November wurde das Bcherberg in der "Best Library of NL" Wettbewerb verliehen.
I1209 02:12:47.190252 140215084762944 seq_generation_validator.py:179] Sample 1019
I1209 02:12:47.190308 140215084762944 seq_generation_validator.py:181]   Data: These are domestic problems about the practical implementation of legislation relating to public contracts.
I1209 02:12:47.190355 140215084762944 seq_generation_validator.py:182]   Reference: Sie sprechen ber die Probleme bei der praktischen Umsetzung des tschechischen Vergabegesetzes.
I1209 02:12:47.190401 140215084762944 seq_generation_validator.py:183]   Hypothesis: Dies sind innenpolitische Probleme hinsichtlich der praktischen Umsetzung der Rechtsvorschriften in Bezug auf ffentliche Vertrge.
I1209 02:12:47.190447 140215084762944 seq_generation_validator.py:179] Sample 1796
I1209 02:12:47.190494 140215084762944 seq_generation_validator.py:181]   Data: Unknown to the majority of the Earth's inhabitants, there are probes, telescopes and small robots such as the Phoenix, deployed to research the depths of the universe.
I1209 02:12:47.190539 140215084762944 seq_generation_validator.py:182]   Reference: Unbemerkt von den Erdenbrgern sind Sonden, Teleskope und kleine Roboter wie Phoenix dabei, die Tiefen des Weltalls zu erforschen.
I1209 02:12:47.190584 140215084762944 seq_generation_validator.py:183]   Hypothesis: Unbekannt bekannt fr die Mehrheit der Bewohner der Erde, es gibt wahrscheinlich, Teleskopes und kleine Roboter wie die Liga, die zur Erforschung der Lagen des Universums eingesetzt werden.
I1209 02:12:47.190631 140215084762944 seq_generation_validator.py:179] Sample 1128
I1209 02:12:47.190677 140215084762944 seq_generation_validator.py:181]   Data: It turns out that most of them are interested and enjoy working with us.
I1209 02:12:47.190721 140215084762944 seq_generation_validator.py:182]   Reference: Dabei hat sich gezeigt, dass die meisten an einer Zusammenarbeit mit uns interessiert sind, und Spa daran haben.
I1209 02:12:47.190766 140215084762944 seq_generation_validator.py:183]   Hypothesis: Es zeigt sich, dass die meisten von ihnen interessiert sind und mit uns arbeiten.
I1209 02:12:47.190818 140215084762944 seq_generation_validator.py:179] Sample 566
I1209 02:12:47.190865 140215084762944 seq_generation_validator.py:181]   Data: In essence, it is an underground cable railway.
I1209 02:12:47.190911 140215084762944 seq_generation_validator.py:182]   Reference: Genau genommen ist es eine unterirdische Standseilbahn.
I1209 02:12:47.190955 140215084762944 seq_generation_validator.py:183]   Hypothesis: Im Wesentlichen ist es eine U-Bahn-Bahn.
I1209 02:12:48.948423 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.76s
I1209 02:12:50.127833 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.18s
I1209 02:12:50.128214 140215084762944 training_utils.py:359] Evaluating bleu at step=40000 with bad count=0 (early_stop_patience=0).
I1209 02:12:50.128320 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.36 (Best 17.36)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:12:50.129659 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.51 (Best 17.51)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:12:50.130356 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.36 (Best 17.36)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:12:50.130877 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.53 (Best 46.53)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:12:50.131385 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=17.83 (Best 17.83)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:12:50.131871 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.00 (Best 18.00)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:12:50.132349 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=17.83 (Best 17.83)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:12:50.132813 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=47.83 (Best 47.83)  step=40000	Elapsed 89.96s  FromSTART 7378.42s
I1209 02:15:19.822481 140215084762944 callbacks.py:220] Update 41000	TrainingLoss=3.31	Speed 0.150 secs/step 6.7 steps/sec
I1209 02:15:19.827791 140215084762944 callbacks.py:238] {'step': 41000, 'lr': 0.00030866548, 'loss': 3.312797784805298, 'src_tokens_per_step': 2032.984, 'src_tokens_per_sec': 13588.244864998296, 'src_real_tokens_per_step': 1652.716, 'src_real_tokens_per_sec': 11046.574739545675, 'trg_tokens_per_step': 2032.984, 'trg_tokens_per_sec': 13588.244864998296, 'trg_real_tokens_per_step': 1688.304, 'trg_real_tokens_per_sec': 11284.441077035572, 'samples_per_step': 55.985, 'samples_per_sec': 374.1976763058291, 'this_step_loss': 402196.65625}
I1209 02:17:51.277453 140215084762944 callbacks.py:220] Update 42000	TrainingLoss=3.30	Speed 0.151 secs/step 6.6 steps/sec
I1209 02:17:52.029338 140215084762944 callbacks.py:238] {'step': 42000, 'lr': 0.00030496877, 'loss': 3.3025360107421875, 'src_tokens_per_step': 2033.072, 'src_tokens_per_sec': 13431.267187065694, 'src_real_tokens_per_step': 1650.856, 'src_real_tokens_per_sec': 10906.19910331288, 'trg_tokens_per_step': 2033.072, 'trg_tokens_per_sec': 13431.267187065694, 'trg_real_tokens_per_step': 1687.32, 'trg_real_tokens_per_sec': 11147.094520056195, 'samples_per_step': 55.921, 'samples_per_sec': 369.4359532608293, 'this_step_loss': 356976.03125}
I1209 02:17:52.754209 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-42000	Elapsed 0.72s
I1209 02:20:24.186176 140215084762944 callbacks.py:220] Update 43000	TrainingLoss=3.29	Speed 0.151 secs/step 6.6 steps/sec
I1209 02:20:24.192222 140215084762944 callbacks.py:238] {'step': 43000, 'lr': 0.00030140177, 'loss': 3.2929270267486572, 'src_tokens_per_step': 2033.144, 'src_tokens_per_sec': 13432.815359338636, 'src_real_tokens_per_step': 1650.016, 'src_real_tokens_per_sec': 10901.520142181025, 'trg_tokens_per_step': 2033.144, 'trg_tokens_per_sec': 13432.815359338636, 'trg_real_tokens_per_step': 1690.32, 'trg_real_tokens_per_sec': 11167.805358694359, 'samples_per_step': 56.01, 'samples_per_sec': 370.0534680654971, 'this_step_loss': 188372.4375}
I1209 02:22:59.669377 140215084762944 callbacks.py:220] Update 44000	TrainingLoss=3.28	Speed 0.155 secs/step 6.4 steps/sec
I1209 02:22:59.675444 140215084762944 callbacks.py:238] {'step': 44000, 'lr': 0.00029795707, 'loss': 3.283226490020752, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 13082.599733879902, 'src_real_tokens_per_step': 1652.96, 'src_real_tokens_per_sec': 10636.912870392622, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 13082.599733879902, 'trg_real_tokens_per_step': 1687.536, 'trg_real_tokens_per_sec': 10859.411841575647, 'samples_per_step': 56.15, 'samples_per_sec': 361.32916566193114, 'this_step_loss': 175325.71875}
I1209 02:23:00.455791 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-44000	Elapsed 0.77s
I1209 02:25:31.736219 140215084762944 callbacks.py:220] Update 45000	TrainingLoss=3.27	Speed 0.151 secs/step 6.6 steps/sec
I1209 02:25:31.742264 140215084762944 callbacks.py:238] {'step': 45000, 'lr': 0.00029462783, 'loss': 3.2742676734924316, 'src_tokens_per_step': 2032.88, 'src_tokens_per_sec': 13444.271731337984, 'src_real_tokens_per_step': 1653.104, 'src_real_tokens_per_sec': 10932.65681012246, 'trg_tokens_per_step': 2032.88, 'trg_tokens_per_sec': 13444.271731337984, 'trg_real_tokens_per_step': 1691.744, 'trg_real_tokens_per_sec': 11188.199025943806, 'samples_per_step': 55.661, 'samples_per_sec': 368.1090909635608, 'this_step_loss': 311389.25}
I1209 02:28:05.063386 140215084762944 callbacks.py:220] Update 46000	TrainingLoss=3.27	Speed 0.153 secs/step 6.5 steps/sec
I1209 02:28:05.071633 140215084762944 callbacks.py:238] {'step': 46000, 'lr': 0.00029140775, 'loss': 3.265388250350952, 'src_tokens_per_step': 2033.112, 'src_tokens_per_sec': 13267.372221591504, 'src_real_tokens_per_step': 1649.384, 'src_real_tokens_per_sec': 10763.298561189684, 'trg_tokens_per_step': 2033.112, 'trg_tokens_per_sec': 13267.372221591504, 'trg_real_tokens_per_step': 1688.368, 'trg_real_tokens_per_sec': 11017.694402976325, 'samples_per_step': 56.166, 'samples_per_sec': 366.5195169759012, 'this_step_loss': 296103.03125}
I1209 02:28:05.879931 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-46000	Elapsed 0.80s
I1209 02:30:38.867996 140215084762944 callbacks.py:220] Update 47000	TrainingLoss=3.26	Speed 0.153 secs/step 6.5 steps/sec
I1209 02:30:38.875567 140215084762944 callbacks.py:238] {'step': 47000, 'lr': 0.00028829102, 'loss': 3.256754159927368, 'src_tokens_per_step': 2033.088, 'src_tokens_per_sec': 13295.482810304431, 'src_real_tokens_per_step': 1653.256, 'src_real_tokens_per_sec': 10811.552047443427, 'trg_tokens_per_step': 2033.088, 'trg_tokens_per_sec': 13295.482810304431, 'trg_real_tokens_per_step': 1687.68, 'trg_real_tokens_per_sec': 11036.669553553305, 'samples_per_step': 56.059, 'samples_per_sec': 366.6006935572174, 'this_step_loss': 359439.28125}
I1209 02:33:14.453413 140215084762944 callbacks.py:220] Update 48000	TrainingLoss=3.25	Speed 0.155 secs/step 6.4 steps/sec
I1209 02:33:14.462649 140215084762944 callbacks.py:238] {'step': 48000, 'lr': 0.00028527217, 'loss': 3.2485556602478027, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 13073.462503544182, 'src_real_tokens_per_step': 1652.28, 'src_real_tokens_per_sec': 10625.612727188469, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 13073.462503544182, 'trg_real_tokens_per_step': 1691.552, 'trg_real_tokens_per_sec': 10878.16620663635, 'samples_per_step': 55.69, 'samples_per_sec': 358.1356506022743, 'this_step_loss': 300659.1875}
I1209 02:33:16.210711 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-48000	Elapsed 1.74s
I1209 02:35:48.432564 140215084762944 callbacks.py:220] Update 49000	TrainingLoss=3.24	Speed 0.152 secs/step 6.6 steps/sec
I1209 02:35:48.440938 140215084762944 callbacks.py:238] {'step': 49000, 'lr': 0.0002823462, 'loss': 3.2405495643615723, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 13361.761222835978, 'src_real_tokens_per_step': 1648.272, 'src_real_tokens_per_sec': 10833.203915355449, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 13361.761222835978, 'trg_real_tokens_per_step': 1690.448, 'trg_real_tokens_per_sec': 11110.40404272158, 'samples_per_step': 55.951, 'samples_per_sec': 367.7357816355873, 'this_step_loss': 334424.5625}
I1209 02:38:22.244242 140215084762944 callbacks.py:220] Update 50000	TrainingLoss=3.23	Speed 0.154 secs/step 6.5 steps/sec
I1209 02:38:22.251619 140215084762944 callbacks.py:238] {'step': 50000, 'lr': 0.0002795085, 'loss': 3.2325639724731445, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 13225.333689846702, 'src_real_tokens_per_step': 1650.904, 'src_real_tokens_per_sec': 10739.250934494798, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 13225.333689846702, 'trg_real_tokens_per_step': 1688.304, 'trg_real_tokens_per_sec': 10982.540662395453, 'samples_per_step': 56.031, 'samples_per_sec': 364.4857418182268, 'this_step_loss': 411914.40625}
I1209 02:38:22.997980 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-50000	Elapsed 0.74s
I1209 02:38:25.600786 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=77.09 (Best 77.09)  step=50000	Elapsed 2.60s  FromSTART 8932.83s
I1209 02:38:25.602369 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.17 (Best 7.17)  step=50000	Elapsed 2.60s  FromSTART 8932.83s
I1209 02:40:09.873972 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 02:40:09.891739 140215084762944 seq_generation_validator.py:179] Sample 2271
I1209 02:40:09.891859 140215084762944 seq_generation_validator.py:181]   Data: Counterfeiters from China and Indonesia will bring lots more of these products down to sell on the streets of Australia.
I1209 02:40:09.891916 140215084762944 seq_generation_validator.py:182]   Reference: Chinesische und indonesische Flscher werden weitaus grere Mengen dieser Produkte auf den Straen Australiens anbieten.
I1209 02:40:09.891967 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die Tierzchter aus China und Indonesien werden in den Straen von Australien viel mehr bringen.
I1209 02:40:09.892014 140215084762944 seq_generation_validator.py:179] Sample 2833
I1209 02:40:09.892060 140215084762944 seq_generation_validator.py:181]   Data: Indeed, the UN Model Law for International Trade Law Uniformity was created for this purpose, with the American Law Institute positioned as arbitrator.
I1209 02:40:09.892105 140215084762944 seq_generation_validator.py:182]   Reference: Tatschlich wurde zu diesem Zweck das Modellgesetz der Vereinten Nationen fr die Vereinheitlichung des internationalen Handelsrechts geschaffen, wobei das American Law Institute als Schlichter eingesetzt wurde.
I1209 02:40:09.892153 140215084762944 seq_generation_validator.py:183]   Hypothesis: Tatschlich wurde das UN-Modell fr das Internationale Handelsgesetz Uniformitt geschaffen, wobei das amerikanische Gesetz als Willatorin positioniert wurde.
I1209 02:40:09.892198 140215084762944 seq_generation_validator.py:179] Sample 1249
I1209 02:40:09.892242 140215084762944 seq_generation_validator.py:181]   Data: On this basis, it should be understood that when a supplier tries to compete in a commercial tender by importing cheap goods, "the rug is pulled" from under the competition's prices to capture a greater market share and, in this way, increase its own profits.
I1209 02:40:09.892288 140215084762944 seq_generation_validator.py:182]   Reference: Unter diesem Begriff ist ein Zustand zu verstehen, bei dem der Hersteller versucht, durch Einfuhr gnstiger Waren im wirtschaftlichen Wettbewerb zu bestehen, indem er die Konkurrenz "unterbietet", um dadurch einen greren Marktanteil an sich zu reien und seinen Gewinn zu erhhen.
I1209 02:40:09.892352 140215084762944 seq_generation_validator.py:183]   Hypothesis: Auf dieser Basis sollte man verstanden werden, dass, wenn ein Lieferant versucht, sich in eine kommerzielle Ausschreibung zu konkurrieren, indem man billige Gter importieren will ", dann wird der Jung unter den Preisen des Wettbewerbs, einen strkeren Marktanteil zu sammeln und damit seine eigenen Gewinne zu erhhen.
I1209 02:40:09.892398 140215084762944 seq_generation_validator.py:179] Sample 2190
I1209 02:40:09.892445 140215084762944 seq_generation_validator.py:181]   Data: In Mexico, we're seen as Americans.
I1209 02:40:09.892490 140215084762944 seq_generation_validator.py:182]   Reference: In Mexiko werden wir als Amerikaner gesehen.
I1209 02:40:09.892537 140215084762944 seq_generation_validator.py:183]   Hypothesis: In Mexiko werden wir als Amerikaner gesehen.
I1209 02:40:09.892581 140215084762944 seq_generation_validator.py:179] Sample 1841
I1209 02:40:09.892624 140215084762944 seq_generation_validator.py:181]   Data: The inner workings of the Gran Telescopio Canarias on the Canarian island of La Palma are huge - the mirror alone has a diameter of 10.4 metres.
I1209 02:40:09.892668 140215084762944 seq_generation_validator.py:182]   Reference: Das Innenleben des Gran Telescopio Canarias auf der Kanareninsel La Palma ist riesig - alleine der Spiegel kommt auf 10,4 Meter im Durchmesser.
I1209 02:40:09.892711 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die inneren Workings der Gran Teleskopio Canarias auf der kanarischen Insel La Palma sind enorm - der Spiegel allein hat einen Durchmesser von 10.4 Meter.
I1209 02:40:11.490476 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.60s
I1209 02:40:13.246141 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.76s
I1209 02:40:13.246897 140215084762944 training_utils.py:359] Evaluating bleu at step=50000 with bad count=0 (early_stop_patience=0).
I1209 02:40:13.247045 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.60 (Best 17.60)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:40:13.248560 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.73 (Best 17.73)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:40:13.249459 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.60 (Best 17.60)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:40:13.250175 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.94 (Best 46.94)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:40:13.250864 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.05 (Best 18.05)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:40:13.251520 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.18 (Best 18.18)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:40:13.252154 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.05 (Best 18.05)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:40:13.252807 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.24 (Best 48.24)  step=50000	Elapsed 93.88s  FromSTART 9021.74s
I1209 02:42:42.245133 140215084762944 callbacks.py:220] Update 51000	TrainingLoss=3.23	Speed 0.149 secs/step 6.7 steps/sec
I1209 02:42:42.284398 140215084762944 callbacks.py:238] {'step': 51000, 'lr': 0.00027675464, 'loss': 3.2251689434051514, 'src_tokens_per_step': 2032.952, 'src_tokens_per_sec': 13651.174677430708, 'src_real_tokens_per_step': 1652.832, 'src_real_tokens_per_sec': 11098.687201885314, 'trg_tokens_per_step': 2032.952, 'trg_tokens_per_sec': 13651.174677430708, 'trg_real_tokens_per_step': 1687.072, 'trg_real_tokens_per_sec': 11328.607151276754, 'samples_per_step': 55.892, 'samples_per_sec': 375.3120856129201, 'this_step_loss': 374531.59375}
I1209 02:45:16.354602 140215084762944 callbacks.py:220] Update 52000	TrainingLoss=3.22	Speed 0.154 secs/step 6.5 steps/sec
I1209 02:45:16.360981 140215084762944 callbacks.py:238] {'step': 52000, 'lr': 0.00027408064, 'loss': 3.2178409099578857, 'src_tokens_per_step': 2032.928, 'src_tokens_per_sec': 13201.308904851645, 'src_real_tokens_per_step': 1652.712, 'src_real_tokens_per_sec': 10732.284489541771, 'trg_tokens_per_step': 2032.928, 'trg_tokens_per_sec': 13201.308904851645, 'trg_real_tokens_per_step': 1687.536, 'trg_real_tokens_per_sec': 10958.422543276361, 'samples_per_step': 55.916, 'samples_per_sec': 363.1040492942616, 'this_step_loss': 409221.4375}
I1209 02:45:17.047808 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-52000	Elapsed 0.68s
I1209 02:47:50.689862 140215084762944 callbacks.py:220] Update 53000	TrainingLoss=3.21	Speed 0.154 secs/step 6.5 steps/sec
I1209 02:47:50.695964 140215084762944 callbacks.py:238] {'step': 53000, 'lr': 0.00027148266, 'loss': 3.210783004760742, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 13238.16687933111, 'src_real_tokens_per_step': 1652.744, 'src_real_tokens_per_sec': 10761.991484972677, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 13238.16687933111, 'trg_real_tokens_per_step': 1691.024, 'trg_real_tokens_per_sec': 11011.255154388362, 'samples_per_step': 55.86, 'samples_per_sec': 363.73742355172595, 'this_step_loss': 188700.59375}
I1209 02:50:24.741200 140215084762944 callbacks.py:220] Update 54000	TrainingLoss=3.20	Speed 0.154 secs/step 6.5 steps/sec
I1209 02:50:24.746963 140215084762944 callbacks.py:238] {'step': 54000, 'lr': 0.00026895717, 'loss': 3.2036616802215576, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 13203.949669727795, 'src_real_tokens_per_step': 1654.312, 'src_real_tokens_per_sec': 10744.358325771529, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 13203.949669727795, 'trg_real_tokens_per_step': 1689.336, 'trg_real_tokens_per_sec': 10971.830777160276, 'samples_per_step': 55.766, 'samples_per_sec': 362.1867497757225, 'this_step_loss': 188282.34375}
I1209 02:50:25.507324 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-54000	Elapsed 0.75s
I1209 02:52:55.829227 140215084762944 callbacks.py:220] Update 55000	TrainingLoss=3.20	Speed 0.150 secs/step 6.7 steps/sec
I1209 02:52:55.835909 140215084762944 callbacks.py:238] {'step': 55000, 'lr': 0.0002665009, 'loss': 3.1969118118286133, 'src_tokens_per_step': 2033.072, 'src_tokens_per_sec': 13531.096288656134, 'src_real_tokens_per_step': 1649.472, 'src_real_tokens_per_sec': 10978.049207033599, 'trg_tokens_per_step': 2033.072, 'trg_tokens_per_sec': 13531.096288656134, 'trg_real_tokens_per_step': 1689.808, 'trg_real_tokens_per_sec': 11246.505169192948, 'samples_per_step': 56.06, 'samples_per_sec': 373.10693273138526, 'this_step_loss': 395324.46875}
I1209 02:55:30.933251 140215084762944 callbacks.py:220] Update 56000	TrainingLoss=3.19	Speed 0.155 secs/step 6.5 steps/sec
I1209 02:55:31.106900 140215084762944 callbacks.py:238] {'step': 56000, 'lr': 0.0002641107, 'loss': 3.1905033588409424, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 13114.399215516252, 'src_real_tokens_per_step': 1651.824, 'src_real_tokens_per_sec': 10655.439686540056, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 13114.399215516252, 'trg_real_tokens_per_step': 1687.696, 'trg_real_tokens_per_sec': 10886.839601080324, 'samples_per_step': 55.994, 'samples_per_sec': 361.20112663826404, 'this_step_loss': 370491.9375}
I1209 02:55:31.814210 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-56000	Elapsed 0.70s
I1209 02:58:04.450634 140215084762944 callbacks.py:220] Update 57000	TrainingLoss=3.18	Speed 0.153 secs/step 6.6 steps/sec
I1209 02:58:04.468324 140215084762944 callbacks.py:238] {'step': 57000, 'lr': 0.0002617837, 'loss': 3.184303045272827, 'src_tokens_per_step': 2032.944, 'src_tokens_per_sec': 13324.851934421262, 'src_real_tokens_per_step': 1651.6, 'src_real_tokens_per_sec': 10825.347601749067, 'trg_tokens_per_step': 2032.944, 'trg_tokens_per_sec': 13324.851934421262, 'trg_real_tokens_per_step': 1687.84, 'trg_real_tokens_per_sec': 11062.88126431106, 'samples_per_step': 55.953, 'samples_per_sec': 366.7417500367314, 'this_step_loss': 151375.421875}
I1209 03:00:38.888030 140215084762944 callbacks.py:220] Update 58000	TrainingLoss=3.18	Speed 0.154 secs/step 6.5 steps/sec
I1209 03:00:39.641970 140215084762944 callbacks.py:238] {'step': 58000, 'lr': 0.0002595171, 'loss': 3.178253412246704, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 13171.878010322063, 'src_real_tokens_per_step': 1652.248, 'src_real_tokens_per_sec': 10705.01462809426, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 13171.878010322063, 'trg_real_tokens_per_step': 1689.104, 'trg_real_tokens_per_sec': 10943.806879095953, 'samples_per_step': 56.041, 'samples_per_sec': 363.0930252438075, 'this_step_loss': 240253.09375}
I1209 03:00:40.823840 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-58000	Elapsed 1.18s
I1209 03:03:13.809682 140215084762944 callbacks.py:220] Update 59000	TrainingLoss=3.17	Speed 0.153 secs/step 6.5 steps/sec
I1209 03:03:16.071214 140215084762944 callbacks.py:238] {'step': 59000, 'lr': 0.00025730842, 'loss': 3.1722872257232666, 'src_tokens_per_step': 2032.848, 'src_tokens_per_sec': 13293.79208569402, 'src_real_tokens_per_step': 1653.664, 'src_real_tokens_per_sec': 10814.121565211524, 'trg_tokens_per_step': 2032.848, 'trg_tokens_per_sec': 13293.79208569402, 'trg_real_tokens_per_step': 1687.152, 'trg_real_tokens_per_sec': 11033.116054403888, 'samples_per_step': 55.613, 'samples_per_sec': 363.68073720302823, 'this_step_loss': 427407.75}
I1209 03:05:46.962930 140215084762944 callbacks.py:220] Update 60000	TrainingLoss=3.17	Speed 0.151 secs/step 6.6 steps/sec
I1209 03:05:46.970546 140215084762944 callbacks.py:238] {'step': 60000, 'lr': 0.00025515517, 'loss': 3.1666951179504395, 'src_tokens_per_step': 2032.912, 'src_tokens_per_sec': 13479.26239463305, 'src_real_tokens_per_step': 1650.792, 'src_real_tokens_per_sec': 10945.608332756696, 'trg_tokens_per_step': 2032.912, 'trg_tokens_per_sec': 13479.26239463305, 'trg_real_tokens_per_step': 1687.08, 'trg_real_tokens_per_sec': 11186.216619675384, 'samples_per_step': 55.978, 'samples_per_sec': 371.1632133249097, 'this_step_loss': 381751.0625}
I1209 03:05:48.054478 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-60000	Elapsed 1.08s
I1209 03:05:52.219958 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=76.23 (Best 76.23)  step=60000	Elapsed 4.17s  FromSTART 10579.45s
I1209 03:05:52.222506 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.02 (Best 7.02)  step=60000	Elapsed 4.17s  FromSTART 10579.45s
I1209 03:07:30.198077 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 03:07:30.931232 140215084762944 seq_generation_validator.py:179] Sample 1927
I1209 03:07:30.932179 140215084762944 seq_generation_validator.py:181]   Data: Things were at their worst during the time after the Second World War.
I1209 03:07:30.932264 140215084762944 seq_generation_validator.py:182]   Reference: Am schlimmsten war es in der Zeit nach dem Zweiten Weltkrieg.
I1209 03:07:30.932340 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die Dinge waren whrend der Zeit nach dem Zweiten Weltkrieg am Schlimmsten.
I1209 03:07:30.932393 140215084762944 seq_generation_validator.py:179] Sample 210
I1209 03:07:30.932440 140215084762944 seq_generation_validator.py:181]   Data: This report was made possible thanks to a journalism award from the Canada health research institutes.
I1209 03:07:30.932487 140215084762944 seq_generation_validator.py:182]   Reference: Die Realisierung dieses Berichts war mglich aufgrund eines Journalismusstipendiums kanadischer Gesundheitsforschungsinstitute.
I1209 03:07:30.932543 140215084762944 seq_generation_validator.py:183]   Hypothesis: Dieser Bericht wurde dank eines Journalismus-Auszeichnung der kanadischen Gesundheitsforschung mglich gemacht.
I1209 03:07:30.932590 140215084762944 seq_generation_validator.py:179] Sample 2217
I1209 03:07:30.932638 140215084762944 seq_generation_validator.py:181]   Data: Despite killing some of its leaders and bottling up Gaza's 1.7m people in one of the most wretched and crowded corners of the planet, Israel has failed to destroy Hamas.
I1209 03:07:30.932683 140215084762944 seq_generation_validator.py:182]   Reference: Obwohl mehrere Hamas-Fhrer gettet und die 1,7 Millionen Einwohner des Gazastreifens in einem der desolatesten und dicht bevlkertesten Ecken des Planeten eingepfercht werden, ist es Israel bis jetzt nicht gelungen, die Hamas zu vernichten.
I1209 03:07:30.932741 140215084762944 seq_generation_validator.py:183]   Hypothesis: Trotz der Ttung einiger seiner Staats- und Regierungschefs und die Stilllegung der 1.7m Menschen in einem der abgewanderten und berdachten Ecken des Planeten, hat Israel die Hamas nicht zerstrt.
I1209 03:07:30.932790 140215084762944 seq_generation_validator.py:179] Sample 1320
I1209 03:07:30.932834 140215084762944 seq_generation_validator.py:181]   Data: World War II broke out, and whether Hitler broke free from the leash, which international bankers were holding him on, or whether his actions were all part of the plan, is difficult to determine, nevertheless the suffering of European Jews in the concentration camps created the foundation to the world's acceptance of the Jewish State.
I1209 03:07:30.932879 140215084762944 seq_generation_validator.py:182]   Reference: Der Zweite Weltkrieg brach aus, und ob sich nun Hitler von der Kette losriss, an der ihn die internationalen Banker anfnglich gehalten hatten, oder ob alles nach Plan lief, ist nur schwer abzuschtzen, auf jeden Fall schuf das Leid der europischen Juden in den Konzentrationslagern die Voraussetzung fr die Anerkennung des jdischen Staates durch die Weltgemeinschaft.
I1209 03:07:30.932927 140215084762944 seq_generation_validator.py:183]   Hypothesis: Der Zweiten Weltkrieg hat sich ausgebrochen, und ob Hitler von der Leash ausgebrochen ist, die internationale Banker ihn fhren, oder ob seine Aktionen alle Teil des Plans waren, ist schwer zu bestimmen, aber das Leid europischer Juden in den Konzentrationslager hat die Grundlage fr die Akzeptanz des jdischen Staates geschaffen.
I1209 03:07:30.932973 140215084762944 seq_generation_validator.py:179] Sample 1400
I1209 03:07:30.933017 140215084762944 seq_generation_validator.py:181]   Data: The European Commission finally met the CR halfway by organising a tender for the purchase of cereals from countries that do not have access to the sea.
I1209 03:07:30.933062 140215084762944 seq_generation_validator.py:182]   Reference: Die Europische Kommission kam Tschechien schlielich so weit entgegen, dass sie eine Ausschreibung ber den Einkauf von Getreide aus Lndern organisierte, die keinen Zugang zum Meer haben.
I1209 03:07:30.933107 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die Europische Kommission hat sich endlich mit der Einfhrung einer Ausschreibung fr den Kauf von Getreide aus Lndern, die nicht Zugang zum Meer haben, erreicht.
I1209 03:07:36.693463 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 5.76s
I1209 03:07:38.425024 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.73s
I1209 03:07:38.425483 140215084762944 training_utils.py:359] Evaluating bleu at step=60000 with bad count=0 (early_stop_patience=0).
I1209 03:07:38.425601 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.78 (Best 17.78)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:07:38.427675 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.95 (Best 17.95)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:07:38.428694 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.78 (Best 17.78)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:07:38.429338 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.16 (Best 47.16)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:07:38.429885 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.26 (Best 18.26)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:07:38.430411 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.44 (Best 18.44)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:07:38.430905 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.26 (Best 18.26)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:07:38.431398 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.48 (Best 48.48)  step=60000	Elapsed 87.70s  FromSTART 10662.18s
I1209 03:10:06.500774 140215084762944 callbacks.py:220] Update 61000	TrainingLoss=3.16	Speed 0.148 secs/step 6.8 steps/sec
I1209 03:10:06.506253 140215084762944 callbacks.py:238] {'step': 61000, 'lr': 0.0002530551, 'loss': 3.1611483097076416, 'src_tokens_per_step': 2032.976, 'src_tokens_per_sec': 13736.532422424812, 'src_real_tokens_per_step': 1649.728, 'src_real_tokens_per_sec': 11146.97967914134, 'trg_tokens_per_step': 2032.976, 'trg_tokens_per_sec': 13736.532422424812, 'trg_real_tokens_per_step': 1688.552, 'trg_real_tokens_per_sec': 11409.307977541428, 'samples_per_step': 55.995, 'samples_per_sec': 378.3503263165317, 'this_step_loss': 398398.65625}
I1209 03:12:38.579688 140215084762944 callbacks.py:220] Update 62000	TrainingLoss=3.16	Speed 0.152 secs/step 6.6 steps/sec
I1209 03:12:38.585304 140215084762944 callbacks.py:238] {'step': 62000, 'lr': 0.00025100604, 'loss': 3.1558687686920166, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 13375.177067352672, 'src_real_tokens_per_step': 1650.864, 'src_real_tokens_per_sec': 10861.09115303399, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 13375.177067352672, 'trg_real_tokens_per_step': 1688.048, 'trg_real_tokens_per_sec': 11105.72597058069, 'samples_per_step': 55.87, 'samples_per_sec': 367.57065555976084, 'this_step_loss': 352570.84375}
I1209 03:12:39.455910 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-62000	Elapsed 0.87s
I1209 03:15:13.803873 140215084762944 callbacks.py:220] Update 63000	TrainingLoss=3.15	Speed 0.154 secs/step 6.5 steps/sec
I1209 03:15:13.820586 140215084762944 callbacks.py:238] {'step': 63000, 'lr': 0.00024900597, 'loss': 3.150559425354004, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 13176.980932363773, 'src_real_tokens_per_step': 1652.912, 'src_real_tokens_per_sec': 10713.845063689309, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 13176.980932363773, 'trg_real_tokens_per_step': 1687.496, 'trg_real_tokens_per_sec': 10938.011636188407, 'samples_per_step': 55.859, 'samples_per_sec': 362.0668682982645, 'this_step_loss': 427506.125}
I1209 03:17:49.707625 140215084762944 callbacks.py:220] Update 64000	TrainingLoss=3.15	Speed 0.156 secs/step 6.4 steps/sec
I1209 03:17:49.712775 140215084762944 callbacks.py:238] {'step': 64000, 'lr': 0.00024705296, 'loss': 3.145381212234497, 'src_tokens_per_step': 2032.848, 'src_tokens_per_sec': 13046.787988464304, 'src_real_tokens_per_step': 1649.536, 'src_real_tokens_per_sec': 10586.697318904047, 'trg_tokens_per_step': 2032.848, 'trg_tokens_per_sec': 13046.787988464304, 'trg_real_tokens_per_step': 1689.584, 'trg_real_tokens_per_sec': 10843.724782522586, 'samples_per_step': 55.941, 'samples_per_sec': 359.0284993578869, 'this_step_loss': 433937.34375}
I1209 03:17:50.313091 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-64000	Elapsed 0.59s
I1209 03:20:23.346450 140215084762944 callbacks.py:220] Update 65000	TrainingLoss=3.14	Speed 0.153 secs/step 6.5 steps/sec
I1209 03:20:23.360316 140215084762944 callbacks.py:238] {'step': 65000, 'lr': 0.00024514517, 'loss': 3.1402957439422607, 'src_tokens_per_step': 2033.16, 'src_tokens_per_sec': 13291.737500659272, 'src_real_tokens_per_step': 1651.288, 'src_real_tokens_per_sec': 10795.25794034343, 'trg_tokens_per_step': 2033.16, 'trg_tokens_per_sec': 13291.737500659272, 'trg_real_tokens_per_step': 1687.432, 'trg_real_tokens_per_sec': 11031.548522601506, 'samples_per_step': 56.167, 'samples_per_sec': 367.19049174660597, 'this_step_loss': 161876.1875}
I1209 03:22:56.919561 140215084762944 callbacks.py:220] Update 66000	TrainingLoss=3.14	Speed 0.153 secs/step 6.5 steps/sec
I1209 03:22:56.926137 140215084762944 callbacks.py:238] {'step': 66000, 'lr': 0.00024328091, 'loss': 3.135477066040039, 'src_tokens_per_step': 2032.96, 'src_tokens_per_sec': 13245.467784506172, 'src_real_tokens_per_step': 1649.408, 'src_real_tokens_per_sec': 10746.488139219047, 'trg_tokens_per_step': 2032.96, 'trg_tokens_per_sec': 13245.467784506172, 'trg_real_tokens_per_step': 1689.816, 'trg_real_tokens_per_sec': 11009.76083628949, 'samples_per_step': 55.816, 'samples_per_sec': 363.66137546237826, 'this_step_loss': 394881.34375}
I1209 03:22:57.709282 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-66000	Elapsed 0.77s
I1209 03:25:32.203138 140215084762944 callbacks.py:220] Update 67000	TrainingLoss=3.13	Speed 0.154 secs/step 6.5 steps/sec
I1209 03:25:32.210724 140215084762944 callbacks.py:238] {'step': 67000, 'lr': 0.00024145858, 'loss': 3.1306750774383545, 'src_tokens_per_step': 2035.112, 'src_tokens_per_sec': 13179.340105715655, 'src_real_tokens_per_step': 1651.088, 'src_real_tokens_per_sec': 10692.409212105205, 'trg_tokens_per_step': 2035.112, 'trg_tokens_per_sec': 13179.340105715655, 'trg_real_tokens_per_step': 1690.808, 'trg_real_tokens_per_sec': 10949.63504979818, 'samples_per_step': 56.023, 'samples_per_sec': 362.80370355170044, 'this_step_loss': 434801.5}
I1209 03:28:07.138556 140215084762944 callbacks.py:220] Update 68000	TrainingLoss=3.13	Speed 0.155 secs/step 6.5 steps/sec
I1209 03:28:07.147021 140215084762944 callbacks.py:238] {'step': 68000, 'lr': 0.00023967656, 'loss': 3.1259818077087402, 'src_tokens_per_step': 2035.296, 'src_tokens_per_sec': 13143.600176981665, 'src_real_tokens_per_step': 1649.896, 'src_real_tokens_per_sec': 10654.75162217257, 'trg_tokens_per_step': 2035.296, 'trg_tokens_per_sec': 13143.600176981665, 'trg_real_tokens_per_step': 1688.68, 'trg_real_tokens_per_sec': 10905.212188726062, 'samples_per_step': 56.144, 'samples_per_sec': 362.56853466840136, 'this_step_loss': 439926.71875}
I1209 03:28:08.741425 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-68000	Elapsed 1.59s
I1209 03:30:39.139420 140215084762944 callbacks.py:220] Update 69000	TrainingLoss=3.12	Speed 0.150 secs/step 6.7 steps/sec
I1209 03:30:39.145138 140215084762944 callbacks.py:238] {'step': 69000, 'lr': 0.00023793345, 'loss': 3.1214094161987305, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 13536.75477243403, 'src_real_tokens_per_step': 1654.608, 'src_real_tokens_per_sec': 11006.702268223364, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 13536.75477243403, 'trg_real_tokens_per_step': 1689.352, 'trg_real_tokens_per_sec': 11237.824602702076, 'samples_per_step': 55.729, 'samples_per_sec': 370.71772329507644, 'this_step_loss': 694150.375}
I1209 03:33:11.438190 140215084762944 callbacks.py:220] Update 70000	TrainingLoss=3.12	Speed 0.152 secs/step 6.6 steps/sec
I1209 03:33:11.447143 140215084762944 callbacks.py:238] {'step': 70000, 'lr': 0.00023622779, 'loss': 3.116905689239502, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 13368.860028009687, 'src_real_tokens_per_step': 1652.48, 'src_real_tokens_per_sec': 10856.0365424877, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 13368.860028009687, 'trg_real_tokens_per_step': 1689.64, 'trg_real_tokens_per_sec': 11100.160718222864, 'samples_per_step': 56.095, 'samples_per_sec': 368.51845096512363, 'this_step_loss': 151675.34375}
I1209 03:33:12.257152 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-70000	Elapsed 0.80s
I1209 03:33:14.769031 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=75.60 (Best 75.60)  step=70000	Elapsed 2.51s  FromSTART 12222.00s
I1209 03:33:14.772785 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.90 (Best 6.90)  step=70000	Elapsed 2.51s  FromSTART 12222.00s
I1209 03:34:50.602625 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 03:34:50.633246 140215084762944 seq_generation_validator.py:179] Sample 1004
I1209 03:34:50.633387 140215084762944 seq_generation_validator.py:181]   Data: Similar conclusions are also reached by the joint audit from the Czech and German auditors.
I1209 03:34:50.633444 140215084762944 seq_generation_validator.py:182]   Reference: Zu einem hnlichen Ergebnis kommt anscheinend auch eine gemeinsame Kontrolle durch tschechische und deutsche Prfer.
I1209 03:34:50.633498 140215084762944 seq_generation_validator.py:183]   Hypothesis: hnliche Schlussfolgerungen werden auch durch die gemeinsame Prfung der tschechischen und deutschen Prfer erreicht.
I1209 03:34:50.633546 140215084762944 seq_generation_validator.py:179] Sample 1163
I1209 03:34:50.633592 140215084762944 seq_generation_validator.py:181]   Data: I began my job as a tutor at the Jedlicka Institute, where I was surrounded by a lot of young people, who were interested in doing something.
I1209 03:34:50.633640 140215084762944 seq_generation_validator.py:182]   Reference: Damals fing ich als Betreuer im Jedlika-Institut an, wo ich vielen jungen Leuten begegnete, die daran interessiert waren, sich mit etwas intensiv zu beschftigen.
I1209 03:34:50.633691 140215084762944 seq_generation_validator.py:183]   Hypothesis: Ich begann meine Arbeit als Tutor im Jedlicka-Institut, wo ich von vielen Jugendlichen umgeben war, die daran interessiert waren, etwas zu unternehmen.
I1209 03:34:50.633736 140215084762944 seq_generation_validator.py:179] Sample 2312
I1209 03:34:50.633780 140215084762944 seq_generation_validator.py:181]   Data: "Boredom actually has a very long history," he said.
I1209 03:34:50.633825 140215084762944 seq_generation_validator.py:182]   Reference: "Tatschlich hat die Langeweile eine sehr lange Geschichte", sagt er.
I1209 03:34:50.633871 140215084762944 seq_generation_validator.py:183]   Hypothesis: "Boreselten hat tatschlich eine sehr lange Geschichte", sagte er.
I1209 03:34:50.633916 140215084762944 seq_generation_validator.py:179] Sample 1264
I1209 03:34:50.633960 140215084762944 seq_generation_validator.py:181]   Data: The austerity measures for all the southern EU states will undoubtedly lead to the same situation, where people are pressured by a catastrophic drop in living standards to emigrate as it was in the 19th century, or to eke out an existence on starvation wages on the edge of society, in the hope that the country will eventually see some foreign investment.
I1209 03:34:50.634008 140215084762944 seq_generation_validator.py:182]   Reference: Die Sparmanahmen praktisch aller EU-Sdstaaten werden zweifellos auf den gleichen Zustand hinauslaufen, bei dem die Menschen unter dem Druck eines katastrophalen Absturzes ihres sozialen Niveaus gezwungen sind, zu emigrieren (wie es im 19. Jahrhundert der Fall war), oder fr einen Hungerlohn am Rande der Gesellschaft zu darben und dabei darauf zu hoffen, dass irgendwann privates Investitionskapital ins Land flieen wird.
I1209 03:34:50.634059 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die Sparmanahmen fr alle sdlichen EU-Staaten werden zweifellos zu einer gleichen Situation fhren, in der die Menschen durch einen katastrophalen Rckgang der Lebensstandards in den Lebensstandards, wie es im 19. Jahrhundert war, oder eine Existenz auf den Rand der Gesellschaft zu schaffen, in der Hoffnung, dass das Land schlielich einige auslndische Investitionen sehen wird.
I1209 03:34:50.634114 140215084762944 seq_generation_validator.py:179] Sample 45
I1209 03:34:50.634158 140215084762944 seq_generation_validator.py:181]   Data: For example, only 16 out of 34 States have adopted laws requiring the presentation of a photo ID card.
I1209 03:34:50.634202 140215084762944 seq_generation_validator.py:182]   Reference: Beispielsweise haben nur 16 der 34 Bundesstaaten Gesetze verabschiedet, die das Vorzeigen eines Lichtbildausweises verlangen.
I1209 03:34:50.634247 140215084762944 seq_generation_validator.py:183]   Hypothesis: Zum Beispiel haben nur 16 von 34 Staaten Gesetze angenommen, die die Vorlage einer FotoID-Karte erfordern.
I1209 03:34:51.261280 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.63s
I1209 03:34:52.657459 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.40s
I1209 03:34:52.657697 140215084762944 training_utils.py:359] Evaluating bleu at step=70000 with bad count=0 (early_stop_patience=0).
I1209 03:34:52.657773 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.21 (Best 18.21)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:34:52.659045 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.37 (Best 18.37)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:34:52.659788 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.21 (Best 18.21)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:34:52.660358 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.23 (Best 47.23)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:34:52.660865 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.75 (Best 18.75)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:34:52.661367 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.94 (Best 18.94)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:34:52.661852 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.75 (Best 18.75)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:34:52.662338 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.56 (Best 48.56)  step=70000	Elapsed 84.94s  FromSTART 12301.97s
I1209 03:37:23.411902 140215084762944 callbacks.py:220] Update 71000	TrainingLoss=3.11	Speed 0.151 secs/step 6.6 steps/sec
I1209 03:37:23.418025 140215084762944 callbacks.py:238] {'step': 71000, 'lr': 0.00023455832, 'loss': 3.1125054359436035, 'src_tokens_per_step': 2035.088, 'src_tokens_per_sec': 13506.16466180475, 'src_real_tokens_per_step': 1652.152, 'src_real_tokens_per_sec': 10964.752855075574, 'trg_tokens_per_step': 2035.088, 'trg_tokens_per_sec': 13506.16466180475, 'trg_real_tokens_per_step': 1689.968, 'trg_real_tokens_per_sec': 11215.72437220447, 'samples_per_step': 56.069, 'samples_per_sec': 372.1102706235458, 'this_step_loss': 178192.234375}
I1209 03:39:57.389316 140215084762944 callbacks.py:220] Update 72000	TrainingLoss=3.11	Speed 0.154 secs/step 6.5 steps/sec
I1209 03:39:57.399358 140215084762944 callbacks.py:238] {'step': 72000, 'lr': 0.00023292375, 'loss': 3.1082255840301514, 'src_tokens_per_step': 2034.96, 'src_tokens_per_sec': 13222.981192445839, 'src_real_tokens_per_step': 1652.064, 'src_real_tokens_per_sec': 10734.958525335556, 'trg_tokens_per_step': 2034.96, 'trg_tokens_per_sec': 13222.981192445839, 'trg_real_tokens_per_step': 1690.616, 'trg_real_tokens_per_sec': 10985.465842890286, 'samples_per_step': 55.763, 'samples_per_sec': 362.34279800799885, 'this_step_loss': 474794.78125}
I1209 03:39:58.154942 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-72000	Elapsed 0.75s
I1209 03:42:32.354406 140215084762944 callbacks.py:220] Update 73000	TrainingLoss=3.10	Speed 0.154 secs/step 6.5 steps/sec
I1209 03:42:32.359977 140215084762944 callbacks.py:238] {'step': 73000, 'lr': 0.00023132288, 'loss': 3.1039791107177734, 'src_tokens_per_step': 2035.072, 'src_tokens_per_sec': 13203.576555415446, 'src_real_tokens_per_step': 1654.312, 'src_real_tokens_per_sec': 10733.200171071312, 'trg_tokens_per_step': 2035.072, 'trg_tokens_per_sec': 13203.576555415446, 'trg_real_tokens_per_step': 1688.072, 'trg_real_tokens_per_sec': 10952.235539112751, 'samples_per_step': 56.046, 'samples_per_sec': 363.6272582123945, 'this_step_loss': 210494.21875}
I1209 03:45:02.772543 140215084762944 callbacks.py:220] Update 74000	TrainingLoss=3.10	Speed 0.150 secs/step 6.7 steps/sec
I1209 03:45:02.777482 140215084762944 callbacks.py:238] {'step': 74000, 'lr': 0.00022975456, 'loss': 3.099946975708008, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 13536.048912868979, 'src_real_tokens_per_step': 1651.84, 'src_real_tokens_per_sec': 10987.543359839867, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 13536.048912868979, 'trg_real_tokens_per_step': 1687.52, 'trg_real_tokens_per_sec': 11224.87599924749, 'samples_per_step': 55.913, 'samples_per_sec': 371.9164760986091, 'this_step_loss': 138412.140625}
I1209 03:45:03.508866 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-74000	Elapsed 0.73s
I1209 03:47:36.157002 140215084762944 callbacks.py:220] Update 75000	TrainingLoss=3.10	Speed 0.153 secs/step 6.6 steps/sec
I1209 03:47:36.161880 140215084762944 callbacks.py:238] {'step': 75000, 'lr': 0.00022821774, 'loss': 3.096101760864258, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 13337.721288711018, 'src_real_tokens_per_step': 1649.336, 'src_real_tokens_per_sec': 10809.804170648968, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 13337.721288711018, 'trg_real_tokens_per_step': 1690.592, 'trg_real_tokens_per_sec': 11080.197396082896, 'samples_per_step': 55.968, 'samples_per_sec': 366.81617318901755, 'this_step_loss': 377034.8125}
I1209 03:50:09.904428 140215084762944 callbacks.py:220] Update 76000	TrainingLoss=3.09	Speed 0.154 secs/step 6.5 steps/sec
I1209 03:50:09.913020 140215084762944 callbacks.py:238] {'step': 76000, 'lr': 0.00022671133, 'loss': 3.092237710952759, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 13242.409165704154, 'src_real_tokens_per_step': 1651.84, 'src_real_tokens_per_sec': 10749.357798679841, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 13242.409165704154, 'trg_real_tokens_per_step': 1689.784, 'trg_real_tokens_per_sec': 10996.278585386246, 'samples_per_step': 56.025, 'samples_per_sec': 364.58299270573303, 'this_step_loss': 332075.90625}
I1209 03:50:11.460268 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-76000	Elapsed 1.54s
I1209 03:52:44.560201 140215084762944 callbacks.py:220] Update 77000	TrainingLoss=3.09	Speed 0.153 secs/step 6.5 steps/sec
I1209 03:52:44.565952 140215084762944 callbacks.py:238] {'step': 77000, 'lr': 0.00022523437, 'loss': 3.088487386703491, 'src_tokens_per_step': 2035.168, 'src_tokens_per_sec': 13298.991630385395, 'src_real_tokens_per_step': 1652.072, 'src_real_tokens_per_sec': 10795.615743169144, 'trg_tokens_per_step': 2035.168, 'trg_tokens_per_sec': 13298.991630385395, 'trg_real_tokens_per_step': 1686.464, 'trg_real_tokens_per_sec': 11020.353416006086, 'samples_per_step': 55.734, 'samples_per_sec': 364.19892585177223, 'this_step_loss': 361440.1875}
I1209 03:55:19.506877 140215084762944 callbacks.py:220] Update 78000	TrainingLoss=3.08	Speed 0.155 secs/step 6.5 steps/sec
I1209 03:55:19.589714 140215084762944 callbacks.py:238] {'step': 78000, 'lr': 0.0002237859, 'loss': 3.084634780883789, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 13140.389118556588, 'src_real_tokens_per_step': 1652.272, 'src_real_tokens_per_sec': 10668.998357596496, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 13140.389118556588, 'trg_real_tokens_per_step': 1688.928, 'trg_real_tokens_per_sec': 10905.692318273708, 'samples_per_step': 55.921, 'samples_per_sec': 361.0913076994307, 'this_step_loss': 389779.71875}
I1209 03:55:20.501959 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-78000	Elapsed 0.91s
I1209 03:57:50.011588 140215084762944 callbacks.py:220] Update 79000	TrainingLoss=3.08	Speed 0.149 secs/step 6.7 steps/sec
I1209 03:57:50.019886 140215084762944 callbacks.py:238] {'step': 79000, 'lr': 0.00022236501, 'loss': 3.080833911895752, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 13617.757718986668, 'src_real_tokens_per_step': 1652.536, 'src_real_tokens_per_sec': 11058.178153698873, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 13617.757718986668, 'trg_real_tokens_per_step': 1691.272, 'trg_real_tokens_per_sec': 11317.385571244802, 'samples_per_step': 55.997, 'samples_per_sec': 374.71183809168195, 'this_step_loss': 207212.78125}
I1209 04:00:22.310391 140215084762944 callbacks.py:220] Update 80000	TrainingLoss=3.08	Speed 0.152 secs/step 6.6 steps/sec
I1209 04:00:22.319334 140215084762944 callbacks.py:238] {'step': 80000, 'lr': 0.00022097088, 'loss': 3.077260971069336, 'src_tokens_per_step': 2035.2, 'src_tokens_per_sec': 13370.621574076291, 'src_real_tokens_per_step': 1651.216, 'src_real_tokens_per_sec': 10847.967901464208, 'trg_tokens_per_step': 2035.2, 'trg_tokens_per_sec': 13370.621574076291, 'trg_real_tokens_per_step': 1686.704, 'trg_real_tokens_per_sec': 11081.112859414689, 'samples_per_step': 56.071, 'samples_per_sec': 368.36877077438663, 'this_step_loss': 178333.953125}
I1209 04:00:23.161189 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-80000	Elapsed 0.84s
I1209 04:00:25.715557 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=74.85 (Best 74.85)  step=80000	Elapsed 2.55s  FromSTART 13852.95s
I1209 04:00:25.717184 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.77 (Best 6.77)  step=80000	Elapsed 2.55s  FromSTART 13852.95s
I1209 04:02:09.013573 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 04:02:09.019184 140215084762944 seq_generation_validator.py:179] Sample 818
I1209 04:02:09.019326 140215084762944 seq_generation_validator.py:181]   Data: In my work, it would be very useful to have a device that does not require touching and would have more images per second.
I1209 04:02:09.019386 140215084762944 seq_generation_validator.py:182]   Reference: Fr meine Arbeit brauchte ich genau so eine Vorrichtung, nur ohne Berhrungssteuerung und mit mehr Bildern pro Sekunde.
I1209 04:02:09.019439 140215084762944 seq_generation_validator.py:183]   Hypothesis: In meiner Arbeit wre es sehr sinnvoll, ein Gert zu haben, das nicht mehr aufkommt und mehr Bilder pro Sekunde htte.
I1209 04:02:09.019488 140215084762944 seq_generation_validator.py:179] Sample 2269
I1209 04:02:09.019535 140215084762944 seq_generation_validator.py:181]   Data: Tobacco firms said they would boost black market trade, leading to cheaper, more accessible cigarettes.
I1209 04:02:09.019582 140215084762944 seq_generation_validator.py:182]   Reference: So hie es, mit den Gesetzen wrde der Schwarzmarkthandel begnstigt, was zu billigeren und leichter zugnglichen Zigaretten fhre.
I1209 04:02:09.019630 140215084762944 seq_generation_validator.py:183]   Hypothesis: Tabakunternehmen sagten, sie wrden den Schwarzmarkt frdern, was zu billigeren, zugnglichen Zigaretten fhrt.
I1209 04:02:09.019676 140215084762944 seq_generation_validator.py:179] Sample 502
I1209 04:02:09.019722 140215084762944 seq_generation_validator.py:181]   Data: AiF [Argumenti i Fakti] newspaper highlighted the five most important reasons why it is a must to visit Israel.
I1209 04:02:09.019769 140215084762944 seq_generation_validator.py:182]   Reference: AIF nennt Ihnen die fnf wichtigsten Grnde fr einen Besuch in Israel.
I1209 04:02:09.019815 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die beiden wichtigsten Grnde, warum es sich um einen Besuch Israels handelt, hat die Zeitung zu den fnf wichtigsten Grnden hervorgehoben.
I1209 04:02:09.019869 140215084762944 seq_generation_validator.py:179] Sample 1434
I1209 04:02:09.019915 140215084762944 seq_generation_validator.py:181]   Data: As an independent non-party man, I stand a far better chance of gaining support from all parliamentary sides.
I1209 04:02:09.019961 140215084762944 seq_generation_validator.py:182]   Reference: Als unabhngiger Parteiloser verfge ich ber ein weitaus greres Potenzial, Untersttzung bei allen im Parlament vertretenen Parteien zu gewinnen.
I1209 04:02:09.020008 140215084762944 seq_generation_validator.py:183]   Hypothesis: Als unabhngige nicht-Partei-Mann bin ich eine weit bessere Chance, von allen parlamentarischen Seiten Untersttzung zu gewinnen.
I1209 04:02:09.020054 140215084762944 seq_generation_validator.py:179] Sample 1439
I1209 04:02:09.020100 140215084762944 seq_generation_validator.py:181]   Data: The important part for me is moving forward.
I1209 04:02:09.020146 140215084762944 seq_generation_validator.py:182]   Reference: Fr mich ist wichtig, dass wir weiterkommen.
I1209 04:02:09.020193 140215084762944 seq_generation_validator.py:183]   Hypothesis: Der wichtige Teil fr mich ist nach vorn.
I1209 04:02:10.008880 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.99s
I1209 04:02:11.363450 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.35s
I1209 04:02:11.363703 140215084762944 training_utils.py:359] Evaluating bleu at step=80000 with bad count=0 (early_stop_patience=0).
I1209 04:02:11.363787 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.39 (Best 18.39)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:02:11.372825 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.50 (Best 18.50)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:02:11.373699 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.39 (Best 18.39)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:02:11.374266 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.46 (Best 47.46)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:02:11.374784 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.90 (Best 18.90)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:02:11.375263 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=19.02 (Best 19.02)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:02:11.375753 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.90 (Best 18.90)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:02:11.376216 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.80 (Best 48.80)  step=80000	Elapsed 92.47s  FromSTART 13940.45s
I1209 04:04:42.608882 140215084762944 callbacks.py:220] Update 81000	TrainingLoss=3.07	Speed 0.151 secs/step 6.6 steps/sec
I1209 04:04:42.615076 140215084762944 callbacks.py:238] {'step': 81000, 'lr': 0.00021960262, 'loss': 3.0738260746002197, 'src_tokens_per_step': 2034.88, 'src_tokens_per_sec': 13461.794276170851, 'src_real_tokens_per_step': 1654.0, 'src_real_tokens_per_sec': 10942.07409419061, 'trg_tokens_per_step': 2034.88, 'trg_tokens_per_sec': 13461.794276170851, 'trg_real_tokens_per_step': 1688.608, 'trg_real_tokens_per_sec': 11171.024094342814, 'samples_per_step': 55.682, 'samples_per_sec': 368.3655197779453, 'this_step_loss': 266654.5}
I1209 04:07:13.664341 140215084762944 callbacks.py:220] Update 82000	TrainingLoss=3.07	Speed 0.151 secs/step 6.6 steps/sec
I1209 04:07:13.674097 140215084762944 callbacks.py:238] {'step': 82000, 'lr': 0.00021825948, 'loss': 3.07030987739563, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 13479.599955270018, 'src_real_tokens_per_step': 1654.272, 'src_real_tokens_per_sec': 10957.487212636823, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 13479.599955270018, 'trg_real_tokens_per_step': 1689.04, 'trg_real_tokens_per_sec': 11187.781816794397, 'samples_per_step': 55.955, 'samples_per_sec': 370.63203450405587, 'this_step_loss': 192914.421875}
I1209 04:07:14.508610 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-82000	Elapsed 0.83s
I1209 04:09:47.369829 140215084762944 callbacks.py:220] Update 83000	TrainingLoss=3.07	Speed 0.153 secs/step 6.5 steps/sec
I1209 04:09:47.378049 140215084762944 callbacks.py:238] {'step': 83000, 'lr': 0.00021694068, 'loss': 3.0669667720794678, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 13318.389949906303, 'src_real_tokens_per_step': 1651.776, 'src_real_tokens_per_sec': 10810.615367251597, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 13318.389949906303, 'trg_real_tokens_per_step': 1689.264, 'trg_real_tokens_per_sec': 11055.968459249258, 'samples_per_step': 55.857, 'samples_per_sec': 365.57532169529793, 'this_step_loss': 178237.203125}
I1209 04:12:18.934776 140215084762944 callbacks.py:220] Update 84000	TrainingLoss=3.06	Speed 0.151 secs/step 6.6 steps/sec
I1209 04:12:18.940735 140215084762944 callbacks.py:238] {'step': 84000, 'lr': 0.00021564549, 'loss': 3.0637807846069336, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 13433.905859989532, 'src_real_tokens_per_step': 1653.824, 'src_real_tokens_per_sec': 10917.557043997533, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 13433.905859989532, 'trg_real_tokens_per_step': 1687.168, 'trg_real_tokens_per_sec': 11137.674191937733, 'samples_per_step': 55.619, 'samples_per_sec': 367.1633772578574, 'this_step_loss': 361672.53125}
I1209 04:12:19.646452 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-84000	Elapsed 0.70s
I1209 04:14:51.614992 140215084762944 callbacks.py:220] Update 85000	TrainingLoss=3.06	Speed 0.152 secs/step 6.6 steps/sec
I1209 04:14:51.621177 140215084762944 callbacks.py:238] {'step': 85000, 'lr': 0.00021437323, 'loss': 3.0605289936065674, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 13397.045818980801, 'src_real_tokens_per_step': 1650.272, 'src_real_tokens_per_sec': 10864.217535155185, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 13397.045818980801, 'trg_real_tokens_per_step': 1690.768, 'trg_real_tokens_per_sec': 11130.814407248783, 'samples_per_step': 55.989, 'samples_per_sec': 368.5917688573785, 'this_step_loss': 377236.625}
I1209 04:17:23.689950 140215084762944 callbacks.py:220] Update 86000	TrainingLoss=3.06	Speed 0.152 secs/step 6.6 steps/sec
I1209 04:17:23.699866 140215084762944 callbacks.py:238] {'step': 86000, 'lr': 0.00021312323, 'loss': 3.0573184490203857, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 13388.965003965039, 'src_real_tokens_per_step': 1650.32, 'src_real_tokens_per_sec': 10857.809539539066, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 13388.965003965039, 'trg_real_tokens_per_step': 1689.072, 'trg_real_tokens_per_sec': 11112.767266086777, 'samples_per_step': 55.921, 'samples_per_sec': 367.91626306447483, 'this_step_loss': 417810.59375}
I1209 04:17:24.454638 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-86000	Elapsed 0.75s
I1209 04:19:55.832273 140215084762944 callbacks.py:220] Update 87000	TrainingLoss=3.05	Speed 0.151 secs/step 6.6 steps/sec
I1209 04:19:55.853543 140215084762944 callbacks.py:238] {'step': 87000, 'lr': 0.00021189486, 'loss': 3.0540924072265625, 'src_tokens_per_step': 2034.912, 'src_tokens_per_sec': 13448.659862613318, 'src_real_tokens_per_step': 1652.656, 'src_real_tokens_per_sec': 10922.343774034001, 'trg_tokens_per_step': 2034.912, 'trg_tokens_per_sec': 13448.659862613318, 'trg_real_tokens_per_step': 1688.8, 'trg_real_tokens_per_sec': 11161.218163724709, 'samples_per_step': 55.861, 'samples_per_sec': 369.18333008279603, 'this_step_loss': 163203.21875}
I1209 04:22:30.295238 140215084762944 callbacks.py:220] Update 88000	TrainingLoss=3.05	Speed 0.154 secs/step 6.5 steps/sec
I1209 04:22:30.305049 140215084762944 callbacks.py:238] {'step': 88000, 'lr': 0.00021068745, 'loss': 3.0509378910064697, 'src_tokens_per_step': 2035.104, 'src_tokens_per_sec': 13183.47057738757, 'src_real_tokens_per_step': 1649.056, 'src_real_tokens_per_sec': 10682.638949392482, 'trg_tokens_per_step': 2035.104, 'trg_tokens_per_sec': 13183.47057738757, 'trg_real_tokens_per_step': 1689.328, 'trg_real_tokens_per_sec': 10943.522288569522, 'samples_per_step': 55.845, 'samples_per_sec': 361.76574484360935, 'this_step_loss': 188412.671875}
I1209 04:22:31.358634 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-88000	Elapsed 1.05s
I1209 04:25:04.048791 140215084762944 callbacks.py:220] Update 89000	TrainingLoss=3.05	Speed 0.153 secs/step 6.6 steps/sec
I1209 04:25:04.740299 140215084762944 callbacks.py:238] {'step': 89000, 'lr': 0.00020950047, 'loss': 3.0479679107666016, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 13333.188031321803, 'src_real_tokens_per_step': 1650.96, 'src_real_tokens_per_sec': 10817.280530663766, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 13333.188031321803, 'trg_real_tokens_per_step': 1690.88, 'trg_real_tokens_per_sec': 11078.840979605046, 'samples_per_step': 55.791, 'samples_per_sec': 365.54907331871283, 'this_step_loss': 306714.75}
I1209 04:27:37.565608 140215084762944 callbacks.py:220] Update 90000	TrainingLoss=3.04	Speed 0.153 secs/step 6.5 steps/sec
I1209 04:27:37.570374 140215084762944 callbacks.py:238] {'step': 90000, 'lr': 0.00020833334, 'loss': 3.044931650161743, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 13322.041610801924, 'src_real_tokens_per_step': 1652.016, 'src_real_tokens_per_sec': 10815.150634961232, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 13322.041610801924, 'trg_real_tokens_per_step': 1689.632, 'trg_real_tokens_per_sec': 11061.408967982645, 'samples_per_step': 55.846, 'samples_per_sec': 365.6035427986442, 'this_step_loss': 209743.828125}
I1209 04:27:38.333815 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-90000	Elapsed 0.76s
I1209 04:27:40.765416 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=74.45 (Best 74.45)  step=90000	Elapsed 2.43s  FromSTART 15488.00s
I1209 04:27:40.771538 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.70 (Best 6.70)  step=90000	Elapsed 2.43s  FromSTART 15488.00s
I1209 04:29:21.626237 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 04:29:21.926622 140215084762944 seq_generation_validator.py:179] Sample 1686
I1209 04:29:21.926804 140215084762944 seq_generation_validator.py:181]   Data: Wentzler is confident that the matter will be brought to a positive conclusion and praises the professionalism of the courts.
I1209 04:29:21.926872 140215084762944 seq_generation_validator.py:182]   Reference: Wentzler ist zuversichtlich, die Sache zu einem guten Abschluss zu bringen, und lobt die Professionalitt der Gerichte.
I1209 04:29:21.926928 140215084762944 seq_generation_validator.py:183]   Hypothesis: W3ler ist zuversichtlich, dass die Angelegenheit zu einem positiven Abschluss gebracht und die Professionalitt der Gerichte lobt.
I1209 04:29:21.926975 140215084762944 seq_generation_validator.py:179] Sample 676
I1209 04:29:21.927021 140215084762944 seq_generation_validator.py:181]   Data: One piece, for example a TT gun can be bought from a warrant officer.
I1209 04:29:21.927066 140215084762944 seq_generation_validator.py:182]   Reference: ber einen Fhnrich wird eine Waffe, zum Beispiel eine Pistole vom Typ TT, gekauft.
I1209 04:29:21.927111 140215084762944 seq_generation_validator.py:183]   Hypothesis: Ein Stck, zum Beispiel kann man aus einem fehlender Offizier kaufen.
I1209 04:29:21.927156 140215084762944 seq_generation_validator.py:179] Sample 2712
I1209 04:29:21.927201 140215084762944 seq_generation_validator.py:181]   Data: The ASPA official website confirms that the document was published last Tuesday.
I1209 04:29:21.927252 140215084762944 seq_generation_validator.py:182]   Reference: Auf der offiziellen Webseite des ASPA kann man sich davon berzeugen, dass das Dokument am vergangenen Dienstag verffentlicht wurde.
I1209 04:29:21.927309 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die ASPA-offizielle Website besttigt, dass das Dokument am letzten Dienstag verffentlicht wurde.
I1209 04:29:21.927354 140215084762944 seq_generation_validator.py:179] Sample 2632
I1209 04:29:21.927398 140215084762944 seq_generation_validator.py:181]   Data: However, he admits that the new regulations give clearer guidance to developers, who previously had obstacles put in their way if they provided for too many small units in a project.
I1209 04:29:21.927443 140215084762944 seq_generation_validator.py:182]   Reference: Er gibt allerdings zu, dass die neuen Normen den Entwicklern klarere Orientierungen geben, denen vorher Hindernisse in den Weg gelegt wurden, wenn sie zu viele kleine Wohneinheiten innerhalb eines Projekts bauen wollten.
I1209 04:29:21.927488 140215084762944 seq_generation_validator.py:183]   Hypothesis: Er weist jedoch fest, dass die neuen Regelungen den Entwickler klarere Ratschlge geben, die bisher Hindernisse auf ihre Art und Weise htten, wenn sie zu viele kleine Einheiten in einem Projekt bereitgestellt haben.
I1209 04:29:21.927535 140215084762944 seq_generation_validator.py:179] Sample 1196
I1209 04:29:21.927579 140215084762944 seq_generation_validator.py:181]   Data: Poland began close cooperation with the ESA in 1994, and in the following years it has participated in a series of agency projects.
I1209 04:29:21.927624 140215084762944 seq_generation_validator.py:182]   Reference: Polen begann im Jahr 1994 eine enge Zusammenarbeit mit der ESA und beteiligte sich in den folgenden Jahren an mehreren ESA-Projekten.
I1209 04:29:21.927668 140215084762944 seq_generation_validator.py:183]   Hypothesis: Polen hat 1994 enge Zusammenarbeit mit der ESA begonnen, und in den folgenden Jahren hat sie an einer Reihe von Agentur-Projekten teilgenommen.
I1209 04:29:23.107919 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.18s
I1209 04:29:24.715342 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.61s
I1209 04:29:24.716566 140215084762944 training_utils.py:359] Evaluating bleu at step=90000 with bad count=0 (early_stop_patience=0).
I1209 04:29:24.716660 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.45 (Best 18.45)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:29:24.717928 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.63 (Best 18.63)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:29:24.718515 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.45 (Best 18.45)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:29:24.718997 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.82 (Best 47.82)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:29:24.719472 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.93 (Best 18.93)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:29:24.719936 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=19.11 (Best 19.11)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:29:24.720405 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.93 (Best 18.93)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:29:24.720860 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=49.15 (Best 49.15)  step=90000	Elapsed 90.19s  FromSTART 15573.22s
I1209 04:31:56.234496 140215084762944 callbacks.py:220] Update 91000	TrainingLoss=3.04	Speed 0.151 secs/step 6.6 steps/sec
I1209 04:31:56.240634 140215084762944 callbacks.py:238] {'step': 91000, 'lr': 0.0002071855, 'loss': 3.042025327682495, 'src_tokens_per_step': 2034.96, 'src_tokens_per_sec': 13436.930092416897, 'src_real_tokens_per_step': 1650.464, 'src_real_tokens_per_sec': 10898.08614815562, 'trg_tokens_per_step': 2034.96, 'trg_tokens_per_sec': 13436.930092416897, 'trg_real_tokens_per_step': 1684.64, 'trg_real_tokens_per_sec': 11123.751774427605, 'samples_per_step': 56.029, 'samples_per_sec': 369.961943305041, 'this_step_loss': 206666.546875}
I1209 04:34:30.515254 140215084762944 callbacks.py:220] Update 92000	TrainingLoss=3.04	Speed 0.154 secs/step 6.5 steps/sec
I1209 04:34:30.562550 140215084762944 callbacks.py:238] {'step': 92000, 'lr': 0.00020605639, 'loss': 3.0392332077026367, 'src_tokens_per_step': 2034.768, 'src_tokens_per_sec': 13195.681702508962, 'src_real_tokens_per_step': 1650.4, 'src_real_tokens_per_sec': 10703.015322543302, 'trg_tokens_per_step': 2034.768, 'trg_tokens_per_sec': 13195.681702508962, 'trg_real_tokens_per_step': 1689.952, 'trg_real_tokens_per_sec': 10959.514148305077, 'samples_per_step': 55.752, 'samples_per_sec': 361.5575074299771, 'this_step_loss': 176196.921875}
I1209 04:34:31.192233 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-92000	Elapsed 0.62s
I1209 04:37:05.021118 140215084762944 callbacks.py:220] Update 93000	TrainingLoss=3.04	Speed 0.154 secs/step 6.5 steps/sec
I1209 04:37:05.026711 140215084762944 callbacks.py:238] {'step': 93000, 'lr': 0.00020494558, 'loss': 3.0363595485687256, 'src_tokens_per_step': 2035.168, 'src_tokens_per_sec': 13236.099126547517, 'src_real_tokens_per_step': 1649.312, 'src_real_tokens_per_sec': 10726.611819075544, 'trg_tokens_per_step': 2035.168, 'trg_tokens_per_sec': 13236.099126547517, 'trg_real_tokens_per_step': 1690.992, 'trg_real_tokens_per_sec': 10997.685564139589, 'samples_per_step': 56.106, 'samples_per_sec': 364.8959582668728, 'this_step_loss': 250354.671875}
I1209 04:39:40.186393 140215084762944 callbacks.py:220] Update 94000	TrainingLoss=3.03	Speed 0.155 secs/step 6.4 steps/sec
I1209 04:39:40.191856 140215084762944 callbacks.py:238] {'step': 94000, 'lr': 0.00020385251, 'loss': 3.033689022064209, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 13121.719660324312, 'src_real_tokens_per_step': 1649.824, 'src_real_tokens_per_sec': 10638.222768659138, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 13121.719660324312, 'trg_real_tokens_per_step': 1687.376, 'trg_real_tokens_per_sec': 10880.36165220592, 'samples_per_step': 55.759, 'samples_per_sec': 359.53935896051024, 'this_step_loss': 361465.8125}
I1209 04:39:40.942084 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-94000	Elapsed 0.73s
I1209 04:42:16.041204 140215084762944 callbacks.py:220] Update 95000	TrainingLoss=3.03	Speed 0.155 secs/step 6.5 steps/sec
I1209 04:42:16.050976 140215084762944 callbacks.py:238] {'step': 95000, 'lr': 0.00020277678, 'loss': 3.031020402908325, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 13126.795509646661, 'src_real_tokens_per_step': 1652.176, 'src_real_tokens_per_sec': 10657.174550842234, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 13126.795509646661, 'trg_real_tokens_per_step': 1687.488, 'trg_real_tokens_per_sec': 10884.950615704174, 'samples_per_step': 55.976, 'samples_per_sec': 361.0668613137734, 'this_step_loss': 328057.5}
I1209 04:44:50.000258 140215084762944 callbacks.py:220] Update 96000	TrainingLoss=3.03	Speed 0.154 secs/step 6.5 steps/sec
I1209 04:44:50.006239 140215084762944 callbacks.py:238] {'step': 96000, 'lr': 0.00020171789, 'loss': 3.02826189994812, 'src_tokens_per_step': 2035.232, 'src_tokens_per_sec': 13226.45127797373, 'src_real_tokens_per_step': 1652.688, 'src_real_tokens_per_sec': 10740.395841698562, 'trg_tokens_per_step': 2035.232, 'trg_tokens_per_sec': 13226.45127797373, 'trg_real_tokens_per_step': 1688.912, 'trg_real_tokens_per_sec': 10975.806335977995, 'samples_per_step': 55.985, 'samples_per_sec': 363.8321698938299, 'this_step_loss': 208121.703125}
I1209 04:44:50.691763 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-96000	Elapsed 0.68s
I1209 04:47:23.613911 140215084762944 callbacks.py:220] Update 97000	TrainingLoss=3.03	Speed 0.153 secs/step 6.5 steps/sec
I1209 04:47:23.619085 140215084762944 callbacks.py:238] {'step': 97000, 'lr': 0.0002006754, 'loss': 3.0255842208862305, 'src_tokens_per_step': 2035.072, 'src_tokens_per_sec': 13314.008300685871, 'src_real_tokens_per_step': 1653.472, 'src_real_tokens_per_sec': 10817.474729617266, 'trg_tokens_per_step': 2035.072, 'trg_tokens_per_sec': 13314.008300685871, 'trg_real_tokens_per_step': 1686.736, 'trg_real_tokens_per_sec': 11035.097089963247, 'samples_per_step': 55.899, 'samples_per_sec': 365.70683985629967, 'this_step_loss': 210146.0}
I1209 04:49:55.317405 140215084762944 callbacks.py:220] Update 98000	TrainingLoss=3.02	Speed 0.152 secs/step 6.6 steps/sec
I1209 04:49:55.336718 140215084762944 callbacks.py:238] {'step': 98000, 'lr': 0.00019964892, 'loss': 3.023081064224243, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 13421.496261544304, 'src_real_tokens_per_step': 1650.608, 'src_real_tokens_per_sec': 10886.26143055709, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 13421.496261544304, 'trg_real_tokens_per_step': 1690.8, 'trg_real_tokens_per_sec': 11151.339886142516, 'samples_per_step': 55.916, 'samples_per_sec': 368.7830145928229, 'this_step_loss': 448912.1875}
I1209 04:49:56.065820 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-98000	Elapsed 0.72s
I1209 04:52:28.062320 140215084762944 callbacks.py:220] Update 99000	TrainingLoss=3.02	Speed 0.152 secs/step 6.6 steps/sec
I1209 04:52:28.067921 140215084762944 callbacks.py:238] {'step': 99000, 'lr': 0.00019863802, 'loss': 3.020521879196167, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 13394.482161316795, 'src_real_tokens_per_step': 1652.56, 'src_real_tokens_per_sec': 10877.369286176192, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 13394.482161316795, 'trg_real_tokens_per_step': 1687.664, 'trg_real_tokens_per_sec': 11108.428473995049, 'samples_per_step': 56.063, 'samples_per_sec': 369.0141079845185, 'this_step_loss': 199056.296875}
I1209 04:55:02.539810 140215084762944 callbacks.py:220] Update 100000	TrainingLoss=3.02	Speed 0.154 secs/step 6.5 steps/sec
I1209 04:55:02.545315 140215084762944 callbacks.py:238] {'step': 100000, 'lr': 0.00019764237, 'loss': 3.017942190170288, 'src_tokens_per_step': 2034.912, 'src_tokens_per_sec': 13179.773415632882, 'src_real_tokens_per_step': 1651.008, 'src_real_tokens_per_sec': 10693.293541635812, 'trg_tokens_per_step': 2034.912, 'trg_tokens_per_sec': 13179.773415632882, 'trg_real_tokens_per_step': 1688.736, 'trg_real_tokens_per_sec': 10937.651278690288, 'samples_per_step': 55.913, 'samples_per_sec': 362.13883990476313, 'this_step_loss': 92777.328125}
I1209 04:55:03.364555 140215084762944 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_lg3-1208/ckpt-100000	Elapsed 0.81s
I1209 04:55:07.587756 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=74.09 (Best 74.09)  step=100000	Elapsed 4.22s  FromSTART 17134.82s
I1209 04:55:07.589056 140215084762944 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.64 (Best 6.64)  step=100000	Elapsed 4.22s  FromSTART 17134.82s
I1209 04:56:50.416374 140215084762944 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1209 04:56:50.429923 140215084762944 seq_generation_validator.py:179] Sample 1940
I1209 04:56:50.430018 140215084762944 seq_generation_validator.py:181]   Data: The future constitution based on Sharia law is fiercely disputed.
I1209 04:56:50.430071 140215084762944 seq_generation_validator.py:182]   Reference: Die knftige Verfassung mit der Scharia als Basis ist heftig umstritten.
I1209 04:56:50.430127 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die zuknftige Verfassung, die auf dem Scharia beruht, ist sehr umstrittenen.
I1209 04:56:50.430181 140215084762944 seq_generation_validator.py:179] Sample 664
I1209 04:56:50.430227 140215084762944 seq_generation_validator.py:181]   Data: Another source of the "black" market is trafficking.
I1209 04:56:50.430272 140215084762944 seq_generation_validator.py:182]   Reference: Ein weiterer Kanal des Schwarzmarkts ist der Schmuggel.
I1209 04:56:50.430326 140215084762944 seq_generation_validator.py:183]   Hypothesis: Eine weitere Quelle des "schwarzen" Marktes ist der Handel.
I1209 04:56:50.430370 140215084762944 seq_generation_validator.py:179] Sample 136
I1209 04:56:50.430415 140215084762944 seq_generation_validator.py:181]   Data: The repercussions of this research on the daily life of the man in the street are more difficult to predict, but it would be wrong to assume that there won't be any.
I1209 04:56:50.430460 140215084762944 seq_generation_validator.py:182]   Reference: Die Auswirkungen dieser Forschungen fr die Allgemeinbevlkerung sind schwieriger vorherzusagen, aber es wre falsch anzunehmen, dass es keine gbe.
I1209 04:56:50.430506 140215084762944 seq_generation_validator.py:183]   Hypothesis: Die Auswirkungen dieser Forschung auf das tgige Leben des Menschen in der Strae sind schwieriger zu vorhersagen, aber es wre falsch, davon auszugehen, dass es keinen Platz gibt.
I1209 04:56:50.430553 140215084762944 seq_generation_validator.py:179] Sample 395
I1209 04:56:50.430597 140215084762944 seq_generation_validator.py:181]   Data: There is a general movement to reappraise hierarchical systems for more horizontal systems.
I1209 04:56:50.430642 140215084762944 seq_generation_validator.py:182]   Reference: Es ist ein allgemeiner Trend zu verzeichnen, bei dem hierarchische Systeme zugunsten strker horizontal ausgerichteter Systeme berdacht werden.
I1209 04:56:50.430686 140215084762944 seq_generation_validator.py:183]   Hypothesis: Es gibt eine allgemeine Bewegung, die hierarchische Systeme fr mehr horizontale Systeme neu aufzuwerten.
I1209 04:56:50.430730 140215084762944 seq_generation_validator.py:179] Sample 604
I1209 04:56:50.430774 140215084762944 seq_generation_validator.py:181]   Data: Many speak about protection of the rights of work migrants, explains the Head of the representative office of the Federal Migration Services of Russia, Viktor Sebelev.
I1209 04:56:50.430818 140215084762944 seq_generation_validator.py:182]   Reference: "Viele reden jetzt ber den Schutz der Rechte von Arbeitsmigranten", sagt der Leiter der Vertretung der Fderalen Migrationsbehrde in Tadschikistan, Viktor Sebelew.
I1209 04:56:50.430863 140215084762944 seq_generation_validator.py:183]   Hypothesis: Viele sprechen ber den Schutz der Rechte von Migranten, erklrt den Leiter des reprsentativen Bros der Bundesrepublik Migration Services von Russland, Viktor Moselev.
I1209 04:56:51.325028 140215084762944 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.89s
I1209 04:56:53.085436 140215084762944 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.76s
I1209 04:56:53.085856 140215084762944 training_utils.py:359] Evaluating bleu at step=100000 with bad count=1 (early_stop_patience=0).
I1209 04:56:53.085955 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.30 (Best 18.45)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.088152 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.44 (Best 18.63)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.088827 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.30 (Best 18.45)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.089400 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.56 (Best 47.82)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.089946 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.80 (Best 18.93)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.090491 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.95 (Best 19.11)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.091013 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.80 (Best 18.93)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.091521 140215084762944 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.85 (Best 49.15)  step=100000	Elapsed 92.00s  FromSTART 17221.85s
I1209 04:56:53.109413 140215084762944 trainer.py:315] {'loss': [3.017942190170288], 'src_tokens': [203368768.0], 'src_real_tokens': [165164288.0], 'trg_tokens': [203368768.0], 'trg_real_tokens': [168868384.0], 'samples': [5591388.0], 'this_step_loss': [92777.328125]}
