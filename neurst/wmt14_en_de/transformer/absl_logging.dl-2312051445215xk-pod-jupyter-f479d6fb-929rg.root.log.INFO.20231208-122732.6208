I1208 12:27:32.124338 140042889676608 configurable.py:222] loading configurations from wmt14_en_de/training_args.yml
I1208 12:27:32.131168 140042889676608 configurable.py:222] loading configurations from wmt14_en_de/translation_bpe.yml
I1208 12:27:32.143639 140042889676608 configurable.py:222] loading configurations from wmt14_en_de/validation_args.yml
I1208 12:27:38.718523 140042889676608 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I1208 12:27:38.719513 140042889676608 training_utils.py:132] Using distribution strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f5df6ff8d68> with num_replicas_in_sync=1
I1208 12:27:38.719594 140042889676608 flags_core.py:514] ==========================================================================
I1208 12:27:38.719641 140042889676608 flags_core.py:515] Parsed all matched flags: 
I1208 12:27:38.719888 140042889676608 flags_core.py:520]  distribution_strategy: mirrored     # (default: mirrored) The distribution strategy.
I1208 12:27:38.719938 140042889676608 flags_core.py:520]  dtype: float16     # (default: float16) The computation type of the whole model.
I1208 12:27:38.719984 140042889676608 flags_core.py:520]  enable_check_numerics: None     # (default: None) Whether to open the tf.debugging.enable_check_numerics. Note that this may lower down the training speed.
I1208 12:27:38.720028 140042889676608 flags_core.py:520]  enable_xla: None     # (default: None) Whether to enable XLA for training.
I1208 12:27:38.720071 140042889676608 flags_core.py:520]  hparams_set: transformer_s     # (default: None) A string indicating a set of pre-defined hyper-parameters, e.g. transformer_base, transformer_big or transformer_768_16e_3d.
I1208 12:27:38.720115 140042889676608 flags_core.py:520]  model_dir: ./wmt14_en_de/benchmark_s-1208     # (default: None) The path to the checkpoint for saving and loading.
I1208 12:27:38.720162 140042889676608 flags_core.py:520]  enable_quant: False     # (default: False) Whether to enable quantization for finetuning.
I1208 12:27:38.720205 140042889676608 flags_core.py:520]  quant_params: None     # (default: None) A dict of parameters for quantization.
I1208 12:27:38.720249 140042889676608 flags_core.py:523]  entry.class: trainer
I1208 12:27:38.720304 140042889676608 flags_core.py:527]  entry.params:
I1208 12:27:38.720398 140042889676608 flags_core.py:535]    criterion.class: label_smoothed_cross_entropy
I1208 12:27:38.720482 140042889676608 flags_core.py:510]    criterion.params: {"label_smoothing": 0.1}     # The criterion for training or evaluation.
I1208 12:27:38.720530 140042889676608 flags_core.py:535]    optimizer.class: Adam
I1208 12:27:38.720589 140042889676608 flags_core.py:510]    optimizer.params: {"epsilon": 1e-09, "beta_1": 0.9, "beta_2": 0.98}     # The optimizer for training.
I1208 12:27:38.720639 140042889676608 flags_core.py:535]    lr_schedule.class: noam
I1208 12:27:38.720694 140042889676608 flags_core.py:510]    lr_schedule.params: {"dmodel": 256, "warmup_steps": 3000, "initial_factor": 1.0}     # The learning schedule for training.
I1208 12:27:38.720739 140042889676608 flags_core.py:535]    validator.class: SeqGenerationValidator
I1208 12:27:38.720808 140042889676608 flags_core.py:510]    validator.params: {"eval_dataset.params": {"src_file": "/root/neurst/wmt14_en_de/newstest2013.en.txt", "trg_file": "/root/neurst/wmt14_en_de/newstest2013.de.txt"}, "eval_search_method.params": {"beam_size": 4, "length_penalty": 0.6, "maximum_decode_length": 160, "extra_decode_length": 50}, "eval_steps": 10000, "eval_start_at": 10000, "eval_criterion.class": "label_smoothed_cross_entropy", "eval_criterion.params": {}, "eval_dataset.class": "ParallelTextDataset", "eval_batch_size": 64, "eval_metric.class": "bleu", "eval_metric.params": {}, "eval_search_method.class": "beam_search", "eval_auto_average_checkpoints": true, "eval_top_checkpoints_to_keep": 10}     # The validation process while training.
I1208 12:27:38.720856 140042889676608 flags_core.py:535]    pruning_schedule.class: PolynomialDecay
I1208 12:27:38.720916 140042889676608 flags_core.py:510]    pruning_schedule.params: {}     # The schedule for weight weight_pruning.
I1208 12:27:38.720989 140042889676608 flags_core.py:510]    train_steps: 100000     # (default: 10000000) The maximum steps for training loop.
I1208 12:27:38.721054 140042889676608 flags_core.py:510]    summary_steps: 1000     # (default: 200) Doing summary(logging & tensorboard) this every steps.
I1208 12:27:38.721118 140042889676608 flags_core.py:510]    save_checkpoint_steps: 2000     # (default: 1000) Saving checkpoints this every steps.
I1208 12:27:38.721205 140042889676608 flags_core.py:523]  task.class: translation
I1208 12:27:38.721269 140042889676608 flags_core.py:527]  task.params:
I1208 12:27:38.721385 140042889676608 flags_core.py:510]    batch_size: 2048     # The number of samples per update.
I1208 12:27:38.721453 140042889676608 flags_core.py:535]    src_data_pipeline.class: TextDataPipeline
I1208 12:27:38.721534 140042889676608 flags_core.py:510]    src_data_pipeline.params: {"language": "en", "tokenizer": "moses", "subtokenizer": "bpe", "subtokenizer_codes": "/root/neurst/wmt14_en_de/codes.bpe", "vocab_path": "/root/neurst/wmt14_en_de/vocab.en"}     # The source side data pipeline.
I1208 12:27:38.721600 140042889676608 flags_core.py:535]    trg_data_pipeline.class: TextDataPipeline
I1208 12:27:38.721677 140042889676608 flags_core.py:510]    trg_data_pipeline.params: {"language": "de", "tokenizer": "moses", "subtokenizer": "bpe", "subtokenizer_codes": "/root/neurst/wmt14_en_de/codes.bpe", "vocab_path": "/root/neurst/wmt14_en_de/vocab.de"}     # The target side data pipeline.
I1208 12:27:38.721741 140042889676608 flags_core.py:510]    max_src_len: 128     # The maximum source length of training data.
I1208 12:27:38.721804 140042889676608 flags_core.py:510]    max_trg_len: 128     # The maximum target length of training data.
I1208 12:27:38.721870 140042889676608 flags_core.py:510]    batch_by_tokens: True     # Whether to batch the data by word tokens.
I1208 12:27:38.721938 140042889676608 flags_core.py:523]  model.class: Transformer
I1208 12:27:38.722002 140042889676608 flags_core.py:527]  model.params:
I1208 12:27:38.722153 140042889676608 flags_core.py:510]    modality.share_source_target_embedding: False     # (default: False) Whether to share source and target embedding table.
I1208 12:27:38.722219 140042889676608 flags_core.py:510]    modality.share_embedding_and_softmax_weights: True     # (default: False) Whether to share the target embedding table and softmax weights.
I1208 12:27:38.722287 140042889676608 flags_core.py:510]    modality.dim: 256     # The default embedding dimension for both source and target side.
I1208 12:27:38.722361 140042889676608 flags_core.py:510]    modality.timing: sinusoids     # The arbitrary parameters for positional encoding of both source and target side.
I1208 12:27:38.722426 140042889676608 flags_core.py:510]    encoder.num_layers: 6     # The number of stacking layers of the encoder.
I1208 12:27:38.722489 140042889676608 flags_core.py:510]    encoder.hidden_size: 256     # The number of hidden units of the encoder.
I1208 12:27:38.722550 140042889676608 flags_core.py:510]    encoder.num_attention_heads: 4     # The number of heads of encoder self-attention.
I1208 12:27:38.722612 140042889676608 flags_core.py:510]    encoder.filter_size: 1024     # The number of the filter size of encoder ffn.
I1208 12:27:38.722674 140042889676608 flags_core.py:510]    encoder.ffn_activation: relu     # (default: relu) The activation function of encoder ffn layer.
I1208 12:27:38.722743 140042889676608 flags_core.py:510]    encoder.attention_dropout_rate: 0.1     # (default: 0.0) The dropout rate of encoder self-attention layer.
I1208 12:27:38.722806 140042889676608 flags_core.py:510]    encoder.attention_type: dot_product     # (default: dot_product) The type of the attention function of encoder self-attention layer.
I1208 12:27:38.722871 140042889676608 flags_core.py:510]    encoder.ffn_dropout_rate: 0.1     # (default: 0.0) The dropout rate of encoder ffn layer.
I1208 12:27:38.722940 140042889676608 flags_core.py:510]    encoder.layer_postprocess_dropout_rate: 0.1     # (default: 0.0) The dropout rate for each layer's post process in encoder.
I1208 12:27:38.723004 140042889676608 flags_core.py:510]    encoder.post_normalize: False     # (default: False) Whether to apply layer norm after each encoder block.
I1208 12:27:38.723068 140042889676608 flags_core.py:510]    decoder.num_layers: 6     # The number of stacking layers of the decoder.
I1208 12:27:38.723130 140042889676608 flags_core.py:510]    decoder.hidden_size: 256     # The number of hidden units of the decoder.
I1208 12:27:38.723192 140042889676608 flags_core.py:510]    decoder.num_attention_heads: 4     # The number of heads of decoder self-attention and encoder-decoder attention.
I1208 12:27:38.723253 140042889676608 flags_core.py:510]    decoder.filter_size: 1024     # The number of the filter size of decoder ffn.
I1208 12:27:38.723322 140042889676608 flags_core.py:510]    decoder.ffn_activation: relu     # (default: relu) The activation function of decoder ffn layer.
I1208 12:27:38.723387 140042889676608 flags_core.py:510]    decoder.attention_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder self-attention and encoder-decoder attention.
I1208 12:27:38.723450 140042889676608 flags_core.py:510]    decoder.attention_type: dot_product     # (default: dot_product) The type of the attention function of decoder self-attention and encoder-decoder attention.
I1208 12:27:38.723514 140042889676608 flags_core.py:510]    decoder.ffn_dropout_rate: 0.1     # (default: 0.0) The dropout rate of decoder ffn layer.
I1208 12:27:38.723578 140042889676608 flags_core.py:510]    decoder.layer_postprocess_dropout_rate: 0.1     # (default: 0.0) The dropout rate for each layer's post process in decoder.
I1208 12:27:38.723642 140042889676608 flags_core.py:510]    decoder.post_normalize: False     # (default: False) Whether to apply layer norm after each decoder block.
I1208 12:27:38.723710 140042889676608 flags_core.py:523]  dataset.class: ParallelTextDataset
I1208 12:27:38.723773 140042889676608 flags_core.py:527]  dataset.params:
I1208 12:27:38.723854 140042889676608 flags_core.py:510]    src_file: /root/neurst/wmt14_en_de/train.en.tok.bpe.txt     # The source text file
I1208 12:27:38.723918 140042889676608 flags_core.py:510]    trg_file: /root/neurst/wmt14_en_de/train.de.tok.bpe.txt     # The target text file
I1208 12:27:38.723982 140042889676608 flags_core.py:510]    data_is_processed: True     # Whether the text data is already processed.
I1208 12:27:38.724046 140042889676608 flags_core.py:545] 
I1208 12:27:38.724103 140042889676608 flags_core.py:546] Other flags:
I1208 12:27:38.724165 140042889676608 flags_core.py:548]  config_paths: ['wmt14_en_de/training_args.yml,wmt14_en_de/translation_bpe.yml,wmt14_en_de/validation_args.yml']
I1208 12:27:38.724222 140042889676608 flags_core.py:563] ==========================================================================
I1208 12:27:38.724284 140042889676608 training_utils.py:74] Using float16 as computation dtype.
I1208 12:27:38.793435 140042889676608 device_compatibility_check.py:124] Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100S-PCIE-32GB, compute capability 7.0
I1208 12:27:38.794038 140042889676608 registry.py:39] Creating task: <class 'neurst.tasks.translation.Translation'>
I1208 12:27:38.794120 140042889676608 registry.py:41]   (task) arguments: 
I1208 12:27:38.794170 140042889676608 registry.py:51]     batch_size: 2048
I1208 12:27:38.794214 140042889676608 registry.py:51]     src_data_pipeline.class: TextDataPipeline
I1208 12:27:38.794256 140042889676608 registry.py:44]     src_data_pipeline.params:
I1208 12:27:38.794306 140042889676608 registry.py:49]       language: en
I1208 12:27:38.794348 140042889676608 registry.py:49]       tokenizer: moses
I1208 12:27:38.794389 140042889676608 registry.py:49]       subtokenizer: bpe
I1208 12:27:38.794436 140042889676608 registry.py:49]       subtokenizer_codes: /root/neurst/wmt14_en_de/codes.bpe
I1208 12:27:38.794477 140042889676608 registry.py:49]       vocab_path: /root/neurst/wmt14_en_de/vocab.en
I1208 12:27:38.794518 140042889676608 registry.py:51]     trg_data_pipeline.class: TextDataPipeline
I1208 12:27:38.794559 140042889676608 registry.py:44]     trg_data_pipeline.params:
I1208 12:27:38.794600 140042889676608 registry.py:49]       language: de
I1208 12:27:38.794640 140042889676608 registry.py:49]       tokenizer: moses
I1208 12:27:38.794680 140042889676608 registry.py:49]       subtokenizer: bpe
I1208 12:27:38.794721 140042889676608 registry.py:49]       subtokenizer_codes: /root/neurst/wmt14_en_de/codes.bpe
I1208 12:27:38.794760 140042889676608 registry.py:49]       vocab_path: /root/neurst/wmt14_en_de/vocab.de
I1208 12:27:38.794801 140042889676608 registry.py:51]     max_src_len: 128
I1208 12:27:38.794842 140042889676608 registry.py:51]     max_trg_len: 128
I1208 12:27:38.794883 140042889676608 registry.py:51]     batch_by_tokens: True
I1208 12:27:38.794923 140042889676608 registry.py:51]     shuffle_buffer: 0
I1208 12:27:38.794965 140042889676608 registry.py:51]     batch_size_per_gpu: None
I1208 12:27:38.795006 140042889676608 registry.py:51]     cache_dataset: None
I1208 12:27:38.795047 140042889676608 registry.py:51]     truncate_src: None
I1208 12:27:38.795088 140042889676608 registry.py:51]     truncate_trg: None
I1208 12:27:38.795128 140042889676608 registry.py:51]     target_begin_of_sentence: bos
I1208 12:27:38.795169 140042889676608 registry.py:51]     gpu_efficient_level: 0
I1208 12:27:38.795211 140042889676608 registry.py:51]     auto_scaling_batch_size: None
I1208 12:27:41.130746 140042889676608 registry.py:39] Creating dataset: <class 'neurst.data.datasets.parallel_text_dataset.ParallelTextDataset'>
I1208 12:27:41.130922 140042889676608 registry.py:41]   (dataset) arguments: 
I1208 12:27:41.130976 140042889676608 registry.py:51]     src_file: /root/neurst/wmt14_en_de/train.en.tok.bpe.txt
I1208 12:27:41.131022 140042889676608 registry.py:51]     trg_file: /root/neurst/wmt14_en_de/train.de.tok.bpe.txt
I1208 12:27:41.131066 140042889676608 registry.py:51]     data_is_processed: True
I1208 12:27:41.131110 140042889676608 registry.py:51]     raw_trg_file: None
I1208 12:27:41.131152 140042889676608 registry.py:51]     src_lang: None
I1208 12:27:41.131194 140042889676608 registry.py:51]     trg_lang: None
I1208 12:27:41.131381 140042889676608 registry.py:39] Creating model: <class 'neurst.models.transformer.Transformer'>
I1208 12:27:41.131430 140042889676608 registry.py:41]   (model) arguments: 
I1208 12:27:41.131473 140042889676608 registry.py:51]     modality.share_source_target_embedding: False
I1208 12:27:41.131515 140042889676608 registry.py:51]     modality.share_embedding_and_softmax_weights: True
I1208 12:27:41.131556 140042889676608 registry.py:51]     modality.dim: 256
I1208 12:27:41.131596 140042889676608 registry.py:51]     modality.timing: sinusoids
I1208 12:27:41.131637 140042889676608 registry.py:51]     encoder.num_layers: 6
I1208 12:27:41.131678 140042889676608 registry.py:51]     encoder.hidden_size: 256
I1208 12:27:41.131718 140042889676608 registry.py:51]     encoder.num_attention_heads: 4
I1208 12:27:41.131766 140042889676608 registry.py:51]     encoder.filter_size: 1024
I1208 12:27:41.131807 140042889676608 registry.py:51]     encoder.ffn_activation: relu
I1208 12:27:41.131852 140042889676608 registry.py:51]     encoder.attention_dropout_rate: 0.1
I1208 12:27:41.131892 140042889676608 registry.py:51]     encoder.attention_type: dot_product
I1208 12:27:41.131934 140042889676608 registry.py:51]     encoder.ffn_dropout_rate: 0.1
I1208 12:27:41.131975 140042889676608 registry.py:51]     encoder.layer_postprocess_dropout_rate: 0.1
I1208 12:27:41.132017 140042889676608 registry.py:51]     encoder.post_normalize: False
I1208 12:27:41.132057 140042889676608 registry.py:51]     decoder.num_layers: 6
I1208 12:27:41.132097 140042889676608 registry.py:51]     decoder.hidden_size: 256
I1208 12:27:41.132145 140042889676608 registry.py:51]     decoder.num_attention_heads: 4
I1208 12:27:41.132185 140042889676608 registry.py:51]     decoder.filter_size: 1024
I1208 12:27:41.132225 140042889676608 registry.py:51]     decoder.ffn_activation: relu
I1208 12:27:41.132266 140042889676608 registry.py:51]     decoder.attention_dropout_rate: 0.1
I1208 12:27:41.132313 140042889676608 registry.py:51]     decoder.attention_type: dot_product
I1208 12:27:41.132354 140042889676608 registry.py:51]     decoder.ffn_dropout_rate: 0.1
I1208 12:27:41.132395 140042889676608 registry.py:51]     decoder.layer_postprocess_dropout_rate: 0.1
I1208 12:27:41.132436 140042889676608 registry.py:51]     decoder.post_normalize: False
I1208 12:27:41.132478 140042889676608 registry.py:51]     modality.source.dim: None
I1208 12:27:41.132519 140042889676608 registry.py:51]     modality.target.dim: None
I1208 12:27:41.132560 140042889676608 registry.py:51]     modality.source.timing: None
I1208 12:27:41.132601 140042889676608 registry.py:51]     modality.target.timing: None
I1208 12:27:41.132641 140042889676608 registry.py:51]     encoder.attention_monotonic: None
I1208 12:27:41.132687 140042889676608 registry.py:51]     encoder.layer_postprocess_epsilon: 1e-06
I1208 12:27:41.132729 140042889676608 registry.py:51]     decoder.layer_postprocess_epsilon: 1e-06
I1208 12:27:41.132770 140042889676608 registry.py:53]   (model) extra args: 
I1208 12:27:41.132817 140042889676608 registry.py:55]     - {'language': 'en', 'vocab_size': 42293, 'eos_id': 42292, 'bos_id': 42291, 'unk_id': 42290, 'pad_id': 42292, 'padding_mode': 2}
I1208 12:27:41.132862 140042889676608 registry.py:55]     - {'language': 'de', 'vocab_size': 43629, 'eos_id': 43628, 'bos_id': 43627, 'unk_id': 43626, 'pad_id': 43628, 'padding_mode': 2}
I1208 12:27:41.132903 140042889676608 registry.py:57]   (model) extra k-v args: 
I1208 12:27:41.132944 140042889676608 registry.py:59]     name: None
I1208 12:27:54.988496 140042889676608 registry.py:39] Creating entry: <class 'neurst.exps.trainer.Trainer'>
I1208 12:27:54.988678 140042889676608 registry.py:41]   (entry) arguments: 
I1208 12:27:54.988735 140042889676608 registry.py:51]     criterion.class: label_smoothed_cross_entropy
I1208 12:27:54.988782 140042889676608 registry.py:44]     criterion.params:
I1208 12:27:54.988832 140042889676608 registry.py:49]       label_smoothing: 0.1
I1208 12:27:54.988877 140042889676608 registry.py:51]     optimizer.class: Adam
I1208 12:27:54.988919 140042889676608 registry.py:44]     optimizer.params:
I1208 12:27:54.988965 140042889676608 registry.py:49]       epsilon: 1e-09
I1208 12:27:54.989008 140042889676608 registry.py:49]       beta_1: 0.9
I1208 12:27:54.989049 140042889676608 registry.py:49]       beta_2: 0.98
I1208 12:27:54.989091 140042889676608 registry.py:51]     lr_schedule.class: noam
I1208 12:27:54.989131 140042889676608 registry.py:44]     lr_schedule.params:
I1208 12:27:54.989173 140042889676608 registry.py:49]       dmodel: 256
I1208 12:27:54.989215 140042889676608 registry.py:49]       warmup_steps: 3000
I1208 12:27:54.989257 140042889676608 registry.py:49]       initial_factor: 1.0
I1208 12:27:54.989304 140042889676608 registry.py:51]     validator.class: SeqGenerationValidator
I1208 12:27:54.989345 140042889676608 registry.py:44]     validator.params:
I1208 12:27:54.989391 140042889676608 registry.py:49]       eval_dataset.params: {'src_file': '/root/neurst/wmt14_en_de/newstest2013.en.txt', 'trg_file': '/root/neurst/wmt14_en_de/newstest2013.de.txt'}
I1208 12:27:54.989438 140042889676608 registry.py:49]       eval_search_method.params: {'beam_size': 4, 'length_penalty': 0.6, 'maximum_decode_length': 160, 'extra_decode_length': 50}
I1208 12:27:54.989479 140042889676608 registry.py:49]       eval_steps: 10000
I1208 12:27:54.989520 140042889676608 registry.py:49]       eval_start_at: 10000
I1208 12:27:54.989560 140042889676608 registry.py:49]       eval_criterion.class: label_smoothed_cross_entropy
I1208 12:27:54.989602 140042889676608 registry.py:49]       eval_criterion.params: {}
I1208 12:27:54.989647 140042889676608 registry.py:49]       eval_dataset.class: ParallelTextDataset
I1208 12:27:54.989688 140042889676608 registry.py:49]       eval_batch_size: 64
I1208 12:27:54.989728 140042889676608 registry.py:49]       eval_metric.class: bleu
I1208 12:27:54.989770 140042889676608 registry.py:49]       eval_metric.params: {}
I1208 12:27:54.989810 140042889676608 registry.py:49]       eval_search_method.class: beam_search
I1208 12:27:54.989852 140042889676608 registry.py:49]       eval_auto_average_checkpoints: True
I1208 12:27:54.989893 140042889676608 registry.py:49]       eval_top_checkpoints_to_keep: 10
I1208 12:27:54.989934 140042889676608 registry.py:51]     pruning_schedule.class: PolynomialDecay
I1208 12:27:54.989974 140042889676608 registry.py:44]     pruning_schedule.params:
I1208 12:27:54.990015 140042889676608 registry.py:51]     train_steps: 100000
I1208 12:27:54.990056 140042889676608 registry.py:51]     summary_steps: 1000
I1208 12:27:54.990097 140042889676608 registry.py:51]     save_checkpoint_steps: 2000
I1208 12:27:54.990139 140042889676608 registry.py:51]     tb_log_dir: None
I1208 12:27:54.990181 140042889676608 registry.py:51]     train_epochs: None
I1208 12:27:54.990221 140042889676608 registry.py:51]     checkpoints_max_to_keep: 8
I1208 12:27:54.990262 140042889676608 registry.py:51]     initial_global_step: None
I1208 12:27:54.990308 140042889676608 registry.py:51]     pretrain_model: None
I1208 12:27:54.990350 140042889676608 registry.py:51]     pretrain_variable_pattern: None
I1208 12:27:54.990390 140042889676608 registry.py:51]     update_cycle: 1
I1208 12:27:54.990432 140042889676608 registry.py:51]     clip_value: None
I1208 12:27:54.990473 140042889676608 registry.py:51]     clip_norm: None
I1208 12:27:54.990514 140042889676608 registry.py:51]     experimental_count_batch_num: None
I1208 12:27:54.990555 140042889676608 registry.py:51]     freeze_variables: None
I1208 12:27:54.990596 140042889676608 registry.py:51]     pruning_variable_pattern: None
I1208 12:27:54.990637 140042889676608 registry.py:51]     nopruning_variable_pattern: None
I1208 12:27:54.990679 140042889676608 registry.py:51]     optimizer_controller: None
I1208 12:27:54.990720 140042889676608 registry.py:51]     optimizer_controller_args: None
I1208 12:27:54.990762 140042889676608 registry.py:57]   (entry) extra k-v args: 
I1208 12:27:54.990806 140042889676608 registry.py:59]     strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f5df6ff8d68>
I1208 12:27:54.990851 140042889676608 registry.py:59]     model: <neurst.models.transformer.Transformer object at 0x7f5c94505940>
I1208 12:27:54.990896 140042889676608 registry.py:59]     task: <neurst.tasks.translation.Translation object at 0x7f5df6ff39e8>
I1208 12:27:54.990936 140042889676608 registry.py:59]     model_dir: ./wmt14_en_de/benchmark_s-1208
I1208 12:27:54.990980 140042889676608 registry.py:59]     custom_dataset: <neurst.data.datasets.parallel_text_dataset.ParallelTextDataset object at 0x7f5df6ff39b0>
I1208 12:27:54.991126 140042889676608 registry.py:39] Creating criterion: <class 'neurst.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropy'>
I1208 12:27:54.991174 140042889676608 registry.py:41]   (criterion) arguments: 
I1208 12:27:54.991219 140042889676608 registry.py:51]     label_smoothing: 0.1
I1208 12:27:54.991267 140042889676608 label_smoothed_cross_entropy.py:38] Using LabelSmoothedCrossEntropy with label_smoothing=0.1
I1208 12:27:54.991331 140042889676608 registry.py:39] Creating optimizer: <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>
I1208 12:27:54.991375 140042889676608 registry.py:57]   (optimizer) extra k-v args: 
I1208 12:27:54.991419 140042889676608 registry.py:59]     epsilon: 1e-09
I1208 12:27:54.991461 140042889676608 registry.py:59]     beta_1: 0.9
I1208 12:27:54.991503 140042889676608 registry.py:59]     beta_2: 0.98
I1208 12:27:54.991692 140042889676608 registry.py:39] Creating validator: <class 'neurst.training.seq_generation_validator.SeqGenerationValidator'>
I1208 12:27:54.991745 140042889676608 registry.py:41]   (validator) arguments: 
I1208 12:27:54.991789 140042889676608 registry.py:44]     eval_dataset.params:
I1208 12:27:54.991832 140042889676608 registry.py:49]       src_file: /root/neurst/wmt14_en_de/newstest2013.en.txt
I1208 12:27:54.991874 140042889676608 registry.py:49]       trg_file: /root/neurst/wmt14_en_de/newstest2013.de.txt
I1208 12:27:54.991915 140042889676608 registry.py:44]     eval_search_method.params:
I1208 12:27:54.991956 140042889676608 registry.py:49]       beam_size: 4
I1208 12:27:54.991998 140042889676608 registry.py:49]       length_penalty: 0.6
I1208 12:27:54.992039 140042889676608 registry.py:49]       maximum_decode_length: 160
I1208 12:27:54.992080 140042889676608 registry.py:49]       extra_decode_length: 50
I1208 12:27:54.992121 140042889676608 registry.py:51]     eval_steps: 10000
I1208 12:27:54.992162 140042889676608 registry.py:51]     eval_start_at: 10000
I1208 12:27:54.992202 140042889676608 registry.py:51]     eval_criterion.class: label_smoothed_cross_entropy
I1208 12:27:54.992243 140042889676608 registry.py:44]     eval_criterion.params:
I1208 12:27:54.992283 140042889676608 registry.py:51]     eval_dataset.class: ParallelTextDataset
I1208 12:27:54.992329 140042889676608 registry.py:51]     eval_batch_size: 64
I1208 12:27:54.992370 140042889676608 registry.py:51]     eval_metric.class: bleu
I1208 12:27:54.992410 140042889676608 registry.py:44]     eval_metric.params:
I1208 12:27:54.992451 140042889676608 registry.py:51]     eval_search_method.class: beam_search
I1208 12:27:54.992492 140042889676608 registry.py:51]     eval_auto_average_checkpoints: True
I1208 12:27:54.992533 140042889676608 registry.py:51]     eval_top_checkpoints_to_keep: 10
I1208 12:27:54.992574 140042889676608 registry.py:51]     eval_on_begin: False
I1208 12:27:54.992615 140042889676608 registry.py:51]     eval_task_args: None
I1208 12:27:54.992656 140042889676608 registry.py:51]     eval_estop_patience: 0
I1208 12:27:54.992698 140042889676608 registry.py:51]     eval_best_checkpoint_path: None
I1208 12:27:54.992738 140042889676608 registry.py:51]     eval_best_avg_checkpoint_path: None
I1208 12:27:54.992840 140042889676608 registry.py:39] Creating pruning_schedule: <class 'neurst.sparsity.pruning_schedule.PolynomialDecay'>
I1208 12:27:54.992885 140042889676608 registry.py:41]   (pruning_schedule) arguments: 
I1208 12:27:54.992928 140042889676608 registry.py:51]     target_sparsity: 0.0
I1208 12:27:54.992969 140042889676608 registry.py:51]     begin_pruning_step: 0
I1208 12:27:54.993010 140042889676608 registry.py:51]     end_pruning_step: -1
I1208 12:27:54.993051 140042889676608 registry.py:51]     pruning_frequency: 100
I1208 12:27:54.993092 140042889676608 registry.py:51]     initial_sparsity: 0.0
I1208 12:27:54.993132 140042889676608 registry.py:51]     polynomial_power: 3
I1208 12:27:54.993251 140042889676608 translation.py:70] Creating training dataset with GPU efficient level=0.
I1208 12:27:55.129405 140042889676608 dataset_utils.py:330] Filtering empty data and datas exceeded max length={'feature': 128, 'label': 128}
I1208 12:27:55.320383 140042889676608 dataset_utils.py:452] The global batch size is 2048 tokens.
I1208 12:27:55.320606 140042889676608 dataset_utils.py:497] The details of batching logic:
I1208 12:27:55.320669 140042889676608 dataset_utils.py:503]    - batch=256, bucket boundary={'feature': 8, 'label': 8}
I1208 12:27:55.320719 140042889676608 dataset_utils.py:503]    - batch=128, bucket boundary={'feature': 16, 'label': 16}
I1208 12:27:55.320772 140042889676608 dataset_utils.py:503]    - batch=85, bucket boundary={'feature': 24, 'label': 24}
I1208 12:27:55.320820 140042889676608 dataset_utils.py:503]    - batch=64, bucket boundary={'feature': 32, 'label': 32}
I1208 12:27:55.320864 140042889676608 dataset_utils.py:503]    - batch=51, bucket boundary={'feature': 40, 'label': 40}
I1208 12:27:55.320909 140042889676608 dataset_utils.py:503]    - batch=42, bucket boundary={'feature': 48, 'label': 48}
I1208 12:27:55.320953 140042889676608 dataset_utils.py:503]    - batch=36, bucket boundary={'feature': 56, 'label': 56}
I1208 12:27:55.321009 140042889676608 dataset_utils.py:503]    - batch=32, bucket boundary={'feature': 64, 'label': 64}
I1208 12:27:55.321053 140042889676608 dataset_utils.py:503]    - batch=28, bucket boundary={'feature': 72, 'label': 72}
I1208 12:27:55.321097 140042889676608 dataset_utils.py:503]    - batch=25, bucket boundary={'feature': 80, 'label': 80}
I1208 12:27:55.321140 140042889676608 dataset_utils.py:503]    - batch=23, bucket boundary={'feature': 88, 'label': 88}
I1208 12:27:55.321183 140042889676608 dataset_utils.py:503]    - batch=21, bucket boundary={'feature': 96, 'label': 96}
I1208 12:27:55.321228 140042889676608 dataset_utils.py:503]    - batch=19, bucket boundary={'feature': 104, 'label': 104}
I1208 12:27:55.321271 140042889676608 dataset_utils.py:503]    - batch=18, bucket boundary={'feature': 112, 'label': 112}
I1208 12:27:55.321323 140042889676608 dataset_utils.py:503]    - batch=17, bucket boundary={'feature': 120, 'label': 120}
I1208 12:27:55.321367 140042889676608 dataset_utils.py:503]    - batch=16, bucket boundary={'feature': 128, 'label': 128}
I1208 12:27:55.321409 140042889676608 dataset_utils.py:506]   Total 16 input shapes are compiled.
I1208 12:28:06.496423 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:06.499846 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:07.510111 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:07.512526 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:09.064939 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:09.067495 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:10.075559 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:10.077908 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:11.138693 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:11.141205 140042889676608 cross_device_ops.py:443] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1208 12:28:12.629242 140042889676608 trainer.py:155] No checkpoint restored from model_dir=./wmt14_en_de/benchmark_s-1208
I1208 12:28:12.629831 140042889676608 registry.py:39] Creating lr_schedule: <class 'neurst.optimizers.schedules.noam_schedule.NoamSchedule'>
I1208 12:28:12.629920 140042889676608 registry.py:41]   (lr_schedule) arguments: 
I1208 12:28:12.629997 140042889676608 registry.py:51]     dmodel: 256
I1208 12:28:12.630085 140042889676608 registry.py:51]     warmup_steps: 3000
I1208 12:28:12.630165 140042889676608 registry.py:51]     initial_factor: 1.0
I1208 12:28:12.630227 140042889676608 registry.py:51]     end_factor: None
I1208 12:28:12.630281 140042889676608 registry.py:51]     start_decay_at: 0
I1208 12:28:12.630344 140042889676608 registry.py:51]     decay_steps: None
I1208 12:28:12.631325 140042889676608 noam_schedule.py:39] Initialize NoamSchedule from global step=0. The result learning rate will be scaled by 1.0
I1208 12:28:12.631666 140042889676608 revised_dynamic_loss_scale.py:50] Using RevisedDynamaicLossScale under FP16 to ensure tf.reduce_all behaviour, especially under XLA
I1208 12:28:12.770113 140042889676608 model_utils.py:80] variable name  | # parameters
I1208 12:28:12.771827 140042889676608 model_utils.py:132]  SequenceToSequence (--/33.10m params)
I1208 12:28:12.771906 140042889676608 model_utils.py:132]    SequenceToSequence/input_symbol_modality_posenc_wrapper (--/10.83m params)
I1208 12:28:12.771966 140042889676608 model_utils.py:132]      SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality (--/10.83m params)
I1208 12:28:12.772017 140042889676608 model_utils.py:132]        SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality/emb (--/10.83m params)
I1208 12:28:12.772069 140042889676608 model_utils.py:137]          SequenceToSequence/input_symbol_modality_posenc_wrapper/input_symbol_modality/emb/weights (42293x256, 10.83m params)
I1208 12:28:12.772118 140042889676608 model_utils.py:132]    SequenceToSequence/target_symbol_modality_posenc_wrapper (--/11.21m params)
I1208 12:28:12.772163 140042889676608 model_utils.py:132]      SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality (--/11.21m params)
I1208 12:28:12.772208 140042889676608 model_utils.py:132]        SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared (--/11.21m params)
I1208 12:28:12.772258 140042889676608 model_utils.py:137]          SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared/weights (43629x256, 11.17m params)
I1208 12:28:12.772310 140042889676608 model_utils.py:137]          SequenceToSequence/target_symbol_modality_posenc_wrapper/target_symbol_modality/shared/bias (43629, 43.63k params)
I1208 12:28:12.772357 140042889676608 model_utils.py:132]    SequenceToSequence/TransformerEncoder (--/4.74m params)
I1208 12:28:12.772403 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerEncoder/layer_0 (--/789.76k params)
I1208 12:28:12.772448 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.772493 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.772539 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.772587 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.772633 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.772680 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.772726 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.772772 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.772819 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.772866 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.772911 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_0/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.772963 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.773009 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.773054 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.773102 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.773148 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.773194 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.773240 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.773285 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.773335 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.773381 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.773426 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_0/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.773471 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerEncoder/layer_1 (--/789.76k params)
I1208 12:28:12.773517 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.773562 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.773607 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.773653 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.773699 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.773744 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.773791 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.773836 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.773881 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.773927 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.773972 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_1/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.774018 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.774066 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.774111 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.774158 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.774204 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.774250 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.774300 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.774346 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.774391 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.774437 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.774482 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_1/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.774528 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerEncoder/layer_2 (--/789.76k params)
I1208 12:28:12.774573 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.774619 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.774664 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.774711 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.774756 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.774801 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.774849 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.774894 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.774940 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.774985 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.775030 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_2/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.775075 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.775122 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.775167 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.775215 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.775261 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.775310 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.775358 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.775403 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.775448 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.775493 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.775538 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_2/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.775583 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerEncoder/layer_3 (--/789.76k params)
I1208 12:28:12.775627 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.775671 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.775716 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.775762 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.775808 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.775853 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.775899 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.775945 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.775990 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.776035 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.776080 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_3/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.776125 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.776170 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.776217 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.776264 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.776315 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.776361 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.776407 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.776453 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.776498 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.776543 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.776588 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_3/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.776634 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerEncoder/layer_4 (--/789.76k params)
I1208 12:28:12.776679 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.776724 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.776769 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.776816 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.776862 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.776907 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.776954 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.777000 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.777044 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.777089 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.777134 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_4/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.777179 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.777223 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.777270 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.777331 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.777378 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.777423 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.777469 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.777515 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.777559 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.777604 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.777650 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_4/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.777695 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerEncoder/layer_5 (--/789.76k params)
I1208 12:28:12.777741 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.777786 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.777830 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.777877 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.777923 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.777968 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.778015 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.778061 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.778106 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.778151 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.778197 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_5/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.778243 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.778288 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.778337 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.778387 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.778434 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.778479 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.778527 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.778572 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.778617 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.778662 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.778707 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerEncoder/layer_5/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.778751 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerEncoder/output_ln (--/512 params)
I1208 12:28:12.778796 140042889676608 model_utils.py:137]        SequenceToSequence/TransformerEncoder/output_ln/gamma (256, 256 params)
I1208 12:28:12.778841 140042889676608 model_utils.py:137]        SequenceToSequence/TransformerEncoder/output_ln/beta (256, 256 params)
I1208 12:28:12.778886 140042889676608 model_utils.py:132]    SequenceToSequence/TransformerDecoder (--/6.32m params)
I1208 12:28:12.778932 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerDecoder/layer_0 (--/1.05m params)
I1208 12:28:12.778977 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.779022 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.779067 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.779114 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.779159 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.779205 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.779252 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.779300 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.779345 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.779391 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.779436 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_0/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.779485 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.779530 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1208 12:28:12.779575 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1208 12:28:12.779622 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.779667 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.779713 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1208 12:28:12.779759 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.779805 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1208 12:28:12.779850 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1208 12:28:12.779897 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1208 12:28:12.779942 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1208 12:28:12.779987 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.780033 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.780078 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_0/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.780123 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.780168 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.780213 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.780260 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.780310 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.780356 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.780402 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.780448 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.780500 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.780546 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.780591 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_0/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.780637 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerDecoder/layer_1 (--/1.05m params)
I1208 12:28:12.780682 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.780727 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.780771 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.780818 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.780864 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.780909 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.780956 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.781002 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.781046 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.781091 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.781136 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_1/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.781182 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.781227 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1208 12:28:12.781271 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1208 12:28:12.781322 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.781368 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.781413 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1208 12:28:12.781460 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.781509 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1208 12:28:12.781554 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1208 12:28:12.781601 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1208 12:28:12.781646 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1208 12:28:12.781691 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.781736 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.781782 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_1/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.781827 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.781873 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.781918 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.781964 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.782011 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.782057 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.782103 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.782148 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.782192 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.782238 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.782283 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_1/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.782332 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerDecoder/layer_2 (--/1.05m params)
I1208 12:28:12.782377 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.782422 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.782466 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.782513 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.782559 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.782608 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.782655 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.782701 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.782746 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.782792 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.782838 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_2/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.782883 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.782928 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1208 12:28:12.782974 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1208 12:28:12.783020 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.783067 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.783112 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1208 12:28:12.783159 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.783205 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1208 12:28:12.783250 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1208 12:28:12.783299 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1208 12:28:12.783345 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1208 12:28:12.783390 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.783435 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.783481 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_2/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.783526 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.783574 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.783619 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.783666 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.783711 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.783756 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.783803 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.783848 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.783893 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.783938 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.783982 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_2/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.784029 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerDecoder/layer_3 (--/1.05m params)
I1208 12:28:12.784074 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.784119 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.784164 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.784211 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.784257 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.784305 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.784352 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.784398 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.784443 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.784489 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.784535 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_3/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.784580 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.784625 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1208 12:28:12.784673 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1208 12:28:12.784720 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.784766 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.784812 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1208 12:28:12.784859 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.784905 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1208 12:28:12.784950 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1208 12:28:12.784998 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1208 12:28:12.785043 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1208 12:28:12.785088 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.785134 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.785179 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_3/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.785224 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.785268 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.785315 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.785362 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.785409 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.785454 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.785501 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.785547 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.785591 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.785637 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.785685 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_3/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.785731 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerDecoder/layer_4 (--/1.05m params)
I1208 12:28:12.785776 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.785821 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.785866 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.785913 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.785958 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.786003 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.786050 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.786095 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.786139 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.786185 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.786230 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_4/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.786275 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.786323 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1208 12:28:12.786369 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1208 12:28:12.786416 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.786461 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.786506 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1208 12:28:12.786553 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.786599 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1208 12:28:12.786644 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1208 12:28:12.786695 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1208 12:28:12.786741 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1208 12:28:12.786786 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.786831 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.786877 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_4/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.786922 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.786967 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.787012 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.787059 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.787105 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.787150 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.787197 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.787242 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.787286 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.787335 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.787380 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_4/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.787426 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerDecoder/layer_5 (--/1.05m params)
I1208 12:28:12.787471 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.787516 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/self_attention (--/263.17k params)
I1208 12:28:12.787561 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/self_attention/output_transform (--/65.79k params)
I1208 12:28:12.787607 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/self_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.787653 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/self_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.787698 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/self_attention/qkv_transform (--/197.38k params)
I1208 12:28:12.787747 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/self_attention/qkv_transform/kernel (256x768, 196.61k params)
I1208 12:28:12.787794 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/self_attention/qkv_transform/bias (768, 768 params)
I1208 12:28:12.787838 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.787884 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.787929 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_5/self_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.787974 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper (--/263.68k params)
I1208 12:28:12.788019 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention (--/263.17k params)
I1208 12:28:12.788064 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/output_transform (--/65.79k params)
I1208 12:28:12.788111 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/output_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.788156 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/output_transform/bias (256, 256 params)
I1208 12:28:12.788201 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/q_transform (--/65.79k params)
I1208 12:28:12.788248 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/q_transform/kernel (256x256, 65.54k params)
I1208 12:28:12.788298 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/q_transform/bias (256, 256 params)
I1208 12:28:12.788344 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/kv_transform (--/131.58k params)
I1208 12:28:12.788392 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/kernel (256x512, 131.07k params)
I1208 12:28:12.788437 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/encdec_attention/kv_transform/bias (512, 512 params)
I1208 12:28:12.788482 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.788527 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.788572 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_5/encdec_attention_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.788617 140042889676608 model_utils.py:132]        SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper (--/526.08k params)
I1208 12:28:12.788661 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ffn (--/525.57k params)
I1208 12:28:12.788706 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ffn/dense1 (--/263.17k params)
I1208 12:28:12.788755 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ffn/dense1/kernel (256x1024, 262.14k params)
I1208 12:28:12.788802 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ffn/dense1/bias (1024, 1.02k params)
I1208 12:28:12.788848 140042889676608 model_utils.py:132]            SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ffn/dense2 (--/262.40k params)
I1208 12:28:12.788895 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ffn/dense2/kernel (1024x256, 262.14k params)
I1208 12:28:12.788941 140042889676608 model_utils.py:137]              SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ffn/dense2/bias (256, 256 params)
I1208 12:28:12.788985 140042889676608 model_utils.py:132]          SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ln (--/512 params)
I1208 12:28:12.789031 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ln/gamma (256, 256 params)
I1208 12:28:12.789075 140042889676608 model_utils.py:137]            SequenceToSequence/TransformerDecoder/layer_5/ffn_prepost_wrapper/ln/beta (256, 256 params)
I1208 12:28:12.789120 140042889676608 model_utils.py:132]      SequenceToSequence/TransformerDecoder/output_ln (--/512 params)
I1208 12:28:12.789166 140042889676608 model_utils.py:137]        SequenceToSequence/TransformerDecoder/output_ln/gamma (256, 256 params)
I1208 12:28:12.789211 140042889676608 model_utils.py:137]        SequenceToSequence/TransformerDecoder/output_ln/beta (256, 256 params)
I1208 12:28:12.812753 140042889676608 checkpoints.py:166] Creates checkpoint manager for directory: ./wmt14_en_de/benchmark_s-1208
I1208 12:28:12.819915 140042889676608 registry.py:39] Creating criterion: <class 'neurst.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropy'>
I1208 12:28:12.820018 140042889676608 registry.py:41]   (criterion) arguments: 
I1208 12:28:12.820071 140042889676608 registry.py:51]     label_smoothing: 0.0
I1208 12:28:12.820182 140042889676608 registry.py:39] Creating dataset: <class 'neurst.data.datasets.parallel_text_dataset.ParallelTextDataset'>
I1208 12:28:12.820230 140042889676608 registry.py:41]   (dataset) arguments: 
I1208 12:28:12.820273 140042889676608 registry.py:51]     src_file: /root/neurst/wmt14_en_de/newstest2013.en.txt
I1208 12:28:12.820322 140042889676608 registry.py:51]     trg_file: /root/neurst/wmt14_en_de/newstest2013.de.txt
I1208 12:28:12.820366 140042889676608 registry.py:51]     raw_trg_file: None
I1208 12:28:12.820409 140042889676608 registry.py:51]     data_is_processed: None
I1208 12:28:12.820451 140042889676608 registry.py:51]     src_lang: None
I1208 12:28:12.820494 140042889676608 registry.py:51]     trg_lang: None
I1208 12:28:15.249181 140042889676608 seq2seq.py:231] Creating evaluation dataset.
I1208 12:28:15.249383 140042889676608 dataset_utils.py:450] The global batch size is 64 samples.
I1208 12:28:15.640926 140042889676608 registry.py:39] Creating search_method: <class 'neurst.layers.search.beam_search.BeamSearch'>
I1208 12:28:15.641078 140042889676608 registry.py:41]   (search_method) arguments: 
I1208 12:28:15.641133 140042889676608 registry.py:51]     beam_size: 4
I1208 12:28:15.641184 140042889676608 registry.py:51]     length_penalty: 0.6
I1208 12:28:15.641228 140042889676608 registry.py:51]     top_k: 1
I1208 12:28:15.641271 140042889676608 registry.py:51]     maximum_decode_length: 160
I1208 12:28:15.641321 140042889676608 registry.py:51]     minimum_decode_length: 0
I1208 12:28:15.641363 140042889676608 registry.py:51]     extra_decode_length: 50
I1208 12:28:15.641406 140042889676608 registry.py:51]     padded_decode: None
I1208 12:28:15.641447 140042889676608 registry.py:51]     ensemble_weights: average
I1208 12:28:15.641488 140042889676608 registry.py:51]     enable_unk: None
I1208 12:28:22.794171 140042889676608 seq2seq.py:223] Creating test dataset.
I1208 12:28:22.794816 140042889676608 dataset_utils.py:450] The global batch size is 64 samples.
I1208 12:28:22.926424 140042889676608 checkpoints.py:209] Creates custom keep-best checkpoint manager for directory: ./wmt14_en_de/benchmark_s-1208/best
I1208 12:28:22.927315 140042889676608 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_s-1208/best
I1208 12:28:23.257382 140042889676608 checkpoints.py:271] Create checkpoint manager for averaged checkpoint of the latest 10 checkpoints to dir: ./wmt14_en_de/benchmark_s-1208/best_avg
I1208 12:28:23.258457 140042889676608 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_s-1208/best_avg
I1208 12:28:23.274985 140042889676608 trainer.py:306] Training for 100000 steps.
I1208 12:28:24.084906 140042889676608 configurable.py:296] Saving model configurations to directory: ./wmt14_en_de/benchmark_s-1208
W1208 12:28:24.415904 140042889676608 deprecation.py:323] From /root/.local/conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
I1208 12:32:19.189004 140042889676608 callbacks.py:220] Update 1000	TrainingLoss=6.79	Speed 0.235 secs/step 4.3 steps/sec
I1208 12:32:19.221737 140042889676608 callbacks.py:238] {'step': 1000, 'lr': 0.0003803629, 'loss': 6.7870965003967285, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 8655.117859493079, 'src_real_tokens_per_step': 1652.174, 'src_real_tokens_per_sec': 7034.099076397555, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 8655.117859493079, 'trg_real_tokens_per_step': 1689.269, 'trg_real_tokens_per_sec': 7192.030326519496, 'samples_per_step': 55.584, 'samples_per_sec': 236.64781255635404, 'this_step_loss': 193330.140625}
I1208 12:35:46.152589 140042889676608 callbacks.py:220] Update 2000	TrainingLoss=6.13	Speed 0.206 secs/step 4.8 steps/sec
I1208 12:35:46.154450 140042889676608 callbacks.py:238] {'step': 2000, 'lr': 0.0007607258, 'loss': 6.130962371826172, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 9855.977721541298, 'src_real_tokens_per_step': 1653.658, 'src_real_tokens_per_sec': 8016.550328685187, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 9855.977721541298, 'trg_real_tokens_per_step': 1688.407, 'trg_real_tokens_per_sec': 8185.005418777263, 'samples_per_step': 55.958, 'samples_per_sec': 271.271401518673, 'this_step_loss': 171714.578125}
I1208 12:35:46.845574 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-2000	Elapsed 0.69s
I1208 12:39:43.528966 140042889676608 callbacks.py:220] Update 3000	TrainingLoss=5.74	Speed 0.237 secs/step 4.2 steps/sec
I1208 12:39:43.535118 140042889676608 callbacks.py:238] {'step': 3000, 'lr': 0.0011410887, 'loss': 5.742501735687256, 'src_tokens_per_step': 2033.16, 'src_tokens_per_sec': 8592.911603274939, 'src_real_tokens_per_step': 1651.425, 'src_real_tokens_per_sec': 6979.553524778334, 'trg_tokens_per_step': 2033.16, 'trg_tokens_per_sec': 8592.911603274939, 'trg_real_tokens_per_step': 1689.757, 'trg_real_tokens_per_sec': 7141.559214235502, 'samples_per_step': 55.79, 'samples_per_sec': 235.78987307772576, 'this_step_loss': 325798.1875}
I1208 12:44:02.383861 140042889676608 callbacks.py:220] Update 4000	TrainingLoss=5.46	Speed 0.259 secs/step 3.9 steps/sec
I1208 12:44:02.390942 140042889676608 callbacks.py:238] {'step': 4000, 'lr': 0.0009882118, 'loss': 5.4556121826171875, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 7856.275177460593, 'src_real_tokens_per_step': 1650.945, 'src_real_tokens_per_sec': 6380.072084341406, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 7856.275177460593, 'trg_real_tokens_per_step': 1688.576, 'trg_real_tokens_per_sec': 6525.496972878486, 'samples_per_step': 56.014, 'samples_per_sec': 216.46593783093894, 'this_step_loss': 296262.53125}
I1208 12:44:03.161716 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-4000	Elapsed 0.76s
I1208 12:48:20.096929 140042889676608 callbacks.py:220] Update 5000	TrainingLoss=5.22	Speed 0.257 secs/step 3.9 steps/sec
I1208 12:48:20.300609 140042889676608 callbacks.py:238] {'step': 5000, 'lr': 0.0008838835, 'loss': 5.218696594238281, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 7914.865078316784, 'src_real_tokens_per_step': 1650.244, 'src_real_tokens_per_sec': 6424.721400050075, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 7914.865078316784, 'trg_real_tokens_per_step': 1689.368, 'trg_real_tokens_per_sec': 6577.038754365897, 'samples_per_step': 55.759, 'samples_per_sec': 217.0806502222654, 'this_step_loss': 568333.75}
I1208 12:52:47.838573 140042889676608 callbacks.py:220] Update 6000	TrainingLoss=5.02	Speed 0.267 secs/step 3.7 steps/sec
I1208 12:52:47.843958 140042889676608 callbacks.py:238] {'step': 6000, 'lr': 0.00080687157, 'loss': 5.017360687255859, 'src_tokens_per_step': 2032.928, 'src_tokens_per_sec': 7601.014186317548, 'src_real_tokens_per_step': 1651.805, 'src_real_tokens_per_sec': 6176.014712783855, 'trg_tokens_per_step': 2032.928, 'trg_tokens_per_sec': 7601.014186317548, 'trg_real_tokens_per_step': 1688.948, 'trg_real_tokens_per_sec': 6314.89049683641, 'samples_per_step': 55.921, 'samples_per_sec': 209.08576905481334, 'this_step_loss': 572328.0625}
I1208 12:52:48.752836 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-6000	Elapsed 0.90s
I1208 12:57:05.195035 140042889676608 callbacks.py:220] Update 7000	TrainingLoss=4.85	Speed 0.256 secs/step 3.9 steps/sec
I1208 12:57:05.212759 140042889676608 callbacks.py:238] {'step': 7000, 'lr': 0.0007470179, 'loss': 4.849551677703857, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 7930.378639488163, 'src_real_tokens_per_step': 1648.645, 'src_real_tokens_per_sec': 6430.823721692684, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 7930.378639488163, 'trg_real_tokens_per_step': 1690.892, 'trg_real_tokens_per_sec': 6595.615420190754, 'samples_per_step': 55.928, 'samples_per_sec': 218.15679488721247, 'this_step_loss': 1033339.9375}
I1208 13:01:31.259425 140042889676608 callbacks.py:220] Update 8000	TrainingLoss=4.71	Speed 0.266 secs/step 3.8 steps/sec
I1208 13:01:31.263159 140042889676608 callbacks.py:238] {'step': 8000, 'lr': 0.00069877127, 'loss': 4.706124305725098, 'src_tokens_per_step': 2032.984, 'src_tokens_per_sec': 7643.821677223802, 'src_real_tokens_per_step': 1651.918, 'src_real_tokens_per_sec': 6211.050661193688, 'trg_tokens_per_step': 2032.984, 'trg_tokens_per_sec': 7643.821677223802, 'trg_real_tokens_per_step': 1689.622, 'trg_real_tokens_per_sec': 6352.81402603967, 'samples_per_step': 55.829, 'samples_per_sec': 209.9115981324632, 'this_step_loss': 980653.5}
I1208 13:01:32.027984 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-8000	Elapsed 0.76s
I1208 13:04:56.795883 140042889676608 callbacks.py:220] Update 9000	TrainingLoss=4.58	Speed 0.205 secs/step 4.9 steps/sec
I1208 13:04:56.799146 140042889676608 callbacks.py:238] {'step': 9000, 'lr': 0.0006588078, 'loss': 4.5828166007995605, 'src_tokens_per_step': 2032.896, 'src_tokens_per_sec': 9931.3402980931, 'src_real_tokens_per_step': 1650.892, 'src_real_tokens_per_sec': 8065.129867636866, 'trg_tokens_per_step': 2032.896, 'trg_tokens_per_sec': 9931.3402980931, 'trg_real_tokens_per_step': 1686.96, 'trg_real_tokens_per_sec': 8241.333461854978, 'samples_per_step': 55.944, 'samples_per_sec': 273.30414425357736, 'this_step_loss': 554454.6875}
I1208 13:09:01.262569 140042889676608 callbacks.py:220] Update 10000	TrainingLoss=4.48	Speed 0.244 secs/step 4.1 steps/sec
I1208 13:09:01.266312 140042889676608 callbacks.py:238] {'step': 10000, 'lr': 0.000625, 'loss': 4.477119445800781, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 8318.899306846482, 'src_real_tokens_per_step': 1650.865, 'src_real_tokens_per_sec': 6755.175416325851, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 8318.899306846482, 'trg_real_tokens_per_step': 1687.753, 'trg_real_tokens_per_sec': 6906.117444146072, 'samples_per_step': 55.714, 'samples_per_sec': 227.97614774386668, 'this_step_loss': 503698.0}
I1208 13:09:01.983217 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-10000	Elapsed 0.71s
I1208 13:09:15.895008 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=101.52 (Best 101.52)  step=10000	Elapsed 13.91s  FromSTART 2460.64s
I1208 13:09:15.896589 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=13.39 (Best 13.39)  step=10000	Elapsed 13.91s  FromSTART 2460.64s
I1208 13:11:39.897799 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 13:11:39.910803 140042889676608 seq_generation_validator.py:179] Sample 1528
I1208 13:11:39.911040 140042889676608 seq_generation_validator.py:181]   Data: We turned the classical spatial configuration on its head and turned the reading area inside out.
I1208 13:11:39.911130 140042889676608 seq_generation_validator.py:182]   Reference: Wir haben die klassische Raumkonfiguration auf den Kopf gestellt und den Lesebereich von innen nach auen gestlpt.
I1208 13:11:39.911207 140042889676608 seq_generation_validator.py:183]   Hypothesis: Wir haben die klassische Konfiguration auf seinem Kopf und haben in diesem Bereich in den Vordergrund genommen.
I1208 13:11:39.911277 140042889676608 seq_generation_validator.py:179] Sample 1757
I1208 13:11:39.911358 140042889676608 seq_generation_validator.py:181]   Data: Condoms can reduce the risk of contraction, however, they do not offer 100% protection.
I1208 13:11:39.911430 140042889676608 seq_generation_validator.py:182]   Reference: Kondome knnen das Risiko einer Ansteckung mindern, bieten aber keinen hundertprozentigen Schutz.
I1208 13:11:39.911499 140042889676608 seq_generation_validator.py:183]   Hypothesis: Allerdings knnen die Gefahr des Verstoes reduziert werden, aber sie bieten nicht nur 100% Schutz.
I1208 13:11:39.911568 140042889676608 seq_generation_validator.py:179] Sample 665
I1208 13:11:39.911635 140042889676608 seq_generation_validator.py:181]   Data: An especially high number of guns and machine guns come from poor countries like Kyrgyzstan.
I1208 13:11:39.911701 140042889676608 seq_generation_validator.py:182]   Reference: Besonders viele Pistolen und Gewehre kommen aus armen Lndern wie Kirgisien.
I1208 13:11:39.911772 140042889676608 seq_generation_validator.py:183]   Hypothesis: Eine besonders hohe Anzahl von goppeln und Maschinenmaschine kommen aus armen Lndern wie Kirgisisin.
I1208 13:11:39.911837 140042889676608 seq_generation_validator.py:179] Sample 1083
I1208 13:11:39.911905 140042889676608 seq_generation_validator.py:181]   Data: Most primary road surfaces are dry and frost-free.
I1208 13:11:39.911972 140042889676608 seq_generation_validator.py:182]   Reference: Die Fahrbahnen der ersten Straenklasse sind berwiegend trocken, jedoch durchgefroren.
I1208 13:11:39.912039 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die meisten Grunddeckeln sind schlichtweg und schlichtweg.
I1208 13:11:39.912106 140042889676608 seq_generation_validator.py:179] Sample 99
I1208 13:11:39.912171 140042889676608 seq_generation_validator.py:181]   Data: While one study noted a decrease in the risk of prostate cancer, another noted an increase.
I1208 13:11:39.912242 140042889676608 seq_generation_validator.py:182]   Reference: Whrend eine Studie die Verringerung des Risikos von Prostatakrebs herausgefunden hat, zeigte eine andere eher eine Erhhung.
I1208 13:11:39.912318 140042889676608 seq_generation_validator.py:183]   Hypothesis: Whrend eine Studie ber die Gefahr einer Verringerung des Risikos von Kreaturen, eine weitere wurde eine Erhhung der Erhhung.
I1208 13:11:40.952734 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.04s
I1208 13:11:41.975833 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.02s
I1208 13:11:41.976110 140042889676608 training_utils.py:359] Evaluating bleu at step=10000 with bad count=0 (early_stop_patience=0).
I1208 13:11:41.976187 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=11.23 (Best 11.23)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:11:41.977523 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=11.13 (Best 11.13)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:11:41.978178 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=11.23 (Best 11.23)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:11:41.978676 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=38.26 (Best 38.26)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:11:41.979141 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=11.53 (Best 11.53)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:11:41.979628 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=11.42 (Best 11.42)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:11:41.980089 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=11.53 (Best 11.53)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:11:41.980569 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=39.61 (Best 39.61)  step=10000	Elapsed 133.12s  FromSTART 2585.74s
I1208 13:16:26.959267 140042889676608 callbacks.py:220] Update 11000	TrainingLoss=4.38	Speed 0.285 secs/step 3.5 steps/sec
I1208 13:16:26.964745 140042889676608 callbacks.py:238] {'step': 11000, 'lr': 0.00059591414, 'loss': 4.383492946624756, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 7135.792577808103, 'src_real_tokens_per_step': 1650.973, 'src_real_tokens_per_sec': 5794.907643296961, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 7135.792577808103, 'trg_real_tokens_per_step': 1690.334, 'trg_real_tokens_per_sec': 5933.064572421673, 'samples_per_step': 55.928, 'samples_per_sec': 196.30702299450837, 'this_step_loss': 852561.8125}
I1208 13:20:07.643798 140042889676608 callbacks.py:220] Update 12000	TrainingLoss=4.30	Speed 0.221 secs/step 4.5 steps/sec
I1208 13:20:07.647101 140042889676608 callbacks.py:238] {'step': 12000, 'lr': 0.00057054433, 'loss': 4.301530361175537, 'src_tokens_per_step': 2033.024, 'src_tokens_per_sec': 9216.0018581929, 'src_real_tokens_per_step': 1647.188, 'src_real_tokens_per_sec': 7466.949563208819, 'trg_tokens_per_step': 2033.024, 'trg_tokens_per_sec': 9216.0018581929, 'trg_real_tokens_per_step': 1688.254, 'trg_real_tokens_per_sec': 7653.107883183669, 'samples_per_step': 55.985, 'samples_per_sec': 253.78837831276437, 'this_step_loss': 807942.3125}
I1208 13:20:08.356673 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-12000	Elapsed 0.70s
I1208 13:24:58.791664 140042889676608 callbacks.py:220] Update 13000	TrainingLoss=4.23	Speed 0.290 secs/step 3.4 steps/sec
I1208 13:24:58.802329 140042889676608 callbacks.py:238] {'step': 13000, 'lr': 0.0005481613, 'loss': 4.227929592132568, 'src_tokens_per_step': 2033.032, 'src_tokens_per_sec': 7001.871119296168, 'src_real_tokens_per_step': 1652.124, 'src_real_tokens_per_sec': 5690.00356172262, 'trg_tokens_per_step': 2033.032, 'trg_tokens_per_sec': 7001.871119296168, 'trg_real_tokens_per_step': 1689.804, 'trg_real_tokens_per_sec': 5819.775500273061, 'samples_per_step': 55.956, 'samples_per_sec': 192.71546161168953, 'this_step_loss': 510847.46875}
I1208 13:28:57.800751 140042889676608 callbacks.py:220] Update 14000	TrainingLoss=4.16	Speed 0.239 secs/step 4.2 steps/sec
I1208 13:28:57.805877 140042889676608 callbacks.py:238] {'step': 14000, 'lr': 0.0005282214, 'loss': 4.161990642547607, 'src_tokens_per_step': 2033.152, 'src_tokens_per_sec': 8510.521885706787, 'src_real_tokens_per_step': 1654.684, 'src_real_tokens_per_sec': 6926.301819012473, 'trg_tokens_per_step': 2033.152, 'trg_tokens_per_sec': 8510.521885706787, 'trg_real_tokens_per_step': 1685.324, 'trg_real_tokens_per_sec': 7054.557055501459, 'samples_per_step': 55.951, 'samples_per_sec': 234.20394049592966, 'this_step_loss': 369910.59375}
I1208 13:28:58.752739 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-14000	Elapsed 0.94s
I1208 13:33:33.945238 140042889676608 callbacks.py:220] Update 15000	TrainingLoss=4.10	Speed 0.275 secs/step 3.6 steps/sec
I1208 13:33:33.950978 140042889676608 callbacks.py:238] {'step': 15000, 'lr': 0.00051031035, 'loss': 4.102343559265137, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 7389.393774771662, 'src_real_tokens_per_step': 1651.502, 'src_real_tokens_per_sec': 6002.942836283557, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 7389.393774771662, 'trg_real_tokens_per_step': 1689.692, 'trg_real_tokens_per_sec': 6141.757313600368, 'samples_per_step': 55.966, 'samples_per_sec': 203.42736416634403, 'this_step_loss': 845432.375}
I1208 13:38:07.458574 140042889676608 callbacks.py:220] Update 16000	TrainingLoss=4.05	Speed 0.273 secs/step 3.7 steps/sec
I1208 13:38:07.462239 140042889676608 callbacks.py:238] {'step': 16000, 'lr': 0.0004941059, 'loss': 4.04863166809082, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 7435.682710990549, 'src_real_tokens_per_step': 1651.122, 'src_real_tokens_per_sec': 6038.729075656707, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 7435.682710990549, 'trg_real_tokens_per_step': 1689.034, 'trg_real_tokens_per_sec': 6177.386483598881, 'samples_per_step': 55.867, 'samples_per_sec': 204.32510575821365, 'this_step_loss': 435440.28125}
I1208 13:38:08.116593 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-16000	Elapsed 0.65s
I1208 13:42:36.060811 140042889676608 callbacks.py:220] Update 17000	TrainingLoss=4.00	Speed 0.268 secs/step 3.7 steps/sec
I1208 13:42:36.065033 140042889676608 callbacks.py:238] {'step': 17000, 'lr': 0.00047935313, 'loss': 3.999528408050537, 'src_tokens_per_step': 2033.04, 'src_tokens_per_sec': 7589.700675437865, 'src_real_tokens_per_step': 1653.008, 'src_real_tokens_per_sec': 6170.973485078599, 'trg_tokens_per_step': 2033.04, 'trg_tokens_per_sec': 7589.700675437865, 'trg_real_tokens_per_step': 1687.93, 'trg_real_tokens_per_sec': 6301.343535342067, 'samples_per_step': 55.956, 'samples_per_sec': 208.89372122279994, 'this_step_loss': 382214.03125}
I1208 13:47:01.965706 140042889676608 callbacks.py:220] Update 18000	TrainingLoss=3.95	Speed 0.266 secs/step 3.8 steps/sec
I1208 13:47:01.968251 140042889676608 callbacks.py:238] {'step': 18000, 'lr': 0.0004658475, 'loss': 3.9538497924804688, 'src_tokens_per_step': 2032.856, 'src_tokens_per_sec': 7647.551146307181, 'src_real_tokens_per_step': 1651.538, 'src_real_tokens_per_sec': 6213.0427954906145, 'trg_tokens_per_step': 2032.856, 'trg_tokens_per_sec': 7647.551146307181, 'trg_real_tokens_per_step': 1689.164, 'trg_real_tokens_per_sec': 6354.590824190608, 'samples_per_step': 55.868, 'samples_per_sec': 210.17395597223293, 'this_step_loss': 431875.5625}
I1208 13:47:03.234522 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-18000	Elapsed 1.26s
I1208 13:51:20.748859 140042889676608 callbacks.py:220] Update 19000	TrainingLoss=3.91	Speed 0.257 secs/step 3.9 steps/sec
I1208 13:51:20.752621 140042889676608 callbacks.py:238] {'step': 19000, 'lr': 0.00045342266, 'loss': 3.9124395847320557, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 7896.967895313164, 'src_real_tokens_per_step': 1651.486, 'src_real_tokens_per_sec': 6415.01816112108, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 7896.967895313164, 'trg_real_tokens_per_step': 1688.274, 'trg_real_tokens_per_sec': 6557.91715518541, 'samples_per_step': 55.975, 'samples_per_sec': 217.42881354655896, 'this_step_loss': 208197.3125}
I1208 13:56:01.601657 140042889676608 callbacks.py:220] Update 20000	TrainingLoss=3.87	Speed 0.281 secs/step 3.6 steps/sec
I1208 13:56:01.607203 140042889676608 callbacks.py:238] {'step': 20000, 'lr': 0.00044194175, 'loss': 3.874399185180664, 'src_tokens_per_step': 2033.048, 'src_tokens_per_sec': 7241.001975020969, 'src_real_tokens_per_step': 1652.616, 'src_real_tokens_per_sec': 5886.0369848381615, 'trg_tokens_per_step': 2033.048, 'trg_tokens_per_sec': 7241.001975020969, 'trg_real_tokens_per_step': 1688.95, 'trg_real_tokens_per_sec': 6015.445914563585, 'samples_per_step': 55.698, 'samples_per_sec': 198.3766876161891, 'this_step_loss': 206231.375}
I1208 13:56:02.553228 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-20000	Elapsed 0.91s
I1208 13:56:06.718629 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=87.96 (Best 87.96)  step=20000	Elapsed 4.16s  FromSTART 5271.47s
I1208 13:56:06.721307 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=9.47 (Best 9.47)  step=20000	Elapsed 4.16s  FromSTART 5271.47s
I1208 13:58:43.186096 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 13:58:43.188793 140042889676608 seq_generation_validator.py:179] Sample 1000
I1208 13:58:43.188974 140042889676608 seq_generation_validator.py:181]   Data: NKU Head: Svarc System audit has failed because of politicians.
I1208 13:58:43.189032 140042889676608 seq_generation_validator.py:182]   Reference: NK-Chef: Kontrolle des Schwarz-Systems versagt durch Verschulden der Politik
I1208 13:58:43.189082 140042889676608 seq_generation_validator.py:183]   Hypothesis: NKU-Geschftsfhrer: Svarc System-Prfung ist wegen Politiker gescheitert.
I1208 13:58:43.189128 140042889676608 seq_generation_validator.py:179] Sample 2402
I1208 13:58:43.189173 140042889676608 seq_generation_validator.py:181]   Data: Our community voted against this amendment.
I1208 13:58:43.189218 140042889676608 seq_generation_validator.py:182]   Reference: Unsere Gemeinde hat gegen diese Gesetzesnderung gestimmt.
I1208 13:58:43.189263 140042889676608 seq_generation_validator.py:183]   Hypothesis: Unsere Gemeinschaft hat gegen diesen nderungsantrag gestimmt.
I1208 13:58:43.189317 140042889676608 seq_generation_validator.py:179] Sample 20
I1208 13:58:43.189362 140042889676608 seq_generation_validator.py:181]   Data: And it often costs over a hundred dollars to obtain the required identity card.
I1208 13:58:43.189406 140042889676608 seq_generation_validator.py:182]   Reference: Allerdings sind hufig mehr als hundert Dollar zu zahlen, um den erforderlichen Ausweis zu erhalten.
I1208 13:58:43.189451 140042889676608 seq_generation_validator.py:183]   Hypothesis: Und es kostet oft ber hundert Dollar, um die gewnschte Identifikation zu erhalten.
I1208 13:58:43.189505 140042889676608 seq_generation_validator.py:179] Sample 1327
I1208 13:58:43.189583 140042889676608 seq_generation_validator.py:181]   Data: In what light stand the words, spoken by David Rockefeller in 1994: "All we need is a major crisis and the nations will accept the New World Order."
I1208 13:58:43.189665 140042889676608 seq_generation_validator.py:182]   Reference: In welchem Licht erscheinen da die Worte David Rockefellers aus dem Jahr 1994: "Das einzige, was wir brauchen, ist eine alles umspannende Krise, und die Menschen werden die neue Weltordnung begren."
I1208 13:58:43.189754 140042889676608 seq_generation_validator.py:183]   Hypothesis: In welchem Licht die Worte, die von David Rockefeller 1994 gesprochen wurden: "All wir brauchen eine groe Krise und die Nationen werden die neue Welt akzeptieren".
I1208 13:58:43.189834 140042889676608 seq_generation_validator.py:179] Sample 2650
I1208 13:58:43.189910 140042889676608 seq_generation_validator.py:181]   Data: "They are very secret secrets, I'll tell my mother, but not everybody" he says.
I1208 13:58:43.189987 140042889676608 seq_generation_validator.py:182]   Reference: "Das sind sehr vertrauliche Geheimnisse, meine Mutter darf sie kennen, aber nicht die ganze Welt", besttigt er.
I1208 13:58:43.190081 140042889676608 seq_generation_validator.py:183]   Hypothesis: "Sie sind sehr Geheimhaltung, ich werde meine Mutter sagen, aber nicht alle", er sagt.
I1208 13:58:44.426296 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.24s
I1208 13:58:45.341072 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 0.91s
I1208 13:58:45.341307 140042889676608 training_utils.py:359] Evaluating bleu at step=20000 with bad count=0 (early_stop_patience=0).
I1208 13:58:45.341383 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=14.84 (Best 14.84)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 13:58:45.342654 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=14.89 (Best 14.89)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 13:58:45.343371 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=14.84 (Best 14.84)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 13:58:45.343912 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=43.33 (Best 43.33)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 13:58:45.344428 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=15.30 (Best 15.30)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 13:58:45.344916 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=15.36 (Best 15.36)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 13:58:45.345402 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=15.30 (Best 15.30)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 13:58:45.345869 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=44.64 (Best 44.64)  step=20000	Elapsed 145.91s  FromSTART 5409.35s
I1208 14:03:32.212820 140042889676608 callbacks.py:220] Update 21000	TrainingLoss=3.84	Speed 0.287 secs/step 3.5 steps/sec
I1208 14:03:32.217909 140042889676608 callbacks.py:238] {'step': 21000, 'lr': 0.00043129097, 'loss': 3.838003396987915, 'src_tokens_per_step': 2032.84, 'src_tokens_per_sec': 7088.204185583365, 'src_real_tokens_per_step': 1652.48, 'src_real_tokens_per_sec': 5761.946662104641, 'trg_tokens_per_step': 2032.84, 'trg_tokens_per_sec': 7088.204185583365, 'trg_real_tokens_per_step': 1688.708, 'trg_real_tokens_per_sec': 5888.268193182008, 'samples_per_step': 55.936, 'samples_per_sec': 195.04033240431667, 'this_step_loss': 449588.53125}
I1208 14:08:04.195752 140042889676608 callbacks.py:220] Update 22000	TrainingLoss=3.80	Speed 0.272 secs/step 3.7 steps/sec
I1208 14:08:04.200438 140042889676608 callbacks.py:238] {'step': 22000, 'lr': 0.0004213749, 'loss': 3.8049726486206055, 'src_tokens_per_step': 2033.152, 'src_tokens_per_sec': 7478.244346443433, 'src_real_tokens_per_step': 1651.616, 'src_real_tokens_per_sec': 6074.896522490949, 'trg_tokens_per_step': 2033.152, 'trg_tokens_per_sec': 7478.244346443433, 'trg_real_tokens_per_step': 1690.204, 'trg_real_tokens_per_sec': 6216.829094596014, 'samples_per_step': 56.121, 'samples_per_sec': 206.42163053561757, 'this_step_loss': 422118.90625}
I1208 14:08:05.192605 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-22000	Elapsed 0.99s
I1208 14:11:59.711799 140042889676608 callbacks.py:220] Update 23000	TrainingLoss=3.77	Speed 0.234 secs/step 4.3 steps/sec
I1208 14:11:59.713810 140042889676608 callbacks.py:238] {'step': 23000, 'lr': 0.00041211277, 'loss': 3.7742116451263428, 'src_tokens_per_step': 2032.936, 'src_tokens_per_sec': 8671.118328927832, 'src_real_tokens_per_step': 1650.164, 'src_real_tokens_per_sec': 7038.474062211928, 'trg_tokens_per_step': 2032.936, 'trg_tokens_per_sec': 8671.118328927832, 'trg_real_tokens_per_step': 1689.18, 'trg_real_tokens_per_sec': 7204.889705754788, 'samples_per_step': 55.675, 'samples_per_sec': 237.47157459116127, 'this_step_loss': 369718.375}
I1208 14:16:46.714616 140042889676608 callbacks.py:220] Update 24000	TrainingLoss=3.75	Speed 0.287 secs/step 3.5 steps/sec
I1208 14:16:46.720618 140042889676608 callbacks.py:238] {'step': 24000, 'lr': 0.00040343578, 'loss': 3.7450368404388428, 'src_tokens_per_step': 2033.032, 'src_tokens_per_sec': 7085.90406595033, 'src_real_tokens_per_step': 1652.608, 'src_real_tokens_per_sec': 5759.979059169774, 'trg_tokens_per_step': 2033.032, 'trg_tokens_per_sec': 7085.90406595033, 'trg_real_tokens_per_step': 1686.52, 'trg_real_tokens_per_sec': 5878.175515833765, 'samples_per_step': 55.998, 'samples_per_sec': 195.17472223018947, 'this_step_loss': 328431.03125}
I1208 14:16:47.465468 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-24000	Elapsed 0.71s
I1208 14:21:31.982300 140042889676608 callbacks.py:220] Update 25000	TrainingLoss=3.72	Speed 0.284 secs/step 3.5 steps/sec
I1208 14:21:31.984023 140042889676608 callbacks.py:238] {'step': 25000, 'lr': 0.00039528473, 'loss': 3.717869758605957, 'src_tokens_per_step': 2032.864, 'src_tokens_per_sec': 7146.8787291456365, 'src_real_tokens_per_step': 1654.028, 'src_real_tokens_per_sec': 5815.016415565084, 'trg_tokens_per_step': 2032.864, 'trg_tokens_per_sec': 7146.8787291456365, 'trg_real_tokens_per_step': 1686.928, 'trg_real_tokens_per_sec': 5930.681954523367, 'samples_per_step': 55.844, 'samples_per_sec': 196.32906861964642, 'this_step_loss': 708563.3125}
I1208 14:25:58.282150 140042889676608 callbacks.py:220] Update 26000	TrainingLoss=3.69	Speed 0.266 secs/step 3.8 steps/sec
I1208 14:25:58.283928 140042889676608 callbacks.py:238] {'step': 26000, 'lr': 0.00038760857, 'loss': 3.692357301712036, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 7636.549905497433, 'src_real_tokens_per_step': 1652.84, 'src_real_tokens_per_sec': 6208.580823634514, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 7636.549905497433, 'trg_real_tokens_per_step': 1688.652, 'trg_real_tokens_per_sec': 6343.101827758324, 'samples_per_step': 55.713, 'samples_per_sec': 209.27534632943883, 'this_step_loss': 382659.03125}
I1208 14:25:59.372197 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-26000	Elapsed 1.08s
I1208 14:30:52.955171 140042889676608 callbacks.py:220] Update 27000	TrainingLoss=3.67	Speed 0.294 secs/step 3.4 steps/sec
I1208 14:30:52.960124 140042889676608 callbacks.py:238] {'step': 27000, 'lr': 0.00038036288, 'loss': 3.667872905731201, 'src_tokens_per_step': 2033.024, 'src_tokens_per_sec': 6926.584095866381, 'src_real_tokens_per_step': 1653.868, 'src_real_tokens_per_sec': 5634.786301323713, 'trg_tokens_per_step': 2033.024, 'trg_tokens_per_sec': 6926.584095866381, 'trg_real_tokens_per_step': 1690.04, 'trg_real_tokens_per_sec': 5758.025574404443, 'samples_per_step': 55.955, 'samples_per_sec': 190.6406481596889, 'this_step_loss': 375173.4375}
I1208 14:35:34.799798 140042889676608 callbacks.py:220] Update 28000	TrainingLoss=3.64	Speed 0.282 secs/step 3.5 steps/sec
I1208 14:35:34.803602 140042889676608 callbacks.py:238] {'step': 28000, 'lr': 0.00037350896, 'loss': 3.6448333263397217, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 7215.787487540931, 'src_real_tokens_per_step': 1651.12, 'src_real_tokens_per_sec': 5860.323301158762, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 7215.787487540931, 'trg_real_tokens_per_step': 1688.58, 'trg_real_tokens_per_sec': 5993.280149153703, 'samples_per_step': 55.723, 'samples_per_sec': 197.7777480198106, 'this_step_loss': 366567.71875}
I1208 14:35:35.684698 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-28000	Elapsed 0.86s
I1208 14:40:07.633515 140042889676608 callbacks.py:220] Update 29000	TrainingLoss=3.62	Speed 0.272 secs/step 3.7 steps/sec
I1208 14:40:07.639591 140042889676608 callbacks.py:238] {'step': 29000, 'lr': 0.00036701263, 'loss': 3.6234374046325684, 'src_tokens_per_step': 2032.888, 'src_tokens_per_sec': 7477.337600583629, 'src_real_tokens_per_step': 1653.516, 'src_real_tokens_per_sec': 6081.937302973228, 'trg_tokens_per_step': 2032.888, 'trg_tokens_per_sec': 7477.337600583629, 'trg_real_tokens_per_step': 1685.924, 'trg_real_tokens_per_sec': 6201.139913721933, 'samples_per_step': 55.619, 'samples_per_sec': 204.57695653024706, 'this_step_loss': 349755.53125}
I1208 14:44:10.731968 140042889676608 callbacks.py:220] Update 30000	TrainingLoss=3.60	Speed 0.243 secs/step 4.1 steps/sec
I1208 14:44:10.737200 140042889676608 callbacks.py:238] {'step': 30000, 'lr': 0.00036084393, 'loss': 3.6025390625, 'src_tokens_per_step': 2032.96, 'src_tokens_per_sec': 8366.494799814009, 'src_real_tokens_per_step': 1651.564, 'src_real_tokens_per_sec': 6796.888093007253, 'trg_tokens_per_step': 2032.96, 'trg_tokens_per_sec': 8366.494799814009, 'trg_real_tokens_per_step': 1686.668, 'trg_real_tokens_per_sec': 6941.355978972875, 'samples_per_step': 55.93, 'samples_per_sec': 230.1757310294337, 'this_step_loss': 830946.4375}
I1208 14:44:11.668478 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-30000	Elapsed 0.90s
I1208 14:44:19.926053 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=82.90 (Best 82.90)  step=30000	Elapsed 8.26s  FromSTART 8164.68s
I1208 14:44:19.927153 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=8.32 (Best 8.32)  step=30000	Elapsed 8.26s  FromSTART 8164.68s
I1208 14:46:33.574797 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 14:46:33.575526 140042889676608 seq_generation_validator.py:179] Sample 1762
I1208 14:46:33.575618 140042889676608 seq_generation_validator.py:181]   Data: "In total, we have already sent around 200,000 euros to Esitjeni," said Annette Lennartz, Chairperson of the association.
I1208 14:46:33.575681 140042889676608 seq_generation_validator.py:182]   Reference: "Insgesamt haben wir schon rund 200 000 Euro nach Esitjeni geschickt", sagt Annette Lennartz, die Vorsitzende des Vereins.
I1208 14:46:33.575729 140042889676608 seq_generation_validator.py:183]   Hypothesis: "Insgesamt haben wir bereits rund 200.000 Euro an Estatjeni", sagte Annette Lennartz, Person der Verein.
I1208 14:46:33.575775 140042889676608 seq_generation_validator.py:179] Sample 2454
I1208 14:46:33.575820 140042889676608 seq_generation_validator.py:181]   Data: Most Tory freshers pretend they're Labour.
I1208 14:46:33.575864 140042889676608 seq_generation_validator.py:182]   Reference: Die meisten Tory-Erstsemester geben vor, Anhnger der Labour-Partei zu sein.
I1208 14:46:33.575910 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die meisten Tory-Gensse sind die Labour.
I1208 14:46:33.575954 140042889676608 seq_generation_validator.py:179] Sample 547
I1208 14:46:33.575998 140042889676608 seq_generation_validator.py:181]   Data: Incidentally, it is the most unusual sea in the world, located in the lowest point of the planet - 417 m below sea level.
I1208 14:46:33.576041 140042889676608 seq_generation_validator.py:182]   Reference: brigens ist dieses ungewhnliche Meer mit einer Lage von 417 Metern unter dem Meeresspiegel das am tiefsten gelegene Meer der Welt.
I1208 14:46:33.576086 140042889676608 seq_generation_validator.py:183]   Hypothesis: Im brigen ist es das ungewhnlichste Meer in der Welt, das sich auf dem kleinsten Punkt des Planeten befindet - 417 m unter Meeresspiegel.
I1208 14:46:33.576130 140042889676608 seq_generation_validator.py:179] Sample 643
I1208 14:46:33.576172 140042889676608 seq_generation_validator.py:181]   Data: The idea was supported by the head of the Duma Committee on Safety and Anti-Corruption, Irina Yarovaya, who promised that the amendments to the law on weapons will be brought to the State Duma in the near future.
I1208 14:46:33.576217 140042889676608 seq_generation_validator.py:182]   Reference: Den Vorschlag hat die Vorsitzende des Duma-Ausschusses fr Sicherheit und Korruptionsbekmpfung, Irina Jarowaja, untersttzt und versprochen, dass die nderungen am Waffengesetz in Krze der Duma vorgelegt werden.
I1208 14:46:33.576269 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Idee wurde von dem Vorsitzenden des Duma-Ausschusses fr Sicherheit und AntiCorruptionsbekmpfung untersttzt, IVIS Yarovaya, der zugesagt hat, dass die nderungsantrge zum Waffenrecht in der nchsten Zukunft auf dem Staat Duma gebracht werden.
I1208 14:46:33.576324 140042889676608 seq_generation_validator.py:179] Sample 1343
I1208 14:46:33.576369 140042889676608 seq_generation_validator.py:181]   Data: The film Argo, directed by the actor Ben Affleck, recounts one episode in this story, which brought America a small victory.
I1208 14:46:33.576413 140042889676608 seq_generation_validator.py:182]   Reference: Der Film Argo des Regisseurs und Schauspielers Ben Affleck beschreibt eine Episode dieser Geschichte, die den USA einen kleinen Teilsieg bescherte.
I1208 14:46:33.576457 140042889676608 seq_generation_validator.py:183]   Hypothesis: Der Film Argo, der von dem Schauspieler Ben Affleck geleitet wurde, ist eine Episode in dieser Geschichte, die Amerika einen kleinen Sieg brachte.
I1208 14:46:34.502459 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.93s
I1208 14:46:35.798449 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.30s
I1208 14:46:35.798679 140042889676608 training_utils.py:359] Evaluating bleu at step=30000 with bad count=0 (early_stop_patience=0).
I1208 14:46:35.798745 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=15.98 (Best 15.98)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:46:35.799950 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=16.09 (Best 16.09)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:46:35.800656 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=15.98 (Best 15.98)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:46:35.801225 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=44.71 (Best 44.71)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:46:35.801752 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=16.43 (Best 16.43)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:46:35.802231 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=16.55 (Best 16.55)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:46:35.802729 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=16.43 (Best 16.43)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:46:35.803305 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=46.03 (Best 46.03)  step=30000	Elapsed 122.74s  FromSTART 8279.39s
I1208 14:50:42.628760 140042889676608 callbacks.py:220] Update 31000	TrainingLoss=3.58	Speed 0.247 secs/step 4.1 steps/sec
I1208 14:50:42.632327 140042889676608 callbacks.py:238] {'step': 31000, 'lr': 0.00035497616, 'loss': 3.5827057361602783, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 8239.058911775392, 'src_real_tokens_per_step': 1651.36, 'src_real_tokens_per_sec': 6692.4278720966, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 8239.058911775392, 'trg_real_tokens_per_step': 1689.58, 'trg_real_tokens_per_sec': 6847.321168089922, 'samples_per_step': 56.063, 'samples_per_sec': 227.20520285906872, 'this_step_loss': 669997.0625}
I1208 14:55:27.748916 140042889676608 callbacks.py:220] Update 32000	TrainingLoss=3.56	Speed 0.285 secs/step 3.5 steps/sec
I1208 14:55:27.751286 140042889676608 callbacks.py:238] {'step': 32000, 'lr': 0.00034938563, 'loss': 3.5641210079193115, 'src_tokens_per_step': 2032.968, 'src_tokens_per_sec': 7132.9178469787075, 'src_real_tokens_per_step': 1650.292, 'src_real_tokens_per_sec': 5790.25211391728, 'trg_tokens_per_step': 2032.968, 'trg_tokens_per_sec': 7132.9178469787075, 'trg_real_tokens_per_step': 1689.66, 'trg_real_tokens_per_sec': 5928.37957573658, 'samples_per_step': 55.93, 'samples_per_sec': 196.23727239263928, 'this_step_loss': 426031.78125}
I1208 14:55:28.765573 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-32000	Elapsed 0.99s
I1208 14:59:56.414168 140042889676608 callbacks.py:220] Update 33000	TrainingLoss=3.55	Speed 0.268 secs/step 3.7 steps/sec
I1208 14:59:56.420716 140042889676608 callbacks.py:238] {'step': 33000, 'lr': 0.00034405116, 'loss': 3.546617269515991, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 7597.825545007606, 'src_real_tokens_per_step': 1652.38, 'src_real_tokens_per_sec': 6175.378444214079, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 7597.825545007606, 'trg_real_tokens_per_step': 1686.908, 'trg_real_tokens_per_sec': 6304.418657071789, 'samples_per_step': 55.821, 'samples_per_sec': 208.61775144608023, 'this_step_loss': 375263.1875}
I1208 15:03:43.785173 140042889676608 callbacks.py:220] Update 34000	TrainingLoss=3.53	Speed 0.227 secs/step 4.4 steps/sec
I1208 15:03:43.790647 140042889676608 callbacks.py:238] {'step': 34000, 'lr': 0.00033895386, 'loss': 3.5295443534851074, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 8945.096411856479, 'src_real_tokens_per_step': 1650.52, 'src_real_tokens_per_sec': 7261.860989199406, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 8945.096411856479, 'trg_real_tokens_per_step': 1688.272, 'trg_real_tokens_per_sec': 7427.96002227035, 'samples_per_step': 55.93, 'samples_per_sec': 246.07753018801515, 'this_step_loss': 411403.53125}
I1208 15:03:45.096062 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-34000	Elapsed 1.27s
I1208 15:07:42.206030 140042889676608 callbacks.py:220] Update 35000	TrainingLoss=3.51	Speed 0.237 secs/step 4.2 steps/sec
I1208 15:07:42.208217 140042889676608 callbacks.py:238] {'step': 35000, 'lr': 0.00033407655, 'loss': 3.5131008625030518, 'src_tokens_per_step': 2032.976, 'src_tokens_per_sec': 8576.616159361847, 'src_real_tokens_per_step': 1652.0, 'src_real_tokens_per_sec': 6969.373910594995, 'trg_tokens_per_step': 2032.976, 'trg_tokens_per_sec': 8576.616159361847, 'trg_real_tokens_per_step': 1689.452, 'trg_real_tokens_per_sec': 7127.374510897419, 'samples_per_step': 56.197, 'samples_per_sec': 237.08105669110589, 'this_step_loss': 471751.90625}
I1208 15:11:34.431014 140042889676608 callbacks.py:220] Update 36000	TrainingLoss=3.50	Speed 0.232 secs/step 4.3 steps/sec
I1208 15:11:34.433817 140042889676608 callbacks.py:238] {'step': 36000, 'lr': 0.0003294039, 'loss': 3.4978623390197754, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 8757.778596738817, 'src_real_tokens_per_step': 1650.252, 'src_real_tokens_per_sec': 7109.000746104966, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 8757.778596738817, 'trg_real_tokens_per_step': 1685.516, 'trg_real_tokens_per_sec': 7260.911970760743, 'samples_per_step': 55.694, 'samples_per_sec': 239.92013798714984, 'this_step_loss': 174173.953125}
I1208 15:11:40.153680 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-36000	Elapsed 5.69s
I1208 15:15:16.870235 140042889676608 callbacks.py:220] Update 37000	TrainingLoss=3.48	Speed 0.217 secs/step 4.6 steps/sec
I1208 15:15:16.875346 140042889676608 callbacks.py:238] {'step': 37000, 'lr': 0.00032492203, 'loss': 3.4829442501068115, 'src_tokens_per_step': 2033.096, 'src_tokens_per_sec': 9384.461999376617, 'src_real_tokens_per_step': 1652.804, 'src_real_tokens_per_sec': 7629.09195159386, 'trg_tokens_per_step': 2033.096, 'trg_tokens_per_sec': 9384.461999376617, 'trg_real_tokens_per_step': 1685.672, 'trg_real_tokens_per_sec': 7780.805641943706, 'samples_per_step': 56.036, 'samples_per_sec': 258.653655605573, 'this_step_loss': 150429.1875}
I1208 15:19:05.641808 140042889676608 callbacks.py:220] Update 38000	TrainingLoss=3.47	Speed 0.229 secs/step 4.4 steps/sec
I1208 15:19:05.651614 140042889676608 callbacks.py:238] {'step': 38000, 'lr': 0.00032061824, 'loss': 3.4687795639038086, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 8890.557710642164, 'src_real_tokens_per_step': 1650.216, 'src_real_tokens_per_sec': 7216.596450086114, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 8890.557710642164, 'trg_real_tokens_per_step': 1688.944, 'trg_real_tokens_per_sec': 7385.958731944328, 'samples_per_step': 56.013, 'samples_per_sec': 244.95170144918816, 'this_step_loss': 347120.5625}
I1208 15:19:06.654819 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-38000	Elapsed 0.97s
I1208 15:22:34.487236 140042889676608 callbacks.py:220] Update 39000	TrainingLoss=3.46	Speed 0.208 secs/step 4.8 steps/sec
I1208 15:22:34.490612 140042889676608 callbacks.py:238] {'step': 39000, 'lr': 0.00031648105, 'loss': 3.4551870822906494, 'src_tokens_per_step': 2033.04, 'src_tokens_per_sec': 9785.678972472035, 'src_real_tokens_per_step': 1651.396, 'src_real_tokens_per_sec': 7948.702982934142, 'trg_tokens_per_step': 2033.04, 'trg_tokens_per_sec': 9785.678972472035, 'trg_real_tokens_per_step': 1687.732, 'trg_real_tokens_per_sec': 8123.5999014127465, 'samples_per_step': 56.055, 'samples_per_sec': 269.8108422863888, 'this_step_loss': 422402.4375}
I1208 15:26:21.121043 140042889676608 callbacks.py:220] Update 40000	TrainingLoss=3.44	Speed 0.227 secs/step 4.4 steps/sec
I1208 15:26:21.127663 140042889676608 callbacks.py:238] {'step': 40000, 'lr': 0.0003125, 'loss': 3.442095994949341, 'src_tokens_per_step': 2033.128, 'src_tokens_per_sec': 8974.275215651842, 'src_real_tokens_per_step': 1651.392, 'src_real_tokens_per_sec': 7289.283457276534, 'trg_tokens_per_step': 2033.128, 'trg_tokens_per_sec': 8974.275215651842, 'trg_real_tokens_per_step': 1688.524, 'trg_real_tokens_per_sec': 7453.184986008411, 'samples_per_step': 55.937, 'samples_per_sec': 246.90724476664383, 'this_step_loss': 376109.0}
I1208 15:26:22.049698 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-40000	Elapsed 0.91s
I1208 15:26:27.581191 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=79.67 (Best 79.67)  step=40000	Elapsed 5.53s  FromSTART 10692.33s
I1208 15:26:27.585556 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.66 (Best 7.66)  step=40000	Elapsed 5.53s  FromSTART 10692.33s
I1208 15:28:50.125663 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 15:28:50.177519 140042889676608 seq_generation_validator.py:179] Sample 1399
I1208 15:28:50.177645 140042889676608 seq_generation_validator.py:181]   Data: The Czech Republic is further away from a port, so according to Palas the EU should be paying us hundreds of millions of Euros.
I1208 15:28:50.177700 140042889676608 seq_generation_validator.py:182]   Reference: Da Tschechien einen weiteren Weg zu Seehfen habe, sollte die EU laut Palas Hunderte Millionen Euro an uns zahlen.
I1208 15:28:50.177764 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Tschechische Republik ist weiter von einem Hafen entfernt, so dass nach Palas die EU hunderte von Millionen Euro zahlen sollte.
I1208 15:28:50.177814 140042889676608 seq_generation_validator.py:179] Sample 874
I1208 15:28:50.177859 140042889676608 seq_generation_validator.py:181]   Data: We gave them Trakhtenberg, and a great career was on its way, but the basic education and mentoring he received from us.
I1208 15:28:50.177904 140042889676608 seq_generation_validator.py:182]   Reference: Wir gaben ihnen Trachtenberg, und es begann seine groe Karriere, aber die Grundlagen lernte er bei uns.
I1208 15:28:50.177949 140042889676608 seq_generation_validator.py:183]   Hypothesis: Wir gaben sie Trakhtenberg, und eine groe Karriere war auf dem Weg, aber die Grundausbildung und die von uns erhalten.
I1208 15:28:50.177994 140042889676608 seq_generation_validator.py:179] Sample 2197
I1208 15:28:50.178038 140042889676608 seq_generation_validator.py:181]   Data: Too many hard feelings.
I1208 15:28:50.178081 140042889676608 seq_generation_validator.py:182]   Reference: Zu viele verletzte Gefhle.
I1208 15:28:50.178132 140042889676608 seq_generation_validator.py:183]   Hypothesis: Zu viele harte Gefhle.
I1208 15:28:50.178177 140042889676608 seq_generation_validator.py:179] Sample 2189
I1208 15:28:50.178221 140042889676608 seq_generation_validator.py:181]   Data: You get the best of both worlds, but you're rooted in neither.
I1208 15:28:50.178265 140042889676608 seq_generation_validator.py:182]   Reference: Man geniet das Beste beider Welten, doch ist in keiner davon verwurzelt.
I1208 15:28:50.178315 140042889676608 seq_generation_validator.py:183]   Hypothesis: Sie erhalten das Beste beider Welten, aber Sie sind in keinem Fall.
I1208 15:28:50.178359 140042889676608 seq_generation_validator.py:179] Sample 2580
I1208 15:28:50.178402 140042889676608 seq_generation_validator.py:181]   Data: Some 400 porters carry sulphur baskets on their shoulders from the crater.
I1208 15:28:50.178445 140042889676608 seq_generation_validator.py:182]   Reference: Ungefhr 400 Trger laden sich am Kraterboden die Krbe mit Schwefel auf ihre Schultern.
I1208 15:28:50.178490 140042889676608 seq_generation_validator.py:183]   Hypothesis: Einige 400 Porter tragen Schwefelbasken auf ihren Schultern aus dem Krater.
I1208 15:28:51.799101 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.62s
I1208 15:28:53.161232 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.36s
I1208 15:28:53.161502 140042889676608 training_utils.py:359] Evaluating bleu at step=40000 with bad count=0 (early_stop_patience=0).
I1208 15:28:53.161577 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=16.77 (Best 16.77)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:28:53.163015 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=16.89 (Best 16.89)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:28:53.163770 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=16.77 (Best 16.77)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:28:53.164312 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=45.64 (Best 45.64)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:28:53.164824 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=17.20 (Best 17.20)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:28:53.165307 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=17.34 (Best 17.34)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:28:53.165785 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=17.20 (Best 17.20)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:28:53.166245 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=46.94 (Best 46.94)  step=40000	Elapsed 131.79s  FromSTART 10816.11s
I1208 15:33:12.323588 140042889676608 callbacks.py:220] Update 41000	TrainingLoss=3.43	Speed 0.259 secs/step 3.9 steps/sec
I1208 15:33:12.329092 140042889676608 callbacks.py:238] {'step': 41000, 'lr': 0.00030866548, 'loss': 3.429246664047241, 'src_tokens_per_step': 2032.984, 'src_tokens_per_sec': 7846.798502372401, 'src_real_tokens_per_step': 1652.716, 'src_real_tokens_per_sec': 6379.0612388719765, 'trg_tokens_per_step': 2032.984, 'trg_tokens_per_sec': 7846.798502372401, 'trg_real_tokens_per_step': 1688.304, 'trg_real_tokens_per_sec': 6516.421820707558, 'samples_per_step': 55.985, 'samples_per_sec': 216.08778728967806, 'this_step_loss': 395817.4375}
I1208 15:37:21.649750 140042889676608 callbacks.py:220] Update 42000	TrainingLoss=3.42	Speed 0.249 secs/step 4.0 steps/sec
I1208 15:37:21.651583 140042889676608 callbacks.py:238] {'step': 42000, 'lr': 0.00030496877, 'loss': 3.4171221256256104, 'src_tokens_per_step': 2033.072, 'src_tokens_per_sec': 8157.025403038739, 'src_real_tokens_per_step': 1650.856, 'src_real_tokens_per_sec': 6623.510789956736, 'trg_tokens_per_step': 2033.072, 'trg_tokens_per_sec': 8157.025403038739, 'trg_real_tokens_per_step': 1687.32, 'trg_real_tokens_per_sec': 6769.810465667387, 'samples_per_step': 55.921, 'samples_per_sec': 224.36441875316237, 'this_step_loss': 358401.40625}
I1208 15:37:22.651812 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-42000	Elapsed 0.99s
I1208 15:41:53.664045 140042889676608 callbacks.py:220] Update 43000	TrainingLoss=3.41	Speed 0.271 secs/step 3.7 steps/sec
I1208 15:41:53.671015 140042889676608 callbacks.py:238] {'step': 43000, 'lr': 0.00030140177, 'loss': 3.40574312210083, 'src_tokens_per_step': 2033.144, 'src_tokens_per_sec': 7504.0601120301735, 'src_real_tokens_per_step': 1650.016, 'src_real_tokens_per_sec': 6089.9863707693985, 'trg_tokens_per_step': 2033.144, 'trg_tokens_per_sec': 7504.0601120301735, 'trg_real_tokens_per_step': 1690.32, 'trg_real_tokens_per_sec': 6238.742995364245, 'samples_per_step': 56.01, 'samples_per_sec': 206.72535092192683, 'this_step_loss': 384470.40625}
I1208 15:46:10.764831 140042889676608 callbacks.py:220] Update 44000	TrainingLoss=3.39	Speed 0.257 secs/step 3.9 steps/sec
I1208 15:46:10.768531 140042889676608 callbacks.py:238] {'step': 44000, 'lr': 0.00029795707, 'loss': 3.394343137741089, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 7910.900825921335, 'src_real_tokens_per_step': 1652.96, 'src_real_tokens_per_sec': 6432.021503625613, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 7910.900825921335, 'trg_real_tokens_per_step': 1687.536, 'trg_real_tokens_per_sec': 6566.564127469723, 'samples_per_step': 56.15, 'samples_per_sec': 218.49168003374442, 'this_step_loss': 352614.84375}
I1208 15:46:11.618640 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-44000	Elapsed 0.82s
I1208 15:49:51.949912 140042889676608 callbacks.py:220] Update 45000	TrainingLoss=3.38	Speed 0.220 secs/step 4.5 steps/sec
I1208 15:49:51.951515 140042889676608 callbacks.py:238] {'step': 45000, 'lr': 0.00029462783, 'loss': 3.383735179901123, 'src_tokens_per_step': 2032.88, 'src_tokens_per_sec': 9229.542774802858, 'src_real_tokens_per_step': 1653.104, 'src_real_tokens_per_sec': 7505.309747352378, 'trg_tokens_per_step': 2032.88, 'trg_tokens_per_sec': 9229.542774802858, 'trg_real_tokens_per_step': 1691.744, 'trg_real_tokens_per_sec': 7680.740433284839, 'samples_per_step': 55.661, 'samples_per_sec': 252.7082662962407, 'this_step_loss': 311554.96875}
I1208 15:53:09.756117 140042889676608 callbacks.py:220] Update 46000	TrainingLoss=3.37	Speed 0.198 secs/step 5.1 steps/sec
I1208 15:53:09.758614 140042889676608 callbacks.py:238] {'step': 46000, 'lr': 0.00029140775, 'loss': 3.3732051849365234, 'src_tokens_per_step': 2033.112, 'src_tokens_per_sec': 10282.399564850019, 'src_real_tokens_per_step': 1649.384, 'src_real_tokens_per_sec': 8341.707354966466, 'trg_tokens_per_step': 2033.112, 'trg_tokens_per_sec': 10282.399564850019, 'trg_real_tokens_per_step': 1688.368, 'trg_real_tokens_per_sec': 8538.867700602177, 'samples_per_step': 56.166, 'samples_per_sec': 284.05776659592095, 'this_step_loss': 150396.546875}
I1208 15:53:10.422532 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-46000	Elapsed 0.64s
I1208 15:56:44.797137 140042889676608 callbacks.py:220] Update 47000	TrainingLoss=3.36	Speed 0.214 secs/step 4.7 steps/sec
I1208 15:56:44.798808 140042889676608 callbacks.py:238] {'step': 47000, 'lr': 0.00028829102, 'loss': 3.3629913330078125, 'src_tokens_per_step': 2033.088, 'src_tokens_per_sec': 9486.97656510431, 'src_real_tokens_per_step': 1653.256, 'src_real_tokens_per_sec': 7714.5706079215915, 'trg_tokens_per_step': 2033.088, 'trg_tokens_per_sec': 9486.97656510431, 'trg_real_tokens_per_step': 1687.68, 'trg_real_tokens_per_sec': 7875.202947140136, 'samples_per_step': 56.059, 'samples_per_sec': 261.58750593342864, 'this_step_loss': 180652.25}
I1208 16:00:49.472751 140042889676608 callbacks.py:220] Update 48000	TrainingLoss=3.35	Speed 0.245 secs/step 4.1 steps/sec
I1208 16:00:49.474471 140042889676608 callbacks.py:238] {'step': 48000, 'lr': 0.00028527217, 'loss': 3.3532845973968506, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 8311.3606542783, 'src_real_tokens_per_step': 1652.28, 'src_real_tokens_per_sec': 6755.157596880817, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 8311.3606542783, 'trg_real_tokens_per_step': 1691.552, 'trg_real_tokens_per_sec': 6915.716672306716, 'samples_per_step': 55.69, 'samples_per_sec': 227.68218859412008, 'this_step_loss': 305055.4375}
I1208 16:00:50.575753 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-48000	Elapsed 1.10s
I1208 16:04:42.376609 140042889676608 callbacks.py:220] Update 49000	TrainingLoss=3.34	Speed 0.232 secs/step 4.3 steps/sec
I1208 16:04:42.379640 140042889676608 callbacks.py:238] {'step': 49000, 'lr': 0.0002823462, 'loss': 3.3437299728393555, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 8773.161052082476, 'src_real_tokens_per_step': 1648.272, 'src_real_tokens_per_sec': 7112.942753162868, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 8773.161052082476, 'trg_real_tokens_per_step': 1690.448, 'trg_real_tokens_per_sec': 7294.948801653286, 'samples_per_step': 55.951, 'samples_per_sec': 241.45059794877037, 'this_step_loss': 334636.96875}
I1208 16:09:08.825235 140042889676608 callbacks.py:220] Update 50000	TrainingLoss=3.33	Speed 0.266 secs/step 3.8 steps/sec
I1208 16:09:08.826764 140042889676608 callbacks.py:238] {'step': 50000, 'lr': 0.0002795085, 'loss': 3.3342630863189697, 'src_tokens_per_step': 2033.08, 'src_tokens_per_sec': 7632.887898694686, 'src_real_tokens_per_step': 1650.904, 'src_real_tokens_per_sec': 6198.06656083708, 'trg_tokens_per_step': 2033.08, 'trg_tokens_per_sec': 7632.887898694686, 'trg_real_tokens_per_step': 1688.304, 'trg_real_tokens_per_sec': 6338.479140475452, 'samples_per_step': 56.031, 'samples_per_sec': 210.35981951116628, 'this_step_loss': 406438.6875}
I1208 16:09:10.418265 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-50000	Elapsed 1.59s
I1208 16:09:17.605266 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=77.73 (Best 77.73)  step=50000	Elapsed 7.19s  FromSTART 13262.35s
I1208 16:09:17.606450 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.29 (Best 7.29)  step=50000	Elapsed 7.19s  FromSTART 13262.35s
I1208 16:11:55.720458 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 16:11:55.721281 140042889676608 seq_generation_validator.py:179] Sample 2880
I1208 16:11:55.721396 140042889676608 seq_generation_validator.py:181]   Data: I remember that Gene Bartow, who had directed here and was at the University of Alabama (Birmingham), said to me 'I've got a very strong player for you, 6'7" tall.
I1208 16:11:55.721452 140042889676608 seq_generation_validator.py:182]   Reference: Ich wei noch, dass mir Gene Bartow, der hier als Trainer arbeitete und an der Universitt von Alabama (Birmingham) angestellt war, sagte: "Ich habe einen sehr krftigen und 2 m groen Spieler.
I1208 16:11:55.721505 140042889676608 seq_generation_validator.py:183]   Hypothesis: Ich erinnere mich daran, dass mich die Band Bartow, die hier und an der University von Alabama (Cambridge), sagte zu mir "Ich habe einen sehr starken Spieler fr Sie, 6 '7" hoch.
I1208 16:11:55.721553 140042889676608 seq_generation_validator.py:179] Sample 2006
I1208 16:11:55.721600 140042889676608 seq_generation_validator.py:181]   Data: Now you have the nose-numbing smell of rakfisk, one of the great Norwegian delicacies.
I1208 16:11:55.721644 140042889676608 seq_generation_validator.py:182]   Reference: So ungefhr muss man sich den nasenbetubenden Geruch von Rakfisk vorstellen, einer der groen Delikatessen Norwegens.
I1208 16:11:55.721691 140042889676608 seq_generation_validator.py:183]   Hypothesis: Jetzt haben Sie die Nase-Verlechtung von rakfisk, eines der groen norwegischen Spezialitten.
I1208 16:11:55.721742 140042889676608 seq_generation_validator.py:179] Sample 2780
I1208 16:11:55.721787 140042889676608 seq_generation_validator.py:181]   Data: Deaths caused by AIDS are nowadays due to late detection
I1208 16:11:55.721831 140042889676608 seq_generation_validator.py:182]   Reference: Ursache fr ein Sterben wegen Aids ist heute eine zu spte Erkennung
I1208 16:11:55.721875 140042889676608 seq_generation_validator.py:183]   Hypothesis: bergriffe, die von AIDS verursacht werden, sind heute aufgrund einer spten Kennzeichnung zurckzufhren.
I1208 16:11:55.721919 140042889676608 seq_generation_validator.py:179] Sample 1059
I1208 16:11:55.721965 140042889676608 seq_generation_validator.py:181]   Data: Snowing in the region has stopped, and only a thin layer of snow remains in the lowlands.
I1208 16:11:55.722008 140042889676608 seq_generation_validator.py:182]   Reference: In der Region schneit es jetzt nicht mehr, und in den tieferen Hhenlagen bleibt lediglich eine dnne Schneedecke.
I1208 16:11:55.722052 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Haut in der Region hat sich eingestellt, und nur eine dnne Schnee bleibt in den Tiefland.
I1208 16:11:55.722096 140042889676608 seq_generation_validator.py:179] Sample 1754
I1208 16:11:55.722141 140042889676608 seq_generation_validator.py:181]   Data: The infectious diseases can cause ulcers in the genital area, discomfort when urinating, discharge, lower abdominal pain and blisters or warts.
I1208 16:11:55.722185 140042889676608 seq_generation_validator.py:182]   Reference: Die Infektionskrankheiten knnen Geschwre im Genitalbereich, Beschwerden beim Urinieren, Ausfluss, Unterbauchschmerzen sowie Blschen oder Warzen hervorrufen.
I1208 16:11:55.722229 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Infektionskrankheiten knnen die Krebserkrankungen im Genital-Bereich verursachen, wenn es darum geht, die Entlastung, die unterdrcklichen Schmerz und die rzten oder die Schwrme zu stoppen.
I1208 16:11:56.646480 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.92s
I1208 16:11:57.963437 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.32s
I1208 16:11:57.963679 140042889676608 training_utils.py:359] Evaluating bleu at step=50000 with bad count=0 (early_stop_patience=0).
I1208 16:11:57.963752 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.40 (Best 17.40)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:11:57.965106 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.53 (Best 17.53)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:11:57.965755 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.40 (Best 17.40)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:11:57.966266 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.15 (Best 46.15)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:11:57.966776 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=17.83 (Best 17.83)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:11:57.967256 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=17.96 (Best 17.96)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:11:57.967751 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=17.83 (Best 17.83)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:11:57.968213 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=47.44 (Best 47.44)  step=50000	Elapsed 147.47s  FromSTART 13401.80s
I1208 16:16:28.150359 140042889676608 callbacks.py:220] Update 51000	TrainingLoss=3.33	Speed 0.270 secs/step 3.7 steps/sec
I1208 16:16:28.152255 140042889676608 callbacks.py:238] {'step': 51000, 'lr': 0.00027675464, 'loss': 3.3254051208496094, 'src_tokens_per_step': 2032.952, 'src_tokens_per_sec': 7526.390319466932, 'src_real_tokens_per_step': 1652.832, 'src_real_tokens_per_sec': 6119.110910884845, 'trg_tokens_per_step': 2032.952, 'trg_tokens_per_sec': 7526.390319466932, 'trg_real_tokens_per_step': 1687.072, 'trg_real_tokens_per_sec': 6245.874161831522, 'samples_per_step': 55.892, 'samples_per_sec': 206.92323662125114, 'this_step_loss': 189554.359375}
I1208 16:20:43.488486 140042889676608 callbacks.py:220] Update 52000	TrainingLoss=3.32	Speed 0.255 secs/step 3.9 steps/sec
I1208 16:20:43.496711 140042889676608 callbacks.py:238] {'step': 52000, 'lr': 0.00027408064, 'loss': 3.3166444301605225, 'src_tokens_per_step': 2032.928, 'src_tokens_per_sec': 7964.215995073504, 'src_real_tokens_per_step': 1652.712, 'src_real_tokens_per_sec': 6474.678564931921, 'trg_tokens_per_step': 2032.928, 'trg_tokens_per_sec': 7964.215995073504, 'trg_real_tokens_per_step': 1687.536, 'trg_real_tokens_per_sec': 6611.105363034186, 'samples_per_step': 55.916, 'samples_per_sec': 219.05699640151056, 'this_step_loss': 205647.515625}
I1208 16:20:44.646367 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-52000	Elapsed 1.14s
I1208 16:25:03.968142 140042889676608 callbacks.py:220] Update 53000	TrainingLoss=3.31	Speed 0.259 secs/step 3.9 steps/sec
I1208 16:25:03.981224 140042889676608 callbacks.py:238] {'step': 53000, 'lr': 0.00027148266, 'loss': 3.3081846237182617, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 7841.915042982424, 'src_real_tokens_per_step': 1652.744, 'src_real_tokens_per_sec': 6375.098885497676, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 7841.915042982424, 'trg_real_tokens_per_step': 1691.024, 'trg_real_tokens_per_sec': 6522.7556220139495, 'samples_per_step': 55.86, 'samples_per_sec': 215.4677456060347, 'this_step_loss': 192932.015625}
I1208 16:29:28.658946 140042889676608 callbacks.py:220] Update 54000	TrainingLoss=3.30	Speed 0.265 secs/step 3.8 steps/sec
I1208 16:29:28.660511 140042889676608 callbacks.py:238] {'step': 54000, 'lr': 0.00026895717, 'loss': 3.2996647357940674, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 7684.328963480254, 'src_real_tokens_per_step': 1654.312, 'src_real_tokens_per_sec': 6252.9156761348395, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 7684.328963480254, 'trg_real_tokens_per_step': 1689.336, 'trg_real_tokens_per_sec': 6385.298273033699, 'samples_per_step': 55.766, 'samples_per_sec': 210.78254621578967, 'this_step_loss': 196324.046875}
I1208 16:29:29.731228 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-54000	Elapsed 1.06s
I1208 16:33:58.369277 140042889676608 callbacks.py:220] Update 55000	TrainingLoss=3.29	Speed 0.269 secs/step 3.7 steps/sec
I1208 16:33:58.370825 140042889676608 callbacks.py:238] {'step': 55000, 'lr': 0.0002665009, 'loss': 3.291550636291504, 'src_tokens_per_step': 2033.072, 'src_tokens_per_sec': 7570.095605310252, 'src_real_tokens_per_step': 1649.472, 'src_real_tokens_per_sec': 6141.770059438284, 'trg_tokens_per_step': 2033.072, 'trg_tokens_per_sec': 7570.095605310252, 'trg_real_tokens_per_step': 1689.808, 'trg_real_tokens_per_sec': 6291.960203385865, 'samples_per_step': 56.06, 'samples_per_sec': 208.7380868133016, 'this_step_loss': 396535.21875}
I1208 16:38:24.595682 140042889676608 callbacks.py:220] Update 56000	TrainingLoss=3.28	Speed 0.266 secs/step 3.8 steps/sec
I1208 16:38:24.597511 140042889676608 callbacks.py:238] {'step': 56000, 'lr': 0.0002641107, 'loss': 3.2837891578674316, 'src_tokens_per_step': 2033.016, 'src_tokens_per_sec': 7638.705876579034, 'src_real_tokens_per_step': 1651.824, 'src_real_tokens_per_sec': 6206.442888729989, 'trg_tokens_per_step': 2033.016, 'trg_tokens_per_sec': 7638.705876579034, 'trg_real_tokens_per_step': 1687.696, 'trg_real_tokens_per_sec': 6341.225722315481, 'samples_per_step': 55.994, 'samples_per_sec': 210.38776716620353, 'this_step_loss': 184949.265625}
I1208 16:38:25.440883 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-56000	Elapsed 0.84s
I1208 16:42:53.999397 140042889676608 callbacks.py:220] Update 57000	TrainingLoss=3.28	Speed 0.268 secs/step 3.7 steps/sec
I1208 16:42:54.001357 140042889676608 callbacks.py:238] {'step': 57000, 'lr': 0.0002617837, 'loss': 3.2762362957000732, 'src_tokens_per_step': 2032.944, 'src_tokens_per_sec': 7571.834164096744, 'src_real_tokens_per_step': 1651.6, 'src_real_tokens_per_sec': 6151.493255801528, 'trg_tokens_per_step': 2032.944, 'trg_tokens_per_sec': 7571.834164096744, 'trg_real_tokens_per_step': 1687.84, 'trg_real_tokens_per_sec': 6286.471528743068, 'samples_per_step': 55.953, 'samples_per_sec': 208.40064309873028, 'this_step_loss': 153481.921875}
I1208 16:47:34.169725 140042889676608 callbacks.py:220] Update 58000	TrainingLoss=3.27	Speed 0.280 secs/step 3.6 steps/sec
I1208 16:47:34.175172 140042889676608 callbacks.py:238] {'step': 58000, 'lr': 0.0002595171, 'loss': 3.26888370513916, 'src_tokens_per_step': 2032.992, 'src_tokens_per_sec': 7258.66875387505, 'src_real_tokens_per_step': 1652.248, 'src_real_tokens_per_sec': 5899.246495437535, 'trg_tokens_per_step': 2032.992, 'trg_tokens_per_sec': 7258.66875387505, 'trg_real_tokens_per_step': 1689.104, 'trg_real_tokens_per_sec': 6030.838501501907, 'samples_per_step': 56.041, 'samples_per_sec': 200.09082949461273, 'this_step_loss': 245440.9375}
I1208 16:47:35.086725 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-58000	Elapsed 0.90s
I1208 16:51:40.285791 140042889676608 callbacks.py:220] Update 59000	TrainingLoss=3.26	Speed 0.245 secs/step 4.1 steps/sec
I1208 16:51:40.293372 140042889676608 callbacks.py:238] {'step': 59000, 'lr': 0.00025730842, 'loss': 3.2616207599639893, 'src_tokens_per_step': 2032.848, 'src_tokens_per_sec': 8293.025775287455, 'src_real_tokens_per_step': 1653.664, 'src_real_tokens_per_sec': 6746.140476644075, 'trg_tokens_per_step': 2032.848, 'trg_tokens_per_sec': 8293.025775287455, 'trg_real_tokens_per_step': 1687.152, 'trg_real_tokens_per_sec': 6882.755140978459, 'samples_per_step': 55.613, 'samples_per_sec': 226.8738451871764, 'this_step_loss': 217766.453125}
I1208 16:55:50.756098 140042889676608 callbacks.py:220] Update 60000	TrainingLoss=3.25	Speed 0.250 secs/step 4.0 steps/sec
I1208 16:55:50.759913 140042889676608 callbacks.py:238] {'step': 60000, 'lr': 0.00025515517, 'loss': 3.2547519207000732, 'src_tokens_per_step': 2032.912, 'src_tokens_per_sec': 8119.0897122574315, 'src_real_tokens_per_step': 1650.792, 'src_real_tokens_per_sec': 6592.970253644462, 'trg_tokens_per_step': 2032.912, 'trg_tokens_per_sec': 8119.0897122574315, 'trg_real_tokens_per_step': 1687.08, 'trg_real_tokens_per_sec': 6737.898084991021, 'samples_per_step': 55.978, 'samples_per_sec': 223.56619662471692, 'this_step_loss': 381057.21875}
I1208 16:55:51.898523 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-60000	Elapsed 1.11s
I1208 16:55:57.269268 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=76.50 (Best 76.50)  step=60000	Elapsed 5.37s  FromSTART 16062.02s
I1208 16:55:57.270313 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=7.06 (Best 7.06)  step=60000	Elapsed 5.37s  FromSTART 16062.02s
I1208 16:57:59.934535 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 16:57:59.937378 140042889676608 seq_generation_validator.py:179] Sample 482
I1208 16:57:59.937478 140042889676608 seq_generation_validator.py:181]   Data: "For the time being, there is no concrete religious implication.
I1208 16:57:59.937532 140042889676608 seq_generation_validator.py:182]   Reference: "Derzeit ist kein konkretes religises Engagement zu erkennen.
I1208 16:57:59.937581 140042889676608 seq_generation_validator.py:183]   Hypothesis: "Fr die Zeit gibt es keine konkreten religisen impliziten.
I1208 16:57:59.937627 140042889676608 seq_generation_validator.py:179] Sample 2510
I1208 16:57:59.937671 140042889676608 seq_generation_validator.py:181]   Data: "At least they don't drop cigarette ends on your magnificent carpet, like some beautiful pop music celebrities do," says Baroness Fiona Thyssen-Bornemisza.
I1208 16:57:59.937722 140042889676608 seq_generation_validator.py:182]   Reference: "Wenigstens treten sie ihre Kippen nicht auf deinem herrlichen Teppich aus, wie es einige schne Berhmtheiten aus der Popmusik tun", sagt Baronin Fiona Thyssen-Bornemisza.
I1208 16:57:59.937768 140042889676608 seq_generation_validator.py:183]   Hypothesis: "Zumindest schlieen sie die Zigaretten nicht auf Ihren herrlichen Teppich, wie einige schne Popmusik-Proben do", sagt Ashton Fiona geht-Bornemisza.
I1208 16:57:59.937813 140042889676608 seq_generation_validator.py:179] Sample 888
I1208 16:57:59.937856 140042889676608 seq_generation_validator.py:181]   Data: I like grotesque, I have grotesque ideas.
I1208 16:57:59.937900 140042889676608 seq_generation_validator.py:182]   Reference: Ich liebe die Groteske, und ich habe groteske Ideen.
I1208 16:57:59.937944 140042889676608 seq_generation_validator.py:183]   Hypothesis: Ich bin wie grotesk, ich habe grotesk Ideen.
I1208 16:57:59.937988 140042889676608 seq_generation_validator.py:179] Sample 2342
I1208 16:57:59.938031 140042889676608 seq_generation_validator.py:181]   Data: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.
I1208 16:57:59.938075 140042889676608 seq_generation_validator.py:182]   Reference: Manchmal nicht, denn im Extremfall fhrt sie dazu, dass Menschen extreme krperliche Risiken auf sich nehmen und dem Glcksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.
I1208 16:57:59.938121 140042889676608 seq_generation_validator.py:183]   Hypothesis: Manchmal nicht, weil sie in seiner extreme Lage die Menschen dazu fhren kann, absurd krperliche Risiken zu bernehmen, zu spielen oder den Missbrauch von Substanz als Weg zu erleichtern, Forschung zeigt.
I1208 16:57:59.938165 140042889676608 seq_generation_validator.py:179] Sample 687
I1208 16:57:59.938218 140042889676608 seq_generation_validator.py:181]   Data: Right now, even if I need a few knuckledusters, I get them through someone I trust.
I1208 16:57:59.938273 140042889676608 seq_generation_validator.py:182]   Reference: Ich komme derzeit sogar an einige Schlagringe ber einen Bekannten, dem ich vertraue.
I1208 16:57:59.938336 140042889676608 seq_generation_validator.py:183]   Hypothesis: Jetzt, auch wenn ich ein paar Knuckleduster bentigen, werde ich sie durch jemand, die ich vertraue.
I1208 16:58:01.397168 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.46s
I1208 16:58:03.096656 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.70s
I1208 16:58:03.096894 140042889676608 training_utils.py:359] Evaluating bleu at step=60000 with bad count=0 (early_stop_patience=0).
I1208 16:58:03.096966 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.79 (Best 17.79)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 16:58:03.098255 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.93 (Best 17.93)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 16:58:03.099066 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.79 (Best 17.79)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 16:58:03.099685 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.85 (Best 46.85)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 16:58:03.100193 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.22 (Best 18.22)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 16:58:03.100689 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.36 (Best 18.36)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 16:58:03.101177 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.22 (Best 18.22)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 16:58:03.101655 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.14 (Best 48.14)  step=60000	Elapsed 111.69s  FromSTART 16165.68s
I1208 17:01:15.705664 140042889676608 callbacks.py:220] Update 61000	TrainingLoss=3.25	Speed 0.193 secs/step 5.2 steps/sec
I1208 17:01:15.707561 140042889676608 callbacks.py:238] {'step': 61000, 'lr': 0.0002530551, 'loss': 3.247938871383667, 'src_tokens_per_step': 2032.976, 'src_tokens_per_sec': 10559.099211413455, 'src_real_tokens_per_step': 1649.728, 'src_real_tokens_per_sec': 8568.542680212013, 'trg_tokens_per_step': 2032.976, 'trg_tokens_per_sec': 10559.099211413455, 'trg_real_tokens_per_step': 1688.552, 'trg_real_tokens_per_sec': 8770.19113439146, 'samples_per_step': 55.995, 'samples_per_sec': 290.83312362915075, 'this_step_loss': 391925.3125}
I1208 17:04:43.727619 140042889676608 callbacks.py:220] Update 62000	TrainingLoss=3.24	Speed 0.208 secs/step 4.8 steps/sec
I1208 17:04:43.729192 140042889676608 callbacks.py:238] {'step': 62000, 'lr': 0.00025100604, 'loss': 3.2414329051971436, 'src_tokens_per_step': 2033.0, 'src_tokens_per_sec': 9777.154634215478, 'src_real_tokens_per_step': 1650.864, 'src_real_tokens_per_sec': 7939.376590289966, 'trg_tokens_per_step': 2033.0, 'trg_tokens_per_sec': 9777.154634215478, 'trg_real_tokens_per_step': 1688.048, 'trg_real_tokens_per_sec': 8118.202816516561, 'samples_per_step': 55.87, 'samples_per_sec': 268.69140649956654, 'this_step_loss': 177682.65625}
I1208 17:04:44.989230 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-62000	Elapsed 1.25s
I1208 17:08:10.001734 140042889676608 callbacks.py:220] Update 63000	TrainingLoss=3.23	Speed 0.205 secs/step 4.9 steps/sec
I1208 17:08:10.005153 140042889676608 callbacks.py:238] {'step': 63000, 'lr': 0.00024900597, 'loss': 3.234912872314453, 'src_tokens_per_step': 2032.92, 'src_tokens_per_sec': 9919.529325568314, 'src_real_tokens_per_step': 1652.912, 'src_real_tokens_per_sec': 8065.2996953071315, 'trg_tokens_per_step': 2032.92, 'trg_tokens_per_sec': 9919.529325568314, 'trg_real_tokens_per_step': 1687.496, 'trg_real_tokens_per_sec': 8234.050557217808, 'samples_per_step': 55.859, 'samples_per_sec': 272.5611379675149, 'this_step_loss': 215887.875}
I1208 17:11:37.921596 140042889676608 callbacks.py:220] Update 64000	TrainingLoss=3.23	Speed 0.208 secs/step 4.8 steps/sec
I1208 17:11:37.923422 140042889676608 callbacks.py:238] {'step': 64000, 'lr': 0.00024705296, 'loss': 3.2285704612731934, 'src_tokens_per_step': 2032.848, 'src_tokens_per_sec': 9781.427088054723, 'src_real_tokens_per_step': 1649.536, 'src_real_tokens_per_sec': 7937.049948211296, 'trg_tokens_per_step': 2032.848, 'trg_tokens_per_sec': 9781.427088054723, 'trg_real_tokens_per_step': 1689.584, 'trg_real_tokens_per_sec': 8129.748365418296, 'samples_per_step': 55.941, 'samples_per_sec': 269.17054926530136, 'this_step_loss': 434385.0625}
I1208 17:11:38.926147 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-64000	Elapsed 1.00s
I1208 17:15:04.540680 140042889676608 callbacks.py:220] Update 65000	TrainingLoss=3.22	Speed 0.206 secs/step 4.9 steps/sec
I1208 17:15:04.542366 140042889676608 callbacks.py:238] {'step': 65000, 'lr': 0.00024514517, 'loss': 3.2223174571990967, 'src_tokens_per_step': 2033.16, 'src_tokens_per_sec': 9891.626739558285, 'src_real_tokens_per_step': 1651.288, 'src_real_tokens_per_sec': 8033.762485742254, 'trg_tokens_per_step': 2033.16, 'trg_tokens_per_sec': 9891.626739558285, 'trg_real_tokens_per_step': 1687.432, 'trg_real_tokens_per_sec': 8209.60843828637, 'samples_per_step': 56.167, 'samples_per_sec': 273.26083489777994, 'this_step_loss': 161800.125}
I1208 17:18:31.638704 140042889676608 callbacks.py:220] Update 66000	TrainingLoss=3.22	Speed 0.207 secs/step 4.8 steps/sec
I1208 17:18:31.640409 140042889676608 callbacks.py:238] {'step': 66000, 'lr': 0.00024328091, 'loss': 3.2163562774658203, 'src_tokens_per_step': 2032.96, 'src_tokens_per_sec': 9820.236042140763, 'src_real_tokens_per_step': 1649.408, 'src_real_tokens_per_sec': 7967.48381168115, 'trg_tokens_per_step': 2032.96, 'trg_tokens_per_sec': 9820.236042140763, 'trg_real_tokens_per_step': 1689.816, 'trg_real_tokens_per_sec': 8162.675108111392, 'samples_per_step': 55.816, 'samples_per_sec': 269.6198129467027, 'this_step_loss': 198237.890625}
I1208 17:18:32.595690 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-66000	Elapsed 0.95s
I1208 17:21:58.426748 140042889676608 callbacks.py:220] Update 67000	TrainingLoss=3.21	Speed 0.206 secs/step 4.9 steps/sec
I1208 17:21:58.428608 140042889676608 callbacks.py:238] {'step': 67000, 'lr': 0.00024145858, 'loss': 3.2104551792144775, 'src_tokens_per_step': 2035.112, 'src_tokens_per_sec': 9890.80368448207, 'src_real_tokens_per_step': 1651.088, 'src_real_tokens_per_sec': 8024.41697253229, 'trg_tokens_per_step': 2035.112, 'trg_tokens_per_sec': 9890.80368448207, 'trg_real_tokens_per_step': 1690.808, 'trg_real_tokens_per_sec': 8217.459282905196, 'samples_per_step': 56.023, 'samples_per_sec': 272.27616701967213, 'this_step_loss': 214900.171875}
I1208 17:25:26.823677 140042889676608 callbacks.py:220] Update 68000	TrainingLoss=3.20	Speed 0.208 secs/step 4.8 steps/sec
I1208 17:25:26.825215 140042889676608 callbacks.py:238] {'step': 68000, 'lr': 0.00023967656, 'loss': 3.204669237136841, 'src_tokens_per_step': 2035.296, 'src_tokens_per_sec': 9770.127120141238, 'src_real_tokens_per_step': 1649.896, 'src_real_tokens_per_sec': 7920.073372626167, 'trg_tokens_per_step': 2035.296, 'trg_tokens_per_sec': 9770.127120141238, 'trg_real_tokens_per_step': 1688.68, 'trg_real_tokens_per_sec': 8106.250032054358, 'samples_per_step': 56.144, 'samples_per_sec': 269.5106839659733, 'this_step_loss': 449878.71875}
I1208 17:25:28.216327 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-68000	Elapsed 1.39s
I1208 17:28:52.958140 140042889676608 callbacks.py:220] Update 69000	TrainingLoss=3.20	Speed 0.205 secs/step 4.9 steps/sec
I1208 17:28:52.959722 140042889676608 callbacks.py:238] {'step': 69000, 'lr': 0.00023793345, 'loss': 3.199031114578247, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 9942.543159198542, 'src_real_tokens_per_step': 1654.608, 'src_real_tokens_per_sec': 8084.257577385511, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 9942.543159198542, 'trg_real_tokens_per_step': 1689.352, 'trg_real_tokens_per_sec': 8254.013462325436, 'samples_per_step': 55.729, 'samples_per_sec': 272.28660234334484, 'this_step_loss': 177197.921875}
I1208 17:33:11.817785 140042889676608 callbacks.py:220] Update 70000	TrainingLoss=3.19	Speed 0.259 secs/step 3.9 steps/sec
I1208 17:33:11.819383 140042889676608 callbacks.py:238] {'step': 70000, 'lr': 0.00023622779, 'loss': 3.193495273590088, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 7863.738735815162, 'src_real_tokens_per_step': 1652.48, 'src_real_tokens_per_sec': 6385.663018217335, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 7863.738735815162, 'trg_real_tokens_per_step': 1689.64, 'trg_real_tokens_per_sec': 6529.260058881643, 'samples_per_step': 56.095, 'samples_per_sec': 216.76738417826624, 'this_step_loss': 153668.484375}
I1208 17:33:12.675068 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-70000	Elapsed 0.85s
I1208 17:33:20.634599 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=75.54 (Best 75.54)  step=70000	Elapsed 7.96s  FromSTART 18305.38s
I1208 17:33:20.635597 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.89 (Best 6.89)  step=70000	Elapsed 7.96s  FromSTART 18305.38s
I1208 17:35:35.736580 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 17:35:35.737241 140042889676608 seq_generation_validator.py:179] Sample 489
I1208 17:35:35.737338 140042889676608 seq_generation_validator.py:181]   Data: The Muslim Brothers argue that they are significantly reduced compared to what they were under the former regime.
I1208 17:35:35.737395 140042889676608 seq_generation_validator.py:182]   Reference: Die Moslembrder argumentieren, dass diese verglichen mit dem ehemaligen Regime deutlich reduziert sind.
I1208 17:35:35.737443 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Moslems sind der Ansicht, dass sie im Vergleich zu dem, was sie unter dem ehemaligen Regime waren, erheblich verringert werden.
I1208 17:35:35.737489 140042889676608 seq_generation_validator.py:179] Sample 379
I1208 17:35:35.737534 140042889676608 seq_generation_validator.py:181]   Data: The difficulty of the challenge is also raised with each increase in computing power.
I1208 17:35:35.737579 140042889676608 seq_generation_validator.py:182]   Reference: Darber hinaus wird die Schwierigkeit der Aufgabe mit jeder Verbesserung der Rechenleistung erhht.
I1208 17:35:35.737623 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Schwierigkeit der Herausforderung wird auch mit jedem Anstieg der Computermacht angesprochen.
I1208 17:35:35.737666 140042889676608 seq_generation_validator.py:179] Sample 1185
I1208 17:35:35.737711 140042889676608 seq_generation_validator.py:181]   Data: How many of those were present at the beginning in 1998?
I1208 17:35:35.737755 140042889676608 seq_generation_validator.py:182]   Reference: Wie viele davon waren von Anfang an, d.h. seit 1998 dabei?
I1208 17:35:35.737798 140042889676608 seq_generation_validator.py:183]   Hypothesis: Wie viele dieser waren Anfang 1998 anwesend?
I1208 17:35:35.737842 140042889676608 seq_generation_validator.py:179] Sample 1299
I1208 17:35:35.737885 140042889676608 seq_generation_validator.py:181]   Data: In every era we can find an Alexander the Great or a Napoleon.
I1208 17:35:35.737928 140042889676608 seq_generation_validator.py:182]   Reference: In jeder Epoche knnen wir einen Alexander den Groen oder einen Napoleon ausmachen.
I1208 17:35:35.737972 140042889676608 seq_generation_validator.py:183]   Hypothesis: In jeder ra finden wir einen Alexander der Groen oder einen Sohn von Jakob.
I1208 17:35:35.738015 140042889676608 seq_generation_validator.py:179] Sample 2798
I1208 17:35:35.738059 140042889676608 seq_generation_validator.py:181]   Data: - I think the problem is not to do with access to treatment.
I1208 17:35:35.738102 140042889676608 seq_generation_validator.py:182]   Reference: - Meiner Meinung nach ist das Problem nicht der Zugang zur Behandlung.
I1208 17:35:35.738145 140042889676608 seq_generation_validator.py:183]   Hypothesis: - Ich denke, das Problem ist nicht der Zugang zur Behandlung.
I1208 17:35:37.806435 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 2.07s
I1208 17:35:40.164610 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 2.36s
I1208 17:35:40.164875 140042889676608 training_utils.py:359] Evaluating bleu at step=70000 with bad count=0 (early_stop_patience=0).
I1208 17:35:40.164952 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=17.88 (Best 17.88)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:35:40.166521 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=17.98 (Best 17.98)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:35:40.168440 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=17.88 (Best 17.88)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:35:40.170798 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=46.97 (Best 46.97)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:35:40.173200 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.37 (Best 18.37)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:35:40.175456 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.48 (Best 18.48)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:35:40.177716 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.37 (Best 18.37)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:35:40.179967 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.26 (Best 48.26)  step=70000	Elapsed 124.07s  FromSTART 18421.43s
I1208 17:39:55.006155 140042889676608 callbacks.py:220] Update 71000	TrainingLoss=3.19	Speed 0.255 secs/step 3.9 steps/sec
I1208 17:39:55.014745 140042889676608 callbacks.py:238] {'step': 71000, 'lr': 0.00023455832, 'loss': 3.1880970001220703, 'src_tokens_per_step': 2035.088, 'src_tokens_per_sec': 7988.512357453084, 'src_real_tokens_per_step': 1652.152, 'src_real_tokens_per_sec': 6485.339537352109, 'trg_tokens_per_step': 2035.088, 'trg_tokens_per_sec': 7988.512357453084, 'trg_real_tokens_per_step': 1689.968, 'trg_real_tokens_per_sec': 6633.782053503472, 'samples_per_step': 56.069, 'samples_per_sec': 220.0926443328431, 'this_step_loss': 179558.84375}
I1208 17:44:14.336494 140042889676608 callbacks.py:220] Update 72000	TrainingLoss=3.18	Speed 0.259 secs/step 3.9 steps/sec
I1208 17:44:14.338961 140042889676608 callbacks.py:238] {'step': 72000, 'lr': 0.00023292375, 'loss': 3.182835578918457, 'src_tokens_per_step': 2034.96, 'src_tokens_per_sec': 7850.302962731375, 'src_real_tokens_per_step': 1652.064, 'src_real_tokens_per_sec': 6373.197956629047, 'trg_tokens_per_step': 2034.96, 'trg_tokens_per_sec': 7850.302962731375, 'trg_real_tokens_per_step': 1690.616, 'trg_real_tokens_per_sec': 6521.920722589666, 'samples_per_step': 55.763, 'samples_per_sec': 215.11796011262615, 'this_step_loss': 238721.875}
I1208 17:44:15.229600 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-72000	Elapsed 0.87s
I1208 17:48:36.389487 140042889676608 callbacks.py:220] Update 73000	TrainingLoss=3.18	Speed 0.261 secs/step 3.8 steps/sec
I1208 17:48:36.391058 140042889676608 callbacks.py:238] {'step': 73000, 'lr': 0.00023132288, 'loss': 3.177665948867798, 'src_tokens_per_step': 2035.072, 'src_tokens_per_sec': 7794.685549782791, 'src_real_tokens_per_step': 1654.312, 'src_real_tokens_per_sec': 6336.307433462928, 'trg_tokens_per_step': 2035.072, 'trg_tokens_per_sec': 7794.685549782791, 'trg_real_tokens_per_step': 1688.072, 'trg_real_tokens_per_sec': 6465.614202049331, 'samples_per_step': 56.046, 'samples_per_sec': 214.66608863132424, 'this_step_loss': 408986.53125}
I1208 17:53:17.475998 140042889676608 callbacks.py:220] Update 74000	TrainingLoss=3.17	Speed 0.281 secs/step 3.6 steps/sec
I1208 17:53:17.477876 140042889676608 callbacks.py:238] {'step': 74000, 'lr': 0.00022975456, 'loss': 3.172682762145996, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 7241.702891723556, 'src_real_tokens_per_step': 1651.84, 'src_real_tokens_per_sec': 5878.268099802965, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 7241.702891723556, 'trg_real_tokens_per_step': 1687.52, 'trg_real_tokens_per_sec': 6005.239601765001, 'samples_per_step': 55.913, 'samples_per_sec': 198.97302660323226, 'this_step_loss': 138411.890625}
I1208 17:53:18.676815 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-74000	Elapsed 1.19s
I1208 17:57:45.630712 140042889676608 callbacks.py:220] Update 75000	TrainingLoss=3.17	Speed 0.267 secs/step 3.7 steps/sec
I1208 17:57:45.632415 140042889676608 callbacks.py:238] {'step': 75000, 'lr': 0.00022821774, 'loss': 3.167936325073242, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 7625.255076622782, 'src_real_tokens_per_step': 1649.336, 'src_real_tokens_per_sec': 6180.029732612978, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 7625.255076622782, 'trg_real_tokens_per_step': 1690.592, 'trg_real_tokens_per_sec': 6334.6151576862685, 'samples_per_step': 55.968, 'samples_per_sec': 209.71100132106685, 'this_step_loss': 185724.34375}
I1208 18:01:16.747099 140042889676608 callbacks.py:220] Update 76000	TrainingLoss=3.16	Speed 0.211 secs/step 4.7 steps/sec
I1208 18:01:16.751039 140042889676608 callbacks.py:238] {'step': 76000, 'lr': 0.00022671133, 'loss': 3.1631557941436768, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 9642.48876158447, 'src_real_tokens_per_step': 1651.84, 'src_real_tokens_per_sec': 7827.168038007772, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 9642.48876158447, 'trg_real_tokens_per_step': 1689.784, 'trg_real_tokens_per_sec': 8006.963940779328, 'samples_per_step': 56.025, 'samples_per_sec': 265.47189154481396, 'this_step_loss': 165242.125}
I1208 18:01:17.453672 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-76000	Elapsed 0.68s
I1208 18:05:02.999004 140042889676608 callbacks.py:220] Update 77000	TrainingLoss=3.16	Speed 0.225 secs/step 4.4 steps/sec
I1208 18:05:03.000739 140042889676608 callbacks.py:238] {'step': 77000, 'lr': 0.00022523437, 'loss': 3.158535957336426, 'src_tokens_per_step': 2035.168, 'src_tokens_per_sec': 9026.090117167849, 'src_real_tokens_per_step': 1652.072, 'src_real_tokens_per_sec': 7327.036761608733, 'trg_tokens_per_step': 2035.168, 'trg_tokens_per_sec': 9026.090117167849, 'trg_real_tokens_per_step': 1686.464, 'trg_real_tokens_per_sec': 7479.567310098901, 'samples_per_step': 55.734, 'samples_per_sec': 247.1835772723593, 'this_step_loss': 179388.5}
I1208 18:08:44.662434 140042889676608 callbacks.py:220] Update 78000	TrainingLoss=3.15	Speed 0.222 secs/step 4.5 steps/sec
I1208 18:08:44.664559 140042889676608 callbacks.py:238] {'step': 78000, 'lr': 0.0002237859, 'loss': 3.1538355350494385, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 9183.841715862052, 'src_real_tokens_per_step': 1652.272, 'src_real_tokens_per_sec': 7456.582244173402, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 9183.841715862052, 'trg_real_tokens_per_step': 1688.928, 'trg_real_tokens_per_sec': 7622.008081288853, 'samples_per_step': 55.921, 'samples_per_sec': 252.36736788883476, 'this_step_loss': 193898.421875}
I1208 18:08:45.509112 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-78000	Elapsed 0.84s
I1208 18:12:23.855032 140042889676608 callbacks.py:220] Update 79000	TrainingLoss=3.15	Speed 0.218 secs/step 4.6 steps/sec
I1208 18:12:23.856606 140042889676608 callbacks.py:238] {'step': 79000, 'lr': 0.00022236501, 'loss': 3.1491878032684326, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 9323.2389555544, 'src_real_tokens_per_step': 1652.536, 'src_real_tokens_per_sec': 7570.852666609032, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 9323.2389555544, 'trg_real_tokens_per_step': 1691.272, 'trg_real_tokens_per_sec': 7748.316001080274, 'samples_per_step': 55.997, 'samples_per_sec': 256.5420885064567, 'this_step_loss': 206198.71875}
I1208 18:16:33.430492 140042889676608 callbacks.py:220] Update 80000	TrainingLoss=3.14	Speed 0.249 secs/step 4.0 steps/sec
I1208 18:16:33.437922 140042889676608 callbacks.py:238] {'step': 80000, 'lr': 0.00022097088, 'loss': 3.1447958946228027, 'src_tokens_per_step': 2035.2, 'src_tokens_per_sec': 8157.227036651652, 'src_real_tokens_per_step': 1651.216, 'src_real_tokens_per_sec': 6618.191724917351, 'trg_tokens_per_step': 2035.2, 'trg_tokens_per_sec': 8157.227036651652, 'trg_real_tokens_per_step': 1686.704, 'trg_real_tokens_per_sec': 6760.430164911796, 'samples_per_step': 56.071, 'samples_per_sec': 224.7365748683642, 'this_step_loss': 359457.53125}
I1208 18:16:34.278033 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-80000	Elapsed 0.81s
I1208 18:16:39.480924 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=74.55 (Best 74.55)  step=80000	Elapsed 5.20s  FromSTART 20904.23s
I1208 18:16:39.482158 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.72 (Best 6.72)  step=80000	Elapsed 5.20s  FromSTART 20904.23s
I1208 18:18:37.503823 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 18:18:37.515472 140042889676608 seq_generation_validator.py:179] Sample 1143
I1208 18:18:37.515640 140042889676608 seq_generation_validator.py:181]   Data: The Tap Tap is nonetheless a band composed of handicapped people.
I1208 18:18:37.515716 140042889676608 seq_generation_validator.py:182]   Reference: The Tap Tap ist jedoch eine Gruppe, die sich aus Menschen mit Behinderungen zusammensetzt.
I1208 18:18:37.515782 140042889676608 seq_generation_validator.py:183]   Hypothesis: Der Tap Tap ist dennoch eine Band aus Behinderten.
I1208 18:18:37.515846 140042889676608 seq_generation_validator.py:179] Sample 1746
I1208 18:18:37.515911 140042889676608 seq_generation_validator.py:181]   Data: In this country, human papilloma viruses are also frequently found in young people.
I1208 18:18:37.515974 140042889676608 seq_generation_validator.py:182]   Reference: Hierzulande kommen zudem Humane Pappilomaviren bei Jugendlichen hufig vor.
I1208 18:18:37.516038 140042889676608 seq_generation_validator.py:183]   Hypothesis: In diesem Land finden auch die menschlichen Papillom-Viren hufig in jungen Menschen.
I1208 18:18:37.516101 140042889676608 seq_generation_validator.py:179] Sample 799
I1208 18:18:37.516164 140042889676608 seq_generation_validator.py:181]   Data: - For example, how would you ask me to give you this bottle of water?
I1208 18:18:37.516226 140042889676608 seq_generation_validator.py:182]   Reference: - Wir wrden Sie mich zum Beispiel bitten, Ihnen diese Sprudeldose zu reichen?
I1208 18:18:37.516288 140042889676608 seq_generation_validator.py:183]   Hypothesis: - wie bitte ich Sie zum Beispiel, Ihnen diese Flasche von Wasser zu geben?
I1208 18:18:37.516364 140042889676608 seq_generation_validator.py:179] Sample 2807
I1208 18:18:37.516428 140042889676608 seq_generation_validator.py:181]   Data: To date, his parents are only aware he had cancer.
I1208 18:18:37.516490 140042889676608 seq_generation_validator.py:182]   Reference: Heute wissen seine Eltern nur, dass er Krebs hatte.
I1208 18:18:37.516553 140042889676608 seq_generation_validator.py:183]   Hypothesis: Zur Zeit sind seine Eltern nur bewusst, dass er Krebskrebs hatte.
I1208 18:18:37.516614 140042889676608 seq_generation_validator.py:179] Sample 327
I1208 18:18:37.516676 140042889676608 seq_generation_validator.py:181]   Data: Thiago Silva, who is one of the best defenders in the world, also helps everyone else progress.
I1208 18:18:37.516737 140042889676608 seq_generation_validator.py:182]   Reference: Thiago Silva, der weltweit einer der besten Verteidiger ist, ermglicht es allen Anderen ebenfalls, Fortschritte zu machen.
I1208 18:18:37.516801 140042889676608 seq_generation_validator.py:183]   Hypothesis: Thiago Silva, der eine der besten Verteidiger der Welt ist, hilft auch allen anderen Fortschritt.
I1208 18:18:38.728543 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 1.21s
I1208 18:18:40.608949 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.88s
I1208 18:18:40.609276 140042889676608 training_utils.py:359] Evaluating bleu at step=80000 with bad count=0 (early_stop_patience=0).
I1208 18:18:40.609390 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.33 (Best 18.33)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:18:40.612742 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.40 (Best 18.40)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:18:40.614554 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.33 (Best 18.33)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:18:40.616839 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.25 (Best 47.25)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:18:40.619068 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.80 (Best 18.80)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:18:40.621307 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.90 (Best 18.90)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:18:40.623317 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.80 (Best 18.80)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:18:40.624104 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.55 (Best 48.55)  step=80000	Elapsed 108.07s  FromSTART 21004.27s
I1208 18:22:25.529957 140042889676608 callbacks.py:220] Update 81000	TrainingLoss=3.14	Speed 0.225 secs/step 4.4 steps/sec
I1208 18:22:25.532073 140042889676608 callbacks.py:238] {'step': 81000, 'lr': 0.00021960262, 'loss': 3.140582323074341, 'src_tokens_per_step': 2034.88, 'src_tokens_per_sec': 9050.792514020151, 'src_real_tokens_per_step': 1654.0, 'src_real_tokens_per_sec': 7356.704482912668, 'trg_tokens_per_step': 2034.88, 'trg_tokens_per_sec': 9050.792514020151, 'trg_real_tokens_per_step': 1688.608, 'trg_real_tokens_per_sec': 7510.634850956587, 'samples_per_step': 55.682, 'samples_per_sec': 247.66385672161016, 'this_step_loss': 133154.953125}
I1208 18:26:02.417191 140042889676608 callbacks.py:220] Update 82000	TrainingLoss=3.14	Speed 0.217 secs/step 4.6 steps/sec
I1208 18:26:02.420506 140042889676608 callbacks.py:238] {'step': 82000, 'lr': 0.00021825948, 'loss': 3.1362950801849365, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 9386.364372847318, 'src_real_tokens_per_step': 1654.272, 'src_real_tokens_per_sec': 7630.120176408758, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 9386.364372847318, 'trg_real_tokens_per_step': 1689.04, 'trg_real_tokens_per_sec': 7790.483174932205, 'samples_per_step': 55.955, 'samples_per_sec': 258.0853538420236, 'this_step_loss': 195182.421875}
I1208 18:26:03.307441 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-82000	Elapsed 0.88s
I1208 18:30:26.739265 140042889676608 callbacks.py:220] Update 83000	TrainingLoss=3.13	Speed 0.263 secs/step 3.8 steps/sec
I1208 18:30:26.743031 140042889676608 callbacks.py:238] {'step': 83000, 'lr': 0.00021694068, 'loss': 3.132202386856079, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 7726.879379528592, 'src_real_tokens_per_step': 1651.776, 'src_real_tokens_per_sec': 6271.953387415191, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 7726.879379528592, 'trg_real_tokens_per_step': 1689.264, 'trg_real_tokens_per_sec': 6414.298952786901, 'samples_per_step': 55.857, 'samples_per_sec': 212.0944367522293, 'this_step_loss': 181737.015625}
I1208 18:34:59.934654 140042889676608 callbacks.py:220] Update 84000	TrainingLoss=3.13	Speed 0.273 secs/step 3.7 steps/sec
I1208 18:34:59.940619 140042889676608 callbacks.py:238] {'step': 84000, 'lr': 0.00021564549, 'loss': 3.128286361694336, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 7451.805601646682, 'src_real_tokens_per_step': 1653.824, 'src_real_tokens_per_sec': 6055.98353782281, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 7451.805601646682, 'trg_real_tokens_per_step': 1687.168, 'trg_real_tokens_per_sec': 6178.082815064623, 'samples_per_step': 55.619, 'samples_per_sec': 203.66601790164302, 'this_step_loss': 179701.265625}
I1208 18:35:00.789899 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-84000	Elapsed 0.83s
I1208 18:40:04.431406 140042889676608 callbacks.py:220] Update 85000	TrainingLoss=3.12	Speed 0.304 secs/step 3.3 steps/sec
I1208 18:40:04.433910 140042889676608 callbacks.py:238] {'step': 85000, 'lr': 0.00021437323, 'loss': 3.1243066787719727, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 6703.599126885859, 'src_real_tokens_per_step': 1650.272, 'src_real_tokens_per_sec': 5436.225281828956, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 6703.599126885859, 'trg_real_tokens_per_step': 1690.768, 'trg_real_tokens_per_sec': 5569.624732957585, 'samples_per_step': 55.989, 'samples_per_sec': 184.43554596110303, 'this_step_loss': 380473.8125}
I1208 18:44:37.541660 140042889676608 callbacks.py:220] Update 86000	TrainingLoss=3.12	Speed 0.273 secs/step 3.7 steps/sec
I1208 18:44:37.544739 140042889676608 callbacks.py:238] {'step': 86000, 'lr': 0.00021312323, 'loss': 3.120399236679077, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 7453.672794722255, 'src_real_tokens_per_step': 1650.32, 'src_real_tokens_per_sec': 6044.571746297877, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 7453.672794722255, 'trg_real_tokens_per_step': 1689.072, 'trg_real_tokens_per_sec': 6186.507397754889, 'samples_per_step': 55.921, 'samples_per_sec': 204.8199722627876, 'this_step_loss': 210936.953125}
I1208 18:44:38.880173 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-86000	Elapsed 1.33s
I1208 18:49:08.146507 140042889676608 callbacks.py:220] Update 87000	TrainingLoss=3.12	Speed 0.269 secs/step 3.7 steps/sec
I1208 18:49:08.148281 140042889676608 callbacks.py:238] {'step': 87000, 'lr': 0.00021189486, 'loss': 3.116499662399292, 'src_tokens_per_step': 2034.912, 'src_tokens_per_sec': 7559.234485944066, 'src_real_tokens_per_step': 1652.656, 'src_real_tokens_per_sec': 6139.240531581895, 'trg_tokens_per_step': 2034.912, 'trg_tokens_per_sec': 7559.234485944066, 'trg_real_tokens_per_step': 1688.8, 'trg_real_tokens_per_sec': 6273.507257248638, 'samples_per_step': 55.861, 'samples_per_sec': 207.5108887358871, 'this_step_loss': 162163.46875}
I1208 18:53:40.501503 140042889676608 callbacks.py:220] Update 88000	TrainingLoss=3.11	Speed 0.272 secs/step 3.7 steps/sec
I1208 18:53:40.503317 140042889676608 callbacks.py:238] {'step': 88000, 'lr': 0.00021068745, 'loss': 3.1126604080200195, 'src_tokens_per_step': 2035.104, 'src_tokens_per_sec': 7474.564488428242, 'src_real_tokens_per_step': 1649.056, 'src_real_tokens_per_sec': 6056.680846300495, 'trg_tokens_per_step': 2035.104, 'trg_tokens_per_sec': 7474.564488428242, 'trg_real_tokens_per_step': 1689.328, 'trg_real_tokens_per_sec': 6204.592530950509, 'samples_per_step': 55.845, 'samples_per_sec': 205.1084631823608, 'this_step_loss': 378968.53125}
I1208 18:53:41.363886 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-88000	Elapsed 0.85s
I1208 18:58:02.584142 140042889676608 callbacks.py:220] Update 89000	TrainingLoss=3.11	Speed 0.261 secs/step 3.8 steps/sec
I1208 18:58:02.586957 140042889676608 callbacks.py:238] {'step': 89000, 'lr': 0.00020950047, 'loss': 3.109020471572876, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 7792.268766539952, 'src_real_tokens_per_step': 1650.96, 'src_real_tokens_per_sec': 6321.905685270356, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 7792.268766539952, 'trg_real_tokens_per_step': 1690.88, 'trg_real_tokens_per_sec': 6474.768549880033, 'samples_per_step': 55.791, 'samples_per_sec': 213.63657513623494, 'this_step_loss': 152611.296875}
I1208 19:02:24.758498 140042889676608 callbacks.py:220] Update 90000	TrainingLoss=3.11	Speed 0.262 secs/step 3.8 steps/sec
I1208 19:02:24.772874 140042889676608 callbacks.py:238] {'step': 90000, 'lr': 0.00020833334, 'loss': 3.105344533920288, 'src_tokens_per_step': 2034.944, 'src_tokens_per_sec': 7764.20076611116, 'src_real_tokens_per_step': 1652.016, 'src_real_tokens_per_sec': 6303.163081061638, 'trg_tokens_per_step': 2034.944, 'trg_tokens_per_sec': 7764.20076611116, 'trg_real_tokens_per_step': 1689.632, 'trg_real_tokens_per_sec': 6446.6845617598965, 'samples_per_step': 55.846, 'samples_per_sec': 213.0768984228774, 'this_step_loss': 104358.53125}
I1208 19:02:26.650911 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-90000	Elapsed 1.87s
I1208 19:02:37.070690 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=73.99 (Best 73.99)  step=90000	Elapsed 10.42s  FromSTART 23661.82s
I1208 19:02:37.071636 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.63 (Best 6.63)  step=90000	Elapsed 10.42s  FromSTART 23661.82s
I1208 19:05:08.723211 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 19:05:08.723797 140042889676608 seq_generation_validator.py:179] Sample 2843
I1208 19:05:08.723878 140042889676608 seq_generation_validator.py:181]   Data: Should this fail, an appeal for review by a higher court, in this case the US Supreme Court, will be filed.
I1208 19:05:08.723929 140042889676608 seq_generation_validator.py:182]   Reference: Wird dem nicht stattgegeben, wird der Cerciorare-Einspruch eingelegt, der die berprfung des Verfahrens durch eine hhere Instanz fordert, in diesem Fall durch den Obersten Gerichtshof der Vereinigten Staaten.
I1208 19:05:08.723979 140042889676608 seq_generation_validator.py:183]   Hypothesis: Sollte dies nicht geschehen, wird ein Appell zur berprfung eines hheren Gerichtshofs in diesem Fall der Oberste Gerichtshof der USA eingereicht.
I1208 19:05:08.724026 140042889676608 seq_generation_validator.py:179] Sample 2506
I1208 19:05:08.724071 140042889676608 seq_generation_validator.py:181]   Data: Valentino has always been fascinated by the rarefied and distant world of the nobility.
I1208 19:05:08.724115 140042889676608 seq_generation_validator.py:182]   Reference: Valentino versprte immer schon Faszination fr die stickige und ferne Welt des Adels.
I1208 19:05:08.724159 140042889676608 seq_generation_validator.py:183]   Hypothesis: Choino wurde immer von der erzwungenen und fernen Welt der Tradition begeistert.
I1208 19:05:08.724202 140042889676608 seq_generation_validator.py:179] Sample 2778
I1208 19:05:08.724246 140042889676608 seq_generation_validator.py:181]   Data: "This does not mean a health relapse for President Chavez" he said.
I1208 19:05:08.724289 140042889676608 seq_generation_validator.py:182]   Reference: "Es bedeutet in keinem Fall einen Rckfall im Gesundheitszustand des Prsidenten Chvez", kommentierte er.
I1208 19:05:08.724343 140042889676608 seq_generation_validator.py:183]   Hypothesis: "Das bedeutet nicht, dass Prsident Chavez" sagte.
I1208 19:05:08.724386 140042889676608 seq_generation_validator.py:179] Sample 429
I1208 19:05:08.724430 140042889676608 seq_generation_validator.py:181]   Data: Former commanders of the anti-Soviet struggle are already thinking about restoring provincial militias, which will escape the central power.
I1208 19:05:08.724474 140042889676608 seq_generation_validator.py:182]   Reference: Ehemalige Kommandeure des antisowjetischen Kampfes bemhen sich bereits um die Wiederherstellung der Milizen in der Provinz, die sich der Macht der Zentralregierung entziehen werden.
I1208 19:05:08.724519 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die frheren Vereinen des Anti-sowjetischen Kampfes denken bereits an die Wiederherstellung der Milizen, die die zentrale Macht entgehen werden.
I1208 19:05:08.724562 140042889676608 seq_generation_validator.py:179] Sample 1467
I1208 19:05:08.724605 140042889676608 seq_generation_validator.py:181]   Data: In the senate, there are only two independents, including me.
I1208 19:05:08.724648 140042889676608 seq_generation_validator.py:182]   Reference: Im Senat sind wir damit nur zwei Unabhngige.
I1208 19:05:08.724691 140042889676608 seq_generation_validator.py:183]   Hypothesis: In der Senate gibt es nur zwei unabhngige Personen, einschlielich mir.
I1208 19:05:09.586480 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.86s
I1208 19:05:11.189533 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 1.60s
I1208 19:05:11.198207 140042889676608 training_utils.py:359] Evaluating bleu at step=90000 with bad count=1 (early_stop_patience=0).
I1208 19:05:11.198459 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.30 (Best 18.33)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:05:11.199808 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.46 (Best 18.40)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:05:11.200475 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.30 (Best 18.33)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:05:11.201005 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.37 (Best 47.25)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:05:11.201529 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.80 (Best 18.80)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:05:11.202042 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.96 (Best 18.90)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:05:11.202674 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.80 (Best 18.80)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:05:11.203166 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.71 (Best 48.55)  step=90000	Elapsed 140.76s  FromSTART 23794.56s
I1208 19:09:37.792634 140042889676608 callbacks.py:220] Update 91000	TrainingLoss=3.10	Speed 0.267 secs/step 3.8 steps/sec
I1208 19:09:37.794088 140042889676608 callbacks.py:238] {'step': 91000, 'lr': 0.0002071855, 'loss': 3.1018025875091553, 'src_tokens_per_step': 2034.96, 'src_tokens_per_sec': 7635.344385830847, 'src_real_tokens_per_step': 1650.464, 'src_real_tokens_per_sec': 6192.682429343045, 'trg_tokens_per_step': 2034.96, 'trg_tokens_per_sec': 7635.344385830847, 'trg_real_tokens_per_step': 1684.64, 'trg_real_tokens_per_sec': 6320.913711397806, 'samples_per_step': 56.029, 'samples_per_sec': 210.22561160598565, 'this_step_loss': 104742.3203125}
I1208 19:13:40.203489 140042889676608 callbacks.py:220] Update 92000	TrainingLoss=3.10	Speed 0.242 secs/step 4.1 steps/sec
I1208 19:13:40.205199 140042889676608 callbacks.py:238] {'step': 92000, 'lr': 0.00020605639, 'loss': 3.0983757972717285, 'src_tokens_per_step': 2034.768, 'src_tokens_per_sec': 8396.534778656025, 'src_real_tokens_per_step': 1650.4, 'src_real_tokens_per_sec': 6810.428018670386, 'trg_tokens_per_step': 2034.768, 'trg_tokens_per_sec': 8396.534778656025, 'trg_real_tokens_per_step': 1689.952, 'trg_real_tokens_per_sec': 6973.640602889031, 'samples_per_step': 55.752, 'samples_per_sec': 230.0623987499463, 'this_step_loss': 176520.609375}
I1208 19:13:40.937704 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-92000	Elapsed 0.73s
I1208 19:17:06.168274 140042889676608 callbacks.py:220] Update 93000	TrainingLoss=3.09	Speed 0.205 secs/step 4.9 steps/sec
I1208 19:17:06.169700 140042889676608 callbacks.py:238] {'step': 93000, 'lr': 0.00020494558, 'loss': 3.0948548316955566, 'src_tokens_per_step': 2035.168, 'src_tokens_per_sec': 9919.885394807221, 'src_real_tokens_per_step': 1649.312, 'src_real_tokens_per_sec': 8039.13289727447, 'trg_tokens_per_step': 2035.168, 'trg_tokens_per_sec': 9919.885394807221, 'trg_real_tokens_per_step': 1690.992, 'trg_real_tokens_per_sec': 8242.290977224413, 'samples_per_step': 56.106, 'samples_per_sec': 273.47378199787636, 'this_step_loss': 125669.453125}
I1208 19:21:08.455215 140042889676608 callbacks.py:220] Update 94000	TrainingLoss=3.09	Speed 0.242 secs/step 4.1 steps/sec
I1208 19:21:08.461028 140042889676608 callbacks.py:238] {'step': 94000, 'lr': 0.00020385251, 'loss': 3.0915725231170654, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 8401.583823315266, 'src_real_tokens_per_step': 1649.824, 'src_real_tokens_per_sec': 6811.448699993162, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 8401.583823315266, 'trg_real_tokens_per_step': 1687.376, 'trg_real_tokens_per_sec': 6966.485553367912, 'samples_per_step': 55.759, 'samples_per_sec': 230.20611172035242, 'this_step_loss': 90059.765625}
I1208 19:21:09.355875 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-94000	Elapsed 0.87s
I1208 19:25:10.158543 140042889676608 callbacks.py:220] Update 95000	TrainingLoss=3.09	Speed 0.241 secs/step 4.2 steps/sec
I1208 19:25:10.168393 140042889676608 callbacks.py:238] {'step': 95000, 'lr': 0.00020277678, 'loss': 3.088304281234741, 'src_tokens_per_step': 2035.04, 'src_tokens_per_sec': 8453.484170888358, 'src_real_tokens_per_step': 1652.176, 'src_real_tokens_per_sec': 6863.080658621769, 'trg_tokens_per_step': 2035.04, 'trg_tokens_per_sec': 8453.484170888358, 'trg_real_tokens_per_step': 1687.488, 'trg_real_tokens_per_sec': 7009.7654574672015, 'samples_per_step': 55.976, 'samples_per_sec': 232.52232386078245, 'this_step_loss': 80365.703125}
I1208 19:29:27.804546 140042889676608 callbacks.py:220] Update 96000	TrainingLoss=3.08	Speed 0.258 secs/step 3.9 steps/sec
I1208 19:29:40.677790 140042889676608 callbacks.py:238] {'step': 96000, 'lr': 0.00020171789, 'loss': 3.0849430561065674, 'src_tokens_per_step': 2035.232, 'src_tokens_per_sec': 7901.852244664259, 'src_real_tokens_per_step': 1652.688, 'src_real_tokens_per_sec': 6416.613134291169, 'trg_tokens_per_step': 2035.232, 'trg_tokens_per_sec': 7901.852244664259, 'trg_real_tokens_per_step': 1688.912, 'trg_real_tokens_per_sec': 6557.2539534757725, 'samples_per_step': 55.985, 'samples_per_sec': 217.36352313521434, 'this_step_loss': 103223.4140625}
I1208 19:29:49.085420 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-96000	Elapsed 8.40s
I1208 19:33:48.621738 140042889676608 callbacks.py:220] Update 97000	TrainingLoss=3.08	Speed 0.239 secs/step 4.2 steps/sec
I1208 19:33:48.623573 140042889676608 callbacks.py:238] {'step': 97000, 'lr': 0.0002006754, 'loss': 3.081674337387085, 'src_tokens_per_step': 2035.072, 'src_tokens_per_sec': 8498.331170645468, 'src_real_tokens_per_step': 1653.472, 'src_real_tokens_per_sec': 6904.793853676678, 'trg_tokens_per_step': 2035.072, 'trg_tokens_per_sec': 8498.331170645468, 'trg_real_tokens_per_step': 1686.736, 'trg_real_tokens_per_sec': 7043.702200929429, 'samples_per_step': 55.899, 'samples_per_sec': 233.43066687955562, 'this_step_loss': 205882.90625}
I1208 19:38:11.773180 140042889676608 callbacks.py:220] Update 98000	TrainingLoss=3.08	Speed 0.263 secs/step 3.8 steps/sec
I1208 19:38:11.774575 140042889676608 callbacks.py:238] {'step': 98000, 'lr': 0.00019964892, 'loss': 3.078575372695923, 'src_tokens_per_step': 2035.008, 'src_tokens_per_sec': 7735.491775696415, 'src_real_tokens_per_step': 1650.608, 'src_real_tokens_per_sec': 6274.306837564623, 'trg_tokens_per_step': 2035.008, 'trg_tokens_per_sec': 7735.491775696415, 'trg_real_tokens_per_step': 1690.8, 'trg_real_tokens_per_sec': 6427.085050450661, 'samples_per_step': 55.916, 'samples_per_sec': 212.54843132304185, 'this_step_loss': 222642.390625}
I1208 19:38:12.932837 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-98000	Elapsed 1.15s
I1208 19:42:38.306372 140042889676608 callbacks.py:220] Update 99000	TrainingLoss=3.08	Speed 0.265 secs/step 3.8 steps/sec
I1208 19:42:38.308052 140042889676608 callbacks.py:238] {'step': 99000, 'lr': 0.00019863802, 'loss': 3.075434446334839, 'src_tokens_per_step': 2034.976, 'src_tokens_per_sec': 7670.3421608390045, 'src_real_tokens_per_step': 1652.56, 'src_real_tokens_per_sec': 6228.918985440667, 'trg_tokens_per_step': 2034.976, 'trg_tokens_per_sec': 7670.3421608390045, 'trg_real_tokens_per_step': 1687.664, 'trg_real_tokens_per_sec': 6361.234890500034, 'samples_per_step': 56.063, 'samples_per_sec': 211.3157071941473, 'this_step_loss': 391855.03125}
I1208 19:47:11.282634 140042889676608 callbacks.py:220] Update 100000	TrainingLoss=3.07	Speed 0.273 secs/step 3.7 steps/sec
I1208 19:47:11.283980 140042889676608 callbacks.py:238] {'step': 100000, 'lr': 0.00019764237, 'loss': 3.0722954273223877, 'src_tokens_per_step': 2034.912, 'src_tokens_per_sec': 7456.578494793103, 'src_real_tokens_per_step': 1651.008, 'src_real_tokens_per_sec': 6049.8295491556255, 'trg_tokens_per_step': 2034.912, 'trg_tokens_per_sec': 7456.578494793103, 'trg_real_tokens_per_step': 1688.736, 'trg_real_tokens_per_sec': 6188.077194975963, 'samples_per_step': 55.913, 'samples_per_sec': 204.8833921955184, 'this_step_loss': 186493.265625}
I1208 19:47:11.941769 140042889676608 callbacks.py:94] Saved checkpoint into ./wmt14_en_de/benchmark_s-1208/ckpt-100000	Elapsed 0.65s
I1208 19:47:17.506178 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: NLL=73.90 (Best 73.90)  step=100000	Elapsed 5.56s  FromSTART 26342.26s
I1208 19:47:17.509508 140042889676608 criterion_validator.py:118] Evaluating (NLL) validation set: PPL=6.61 (Best 6.61)  step=100000	Elapsed 5.56s  FromSTART 26342.26s
I1208 19:49:36.871262 140042889676608 seq_generation_validator.py:177] ===== Generation examples (Total 3000) =====
I1208 19:49:36.873636 140042889676608 seq_generation_validator.py:179] Sample 2483
I1208 19:49:36.873728 140042889676608 seq_generation_validator.py:181]   Data: Give it a chance.
I1208 19:49:36.873781 140042889676608 seq_generation_validator.py:182]   Reference: Gib dem Ganzen eine Chance.
I1208 19:49:36.873828 140042889676608 seq_generation_validator.py:183]   Hypothesis: Geben Sie ihn eine Chance.
I1208 19:49:36.873873 140042889676608 seq_generation_validator.py:179] Sample 660
I1208 19:49:36.873918 140042889676608 seq_generation_validator.py:181]   Data: Explosions have often been heard at military warehouses.
I1208 19:49:36.873962 140042889676608 seq_generation_validator.py:182]   Reference: Bei vielen Militrlagern kam es hufig zu Explosionen.
I1208 19:49:36.874008 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Explosionen wurden oft bei Militrlager gehrt.
I1208 19:49:36.874052 140042889676608 seq_generation_validator.py:179] Sample 394
I1208 19:49:36.874095 140042889676608 seq_generation_validator.py:181]   Data: They just need to provide an address that a phone can "photograph and recognise" says Pierre Noizat, who confirms he has thousands of users.
I1208 19:49:36.874140 140042889676608 seq_generation_validator.py:182]   Reference: Sie mssen einfach nur eine Adresse angeben, die von einem Telefon "fotografiert und erkannt" werden kann, gibt Pierre Noizat an, der versichert, dass er tausende Benutzer hat.
I1208 19:49:36.874186 140042889676608 seq_generation_validator.py:183]   Hypothesis: Sie mssen lediglich eine Adresse liefern, die ein Telefon "Foto und Anerkennung" sagt Pierre Noizat, der besttigt, dass er Tausende von Nutzern hat.
I1208 19:49:36.874231 140042889676608 seq_generation_validator.py:179] Sample 2858
I1208 19:49:36.874274 140042889676608 seq_generation_validator.py:181]   Data: The Government ignored an order of the Senate to explain under what terms and conditions the Mexican Ambassador in Japan signed the Anti-Counterfeiting Trade Agreement, known by its acronym ACTA, according to the Mexican Institute of Industrial Property, and the matter has already been archived.
I1208 19:49:36.874325 140042889676608 seq_generation_validator.py:182]   Reference: Weil die Regierung einer Anfrage des Senats, zu erklren, unter welchen Bedingungen und Umstnden der mexikanische Botschafter in Japan das Anti-Counterfeiting Trade Agreement unterzeichnete, das unter der englischen Abkrzung ACTA bekannt ist, nicht nachgekommen ist, ist die Angelegenheit gem dem mexikanischen Institut fr intellektuelles Eigentum bereits archiviert worden.
I1208 19:49:36.874372 140042889676608 seq_generation_validator.py:183]   Hypothesis: Die Regierung hat eine Reihenfolge des Senats ignoriert, um zu erklren, welche Bedingungen und Bedingungen der mexikanische Botschafter in Japan das Abkommen ber die Bekmpfung des Handelsabkommens unterzeichnet hat, das durch seine Akronyologie ACTA bekannt ist, gem dem mexikanischen Institut fr Industrie-Eigentum, und die Angelegenheit ist bereits Archiviert worden.
I1208 19:49:36.874418 140042889676608 seq_generation_validator.py:179] Sample 1617
I1208 19:49:36.874464 140042889676608 seq_generation_validator.py:181]   Data: Does Germany view itself as playing the role of an important international player - does Germany actually want to assume a leading role?
I1208 19:49:36.874513 140042889676608 seq_generation_validator.py:182]   Reference: Sieht sich Deutschland selbst in der Rolle als wichtiger internationaler Spieler - will Deutschland berhaupt eine Hauptrolle bernehmen?
I1208 19:49:36.874557 140042889676608 seq_generation_validator.py:183]   Hypothesis: Stimmt Deutschland selbst die Rolle eines wichtigen internationalen Spielers - will Deutschland tatschlich eine fhrende Rolle bernehmen?
I1208 19:49:37.734259 140042889676608 training_utils.py:351] Checking the best checkpoints kept and a new checkpoint was saved. Elapsed 0.86s
I1208 19:49:39.868169 140042889676608 training_utils.py:356] An averaged checkpoint was saved. Elapsed 2.13s
I1208 19:49:39.868424 140042889676608 training_utils.py:359] Evaluating bleu at step=100000 with bad count=0 (early_stop_patience=0).
I1208 19:49:39.868491 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: sacre_bleu=18.36 (Best 18.36)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.869794 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: tok_bleu=18.45 (Best 18.45)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.870442 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: detok_bleu=18.36 (Best 18.36)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.870944 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: chrf=47.41 (Best 47.41)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.871441 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_sacre_bleu=18.84 (Best 18.84)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.871909 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_tok_bleu=18.94 (Best 18.94)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.872386 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_detok_bleu=18.84 (Best 18.84)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.872850 140042889676608 seq_generation_validator.py:195] Evaluating (bleu) validation set: uncased_chrf=48.71 (Best 48.71)  step=100000	Elapsed 128.61s  FromSTART 26462.85s
I1208 19:49:39.880873 140042889676608 trainer.py:315] {'loss': [3.0722954273223877], 'src_tokens': [203368768.0], 'src_real_tokens': [165164288.0], 'trg_tokens': [203368768.0], 'trg_real_tokens': [168868384.0], 'samples': [5591388.0], 'this_step_loss': [186493.265625]}
